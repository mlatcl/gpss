<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Bayesian Learning of GP-LVM</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="../assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Bayesian Learning of GP-LVM</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="venue" style="text-align:center">Gaussian Process Summer School</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
</section>
<section id="selecting-data-dimensionality" class="slide level2">
<h2>Selecting Data Dimensionality</h2>
<ul>
<li>GP-LVM Provides probabilistic non-linear dimensionality reduction.</li>
<li>How to select the dimensionality?</li>
<li>Need to estimate marginal likelihood.</li>
<li>In standard GP-LVM it increases with increasing <span class="math inline">\(q\)</span>.</li>
</ul>
</section>
<section id="integrate-mapping-function-and-latent-variables" class="slide level2">
<h2>Integrate Mapping Function and Latent Variables</h2>
<table>
<tr>
<td width="40%">
<p><strong>Bayesian GP-LVM</strong></p>
<ul>
<li>Start with a standard GP-LVM.</li>
<li>Apply standard latent variable approach:
<ul>
<li>Define Gaussian prior over , <span class="math inline">\(\mathbf{Z}\)</span>.</li>
<li>Integrate out .</li>
<li>Unfortunately integration is intractable.</li>
</ul></li>
</ul>
</td>
<td width="60%">
<center>
{ }
</center>
</td>
</tr>
</table>
</section>
<section id="standard-variational-approach-fails" class="slide level2">
<h2>Standard Variational Approach Fails</h2>
<ul>
<li>Standard variational bound has the form: <span class="math display">\[
\mathcal{L}= \left&lt;\log p(\mathbf{ y}|\mathbf{Z})\right&gt;_{q(\mathbf{Z})} + \text{KL}\left( q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)
\]</span></li>
</ul>
</section>
<section id="standard-variational-approach-fails-1" class="slide level2">
<h2>Standard Variational Approach Fails</h2>
<ul>
<li>Requires expectation of <span class="math inline">\(\log p(\mathbf{ y}|\mathbf{Z})\)</span> under <span class="math inline">\(q(\mathbf{Z})\)</span>. <span class="math display">\[
\begin{align}
\log p(\mathbf{ y}|\mathbf{Z}) = &amp; -\frac{1}{2}\mathbf{ y}^\top\left(\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}+\sigma^2\mathbf{I}\right)^{-1}\mathbf{ y}\\ &amp; -\frac{1}{2}\log \det{\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}+\sigma^2 \mathbf{I}} -\frac{n}{2}\log 2\pi
\end{align}
\]</span> <span class="math inline">\(\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}\)</span> is dependent on <span class="math inline">\(\mathbf{Z}\)</span> and it appears in the inverse.</li>
</ul>
</section>
<section id="variational-bayesian-gp-lvm" class="slide level2">
<h2>Variational Bayesian GP-LVM</h2>
<ul>
<li>Consider collapsed variational bound, <span class="fragment" data-fragment-index="1"><small><span class="math display">\[
  p(\mathbf{ y})\geq \prod_{i=1}^nc_i \int \mathcal{N}\left(\mathbf{ y}|\left&lt;\mathbf{ f}\right&gt;,\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{ u}
\]</span></small></span> <span class="fragment" data-fragment-index="2"><small><span class="math display">\[
  p(\mathbf{ y}|\mathbf{Z})\geq \prod_{i=1}^nc_i \int \mathcal{N}\left(\mathbf{ y}|\left&lt;\mathbf{ f}\right&gt;_{p(\mathbf{ f}|\mathbf{ u}, \mathbf{Z})},\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{ u}
\]</span></small></span> <span class="fragment" data-fragment-index="3"><small><span class="math display">\[
    \int p(\mathbf{ y}|\mathbf{Z})p(\mathbf{Z}) \text{d}\mathbf{Z}\geq \int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{ y}|\left&lt;\mathbf{ f}\right&gt;_{p(\mathbf{ f}|\mathbf{ u}, \mathbf{Z})},\sigma^2\mathbf{I}\right) p(\mathbf{Z})\text{d}\mathbf{Z}p(\mathbf{ u}) \text{d}\mathbf{ u}
\]</span></small></span></li>
</ul>
</section>
<section id="variational-bayesian-gp-lvm-1" class="slide level2">
<h2>Variational Bayesian GP-LVM</h2>
<ul>
<li>Apply variational lower bound to the inner integral. <small><span class="math display">\[
  \begin{align}
  \int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{ y}|\left&lt;\mathbf{ f}\right&gt;_{p(\mathbf{ f}|\mathbf{ u}, \mathbf{Z})},\sigma^2\mathbf{I}\right) p(\mathbf{Z})\text{d}\mathbf{Z}\geq &amp; \left&lt;\sum_{i=1}^n\log  c_i\right&gt;_{q(\mathbf{Z})}\\ &amp; +\left&lt;\log\mathcal{N}\left(\mathbf{ y}|\left&lt;\mathbf{ f}\right&gt;_{p(\mathbf{ f}|\mathbf{ u}, \mathbf{Z})},\sigma^2\mathbf{I}\right)\right&gt;_{q(\mathbf{Z})}\\&amp; + \text{KL}\left( q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)    
  \end{align}
\]</span></small></li>
<li>Which is analytically tractable for Gaussian <span class="math inline">\(q(\mathbf{Z})\)</span> and some covariance functions.</li>
</ul>
</section>
<section id="required-expectations" class="slide level2">
<h2>Required Expectations</h2>
<ul>
<li>Need expectations under <span class="math inline">\(q(\mathbf{Z})\)</span> of: <small><span class="math display">\[
\log c_i = \frac{1}{2\sigma^2} \left[k_{i, i} - \mathbf{ k}_{i, \mathbf{ u}}^\top \mathbf{K}_{\mathbf{ u}, \mathbf{ u}}^{-1} \mathbf{ k}_{i, \mathbf{ u}}\right]
\]</span></small> and <small><span class="math display">\[
\log \mathcal{N}\left(\mathbf{ y}|\left&lt;\mathbf{ f}\right&gt;_{p(\mathbf{ f}|\mathbf{ u},\mathbf{Y})},\sigma^2\mathbf{I}\right) = -\frac{1}{2}\log 2\pi\sigma^2 - \frac{1}{2\sigma^2}\left(y_i - \mathbf{K}_{\mathbf{ f}, \mathbf{ u}}\mathbf{K}_{\mathbf{ u},\mathbf{ u}}^{-1}\mathbf{ u}\right)^2
\]</span></small></li>
</ul>
</section>
<section id="required-expectations-1" class="slide level2">
<h2>Required Expectations</h2>
<ul>
<li>This requires the expectations <span class="math display">\[
\left&lt;\mathbf{K}_{\mathbf{ f},\mathbf{ u}}\right&gt;_{q(\mathbf{Z})}
\]</span> and <span class="math display">\[
\left&lt;\mathbf{K}_{\mathbf{ f},\mathbf{ u}}\mathbf{K}_{\mathbf{ u},\mathbf{ u}}^{-1}\mathbf{K}_{\mathbf{ u},\mathbf{ f}}\right&gt;_{q(\mathbf{Z})}
\]</span> which can be computed analytically for some covariance functions <span class="citation" data-cites="Damianou:variational15">(Damianou et al., 2016)</span> or through sampling <span class="citation" data-cites="Damianou:thesis2015 Salimbeni:doubly2017">(Damianou, 2015; Salimbeni and Deisenroth, 2017)</span>.</li>
</ul>
</section>
<section id="manifold-relevance-determination" class="slide level2">
<h2>Manifold Relevance Determination</h2>
<!-- SECTION Modeling Multiple 'Views' -->
</section>
<section id="modeling-multiple-views" class="slide level2">
<h2>Modeling Multiple ‘Views’</h2>
<ul>
<li>Single space to model correlations between two different data sources, e.g., images &amp; text, image &amp; pose.</li>
<li>Shared latent spaces: <span class="citation" data-cites="Shon:learning05">Shon et al. (n.d.)</span>;<span class="citation" data-cites="Navaratnam:joint07">Navaratnam et al. (2007)</span>;<span class="citation" data-cites="Ek:pose07">Ek, Torr, et al. (2008)</span></li>
<li>Effective when the `views’ are correlated.</li>
<li>But not all information is shared between both `views’.</li>
<li>PCA applied to concatenated data vs CCA applied to data.</li>
</ul>
<!-- SECTION Shared-Private Factorization -->
</section>
<section id="shared-private-factorization" class="slide level2">
<h2>Shared-Private Factorization</h2>
<ul>
<li><p>In real scenarios, the `views’ are neither fully independent, nor fully correlated.</p></li>
<li><p>Shared models</p>
<ul>
<li>either allow information relevant to a single view to be mixed in the shared signal,</li>
<li>or are unable to model such private information.</li>
</ul></li>
<li><p>Solution: Model shared and private information <span class="citation" data-cites="Klami:group11">Virtanen et al. (n.d.)</span>,<span class="citation" data-cites="Ek:ambiguity08">Ek, Rihan, et al. (2008)</span>,<span class="citation" data-cites="Leen:gplvmcca06">Leen and Fyfe (2006)</span>,<span class="citation" data-cites="Klami:local07">Klami and Kaski (n.d.)</span>,<span class="citation" data-cites="Klami:probabilistic08">Klami and Kaski (2008)</span>,<span class="citation" data-cites="Tucker:battery58">Tucker (1958)</span></p></li>
<li><p>Probabilistic CCA is case when dimensionality of <span class="math inline">\(\mathbf{Z}\)</span> matches <span class="math inline">\(\mathbf{Y}^{(i)}\)</span> (cf Inter Battery Factor Analysis <span class="citation" data-cites="Tucker:battery58">Tucker (1958)</span>).</p></li>
</ul>
<!--frame failure start-->
<!-- SECTION Manifold Relevance Determination -->
</section>
<section id="manifold-relevance-determination-1" class="slide level2">
<h2>Manifold Relevance Determination</h2>
<pre><code>\def\layersep{2cm}
\begin{center}
  \begin{tikzpicture}[node distance=\layersep]</code></pre>
<p> = [text width=4em, text centered] % Draw the input layer nodes / in {1,…,8} % This is the same as writing / in {1/1,2/2,3/3,4/4} (Y-) at (, 0) {<span class="math inline">\(y_\x\)</span>};</p>
<pre><code>% Draw the hidden layer nodes
\foreach \name / \x in {1,...,6}
    \path[xshift=1cm]
        node[latent] (X-\name) at (\x cm, \layersep) {$\latentScalar_\x$};

% Connect every node in the latent layer with every node in the
% data layer.
\foreach \source in {1,...,6}
    \foreach \dest in {1,...,8}
        \draw[-&gt;] (X-\source) -- (Y-\dest);



%  Annotate the layers
\node[annot,left of=X-1, node distance=1cm] (ls) {Latent space};
\node[annot,left of=Y-1, node distance=1cm] (ds) {Data space};</code></pre>
<p>\end{tikzpicture} \end{center}</p>
<!--frame failure end-->
<!--frame failure start-->
<!-- SECTION Shared GP-LVM -->
</section>
<section id="shared-gp-lvm" class="slide level2">
<h2>Shared GP-LVM</h2>
<pre><code>\def\layersep{2cm}
\begin{center}
  \begin{tikzpicture}[node distance=\layersep]</code></pre>
<p> = [text width=4em, text centered] % Draw the input layer nodes / in {1,…,4} % This is the same as writing / in {1/1,2/2,3/3,4/4} (Y-) at (, 0) {<span class="math inline">\(y^{(1)}_\x\)</span>};</p>
<pre><code>\foreach \name / \x in {1,...,4}
% This is the same as writing \foreach \name / \x in {1/1,2/2,3/3,4/4}
    \node[obs] (Z-\name) at (\x+5, 0) {$\dataScalar^{(2)}_\x$};

% Draw the hidden layer nodes
\foreach \name / \x in {1,...,6}
    \path[xshift=2cm]
        node[latent] (X-\name) at (\x cm, \layersep) {$\latentScalar_\x$};

% Connect every node in the latent layer with every node in the
% data layer.
\foreach \source in {1,...,6}
    \foreach \dest in {1,...,4}
        \draw[-&gt;] (X-\source) -- (Y-\dest);

\foreach \source in {1,...,6}
    \foreach \dest in {1,...,4}
        \draw[-&gt;] (X-\source) -- (Z-\dest);


%  Annotate the layers
\node[annot,left of=X-1, node distance=1cm] (ls) {Latent space};
\node[annot,left of=Y-1, node distance=1cm] (ds) {Data space};</code></pre>
<p>\end{tikzpicture} Separate ARD parameters for mappings to <span class="math inline">\(\mathbf{Y}^{(1)}\)</span> and <span class="math inline">\(\mathbf{Y}^{(2)}\)</span>. \end{center}</p>
<!-- SECTION Manifold Relevance Determination Results -->
</section>
<section id="manifold-relevance-determination-results" class="slide level2">
<h2>Manifold Relevance Determination Results</h2>
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="figure">
<div id="yale-faces-0-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gplvm/andreas-deep-talk-0.svg" width="90%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Yale Faces data set shows different people under different lighting conditions
</aside>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<div class="figure">
<div id="yale-faces-1-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gplvm/andreas-deep-talk-1.svg" width="90%" style=" ">
</object>
</div>
</div>
<aside class="notes">
ARD Demonstrates not all of the latent space is used.
</aside>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="figure">
<div id="yale-faces-2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gplvm/andreas-deep-talk-2.svg" width="90%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Other applications include inferring pose from silhouette
</aside>
</section>
<section id="manifold-relevance-determination-2" class="slide level2">
<h2>Manifold Relevance Determination</h2>
<div class="figure">
<div id="manifold-relevance-determination-figure" class="figure-frame">
<iframe width="800" height="600" src="https://www.youtube.com/embed/UvLI8o8z4IU?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
A short video description of the Manifold Relevance Determination method as published at ICML 2012
</aside>
<!--frame end-->
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Damianou:thesis2015">
<p>Damianou, A., 2015. Deep Gaussian processes and variational propagation of uncertainty (PhD thesis). University of Sheffield.</p>
</div>
<div id="ref-Damianou:variational15">
<p>Damianou, A., Titsias, M.K., Lawrence, N.D., 2016. Variational inference for latent variables and uncertain inputs in Gaussian processes. Journal of Machine Learning Research 17.</p>
</div>
<div id="ref-Ek:ambiguity08">
<p>Ek, C.H., Rihan, J., Torr, P.H.S., Rogez, G., Lawrence, N.D., 2008. Ambiguity modeling in latent spaces, in: Popescu-Belis, A., Stiefelhagen, R. (Eds.), Machine Learning for Multimodal Interaction (Mlmi 2008), LNCS. Springer-Verlag, pp. 62–73.</p>
</div>
<div id="ref-Ek:pose07">
<p>Ek, C.H., Torr, P.H.S., Lawrence, N.D., 2008. Gaussian process latent variable models for human pose estimation, in: Popescu-Belis, A., Renals, S., Bourlard, H. (Eds.), Machine Learning for Multimodal Interaction (Mlmi 2007), LNCS. Springer-Verlag, Brno, Czech Republic, pp. 132–143. <a href="https://doi.org/10.1007/978-3-540-78155-4_12">https://doi.org/10.1007/978-3-540-78155-4_12</a></p>
</div>
<div id="ref-Klami:local07">
<p>Klami, A., Kaski, S., n.d. Local dependent components analysis, in:.</p>
</div>
<div id="ref-Klami:probabilistic08">
<p>Klami, A., Kaski, S., 2008. Probabilistic approach to detecting dependencies between data sets. Neurocomputing 72, 39–46.</p>
</div>
<div id="ref-Leen:gplvmcca06">
<p>Leen, G., Fyfe, C., 2006. A Gaussian process latent variable model formulation of canonical correlation analysis, in:. Bruges (Belgium).</p>
</div>
<div id="ref-Navaratnam:joint07">
<p>Navaratnam, R., Fitzgibbon, A., Cipolla, R., 2007. The joint manifold model for semi-supervised multi-valued regression, in: IEEE International Conference on Computer Vision (Iccv). IEEE Computer Society Press.</p>
</div>
<div id="ref-Salimbeni:doubly2017">
<p>Salimbeni, H., Deisenroth, M., 2017. Doubly stochastic variational inference for deep Gaussian processes, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural Information Processing Systems 30. Curran Associates, Inc., pp. 4591–4602.</p>
</div>
<div id="ref-Shon:learning05">
<p>Shon, A.P., Grochow, K., Hertzmann, A., Rao, R.P.N., n.d. Learning shared latent structure for image synthesis and robotic imitation, in:.</p>
</div>
<div id="ref-Tucker:battery58">
<p>Tucker, L.R., 1958. An inter-battery method of factor analysis. Psychometrika 23, 111–136.</p>
</div>
<div id="ref-Klami:group11">
<p>Virtanen, S., Klami, A., Kaski, S., n.d. Bayesian CCA via group sparsity, in:.</p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
