<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Unsupervised Learning wtih Gaussian Processes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="../assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Unsupervised Learning wtih Gaussian Processes</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="venue" style="text-align:center">Gaussian Process Summer School</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<!-- SECTION Probabilistic PCA -->
</section>
<section id="probabilistic-pca" class="slide level2">
<h2>Probabilistic PCA</h2>
<p><span class="math display">\[
\boldsymbol{\Sigma} = \noiseStd^2 \mathbf{I}.
\]</span></p>
<p><span class="math display">\[
p(\mathbf{Y}|\mathbf{W}, \noiseStd^2)
= \prod_{i=1}^n\mathcal{N}\left(\mathbf{ y}_{i, :}|\mathbf{0},\mathbf{W}\mathbf{W}^\top + \noiseStd^2 \mathbf{I}\right)
\]</span></p>
<p><span class="math display">\[
\mathbf{W}= \mathbf{U}\mathbf{L} \mathbf{R}^\top
\]</span></p>
<p><span class="math display">\[
\mathbf{S} = \sum_{i=1}^n(\mathbf{ y}_{i, :} - \boldsymbol{ \mu})(\mathbf{ y}_{i,
:} - \boldsymbol{ \mu})^\top,
\]</span></p>
<p><span class="math display">\[
\ell_i = \sqrt{\lambda_i - \noiseStd^2}
\]</span></p>
</section>
<section id="python-implementation-of-probabilistic-pca" class="slide level2">
<h2>Python Implementation of Probabilistic PCA</h2>
<p>We will now implement this algorithm in python.</p>
<!-- SECTION Posterior for Principal Component Analysis -->
</section>
<section id="posterior-for-principal-component-analysis" class="slide level2">
<h2>Posterior for Principal Component Analysis</h2>
<p><span class="math display">\[
p(\mathbf{ z}_{i, :} | \mathbf{ y}_{i, :})
\]</span></p>
<p><span class="math display">\[
p(\mathbf{ z}_{i, :} | \mathbf{ y}_{i, :}) \propto p(\mathbf{ y}_{i,
:}|\mathbf{W}, \mathbf{ z}_{i, :}, \noiseStd^2) p(\mathbf{ z}_{i, :})
\]</span></p>
<p><span class="math display">\[
\log p(\mathbf{ z}_{i, :} | \mathbf{ y}_{i, :}) = \log p(\mathbf{ y}_{i, :}|\mathbf{W},
\mathbf{ z}_{i, :}, \noiseStd^2) + \log p(\mathbf{ z}_{i, :}) + \text{const}
\]</span></p>
<p><span class="math display">\[
\log p(\mathbf{ z}_{i, :} | \mathbf{ y}_{i, :}) =
-\frac{1}{2\noiseStd^2} (\mathbf{ y}_{i, :} - \mathbf{W}\mathbf{ z}_{i,
:})^\top(\mathbf{ y}_{i, :} - \mathbf{W}\mathbf{ z}_{i, :}) - \frac{1}{2}
\mathbf{ z}_{i, :}^\top \mathbf{ z}_{i, :} + \text{const}
\]</span></p>
</section>
<section id="exercise-0" class="slide level2">
<h2>Exercise 0</h2>
<p>Multiply out the terms in the brackets. Then collect the quadratic term and the linear terms together. Show that the posterior has the form <span class="math display">\[
\mathbf{ z}_{i, :} | \mathbf{W}\sim \mathcal{N}\left(\boldsymbol{ \mu}_x,\mathbf{C}_x\right)
\]</span> where <span class="math display">\[
\mathbf{C}_x = \left(\noiseStd^{-2}
\mathbf{W}^\top\mathbf{W}+ \mathbf{I}\right)^{-1}
\]</span> and <span class="math display">\[
\boldsymbol{ \mu}_x
= \mathbf{C}_x \noiseStd^{-2}\mathbf{W}^\top \mathbf{ y}_{i, :} 
\]</span> Compare this to the posterior for the Bayesian linear regression from last week, do they have similar forms? What matches and what differs?</p>
</section>
<section id="python-implementation-of-the-posterior" class="slide level2">
<h2>Python Implementation of the Posterior</h2>
<p>Now let’s implement the system in code.</p>
</section>
<section id="exercise-1" class="slide level2">
<h2>Exercise 1</h2>
<p>Use the values for <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\noiseStd^2\)</span> you have computed, along with the data set <span class="math inline">\(\mathbf{Y}\)</span> to compute the posterior density over <span class="math inline">\(\mathbf{Z}\)</span>. Write a function of the form</p>
<p>python mu_x, C_x = posterior(Y, W, sigma2)} where <code>mu_x</code> and <code>C_x</code> are the posterior mean and posterior covariance for the given <span class="math inline">\(\mathbf{Y}\)</span>.</p>
<p>Don’t forget to subtract the mean of the data <code>Y</code> inside your function before computing the posterior: remember we assumed at the beginning of our analysis that the data had been centred (i.e. the mean was removed).}{20}</p>
</section>
<section>
<section id="numerically-stable-and-efficient-version" class="title-slide slide level1">
<h1>Numerically Stable and Efficient Version</h1>
<p>Just as we saw for  and  computation of a matrix such as <span class="math inline">\(\mathbf{Y}^\top\mathbf{Y}\)</span> (or its centred version) can be a bad idea in terms of loss of numerical accuracy. Fortunately, we can find the eigenvalues and eigenvectors of the matrix <span class="math inline">\(\mathbf{Y}^\top\mathbf{Y}\)</span> without direct computation of the matrix. This can be done with the <a href="http://en.wikipedia.org/wiki/Singular_value_decomposition"><em>singular value decomposition</em></a>. The singular value decompsition takes a matrix, <span class="math inline">\(\mathbf{Z}\)</span> and represents it in the form, <span class="math display">\[
\mathbf{Z} = \mathbf{U}\boldsymbol{\Lambda}\mathbf{V}^\top
\]</span> where <span class="math inline">\(\mathbf{U}\)</span> is a matrix of orthogonal vectors in the columns, meaning <span class="math inline">\(\mathbf{U}^\top\mathbf{U} = \mathbf{I}\)</span>. It has the same number of rows and columns as <span class="math inline">\(\mathbf{Z}\)</span>. The matrices <span class="math inline">\(\mathbf{\Lambda}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> are both square with dimensionality given by the number of columns of <span class="math inline">\(\mathbf{Z}\)</span>. The matrix <span class="math inline">\(\mathbf{\Lambda}\)</span> is <em>diagonal</em> and <span class="math inline">\(\mathbf{V}\)</span> is an orthogonal matrix so <span class="math inline">\(\mathbf{V}^\top\mathbf{V} = \mathbf{V}\mathbf{V}^\top = \mathbf{I}\)</span>. The eigenvalues of the matrix <span class="math inline">\(\mathbf{Y}^\top\mathbf{Y}\)</span> are then given by the singular values of the matrix <span class="math inline">\(\mathbf{Y}^\top\)</span> squared and the eigenvectors are given by <span class="math inline">\(\mathbf{U}\)</span>.</p>
</section>
<section id="solution-for-mathbfw" class="slide level2">
<h2>Solution for <span class="math inline">\(\mathbf{W}\)</span></h2>
<p>Given the singular value decomposition of <span class="math inline">\(\mathbf{Y}\)</span> then we have <span class="math display">\[
\mathbf{W}=
\mathbf{U}\mathbf{L}\mathbf{R}^\top
\]</span> where <span class="math inline">\(\mathbf{R}\)</span> is an arbitrary rotation matrix. This implies that the posterior is given by <span class="math display">\[
\mathbf{C}_x =
\left[\noiseStd^{-2}\mathbf{R}\mathbf{L}^2\mathbf{R}^\top + \mathbf{I}\right]^{-1}
\]</span> because <span class="math inline">\(\mathbf{U}^\top \mathbf{U} = \mathbf{I}\)</span>. Since, by convention, we normally take <span class="math inline">\(\mathbf{R} = \mathbf{I}\)</span> to ensure that the principal components are orthonormal we can write <span class="math display">\[
\mathbf{C}_x = \left[\noiseStd^{-2}\mathbf{L}^2 +
\mathbf{I}\right]^{-1}
\]</span> which implies that <span class="math inline">\(\mathbf{C}_x\)</span> is actually diagonal with elements given by <span class="math display">\[
c_i = \frac{\noiseStd^2}{\noiseStd^2 + \ell^2_i}
\]</span> and allows us to write <span class="math display">\[
\boldsymbol{ \mu}_x = [\mathbf{L}^2 + \noiseStd^2
\mathbf{I}]^{-1} \mathbf{L} \mathbf{U}^\top \mathbf{ y}_{i, :}
\]</span> <span class="math display">\[
\boldsymbol{ \mu}_x = \mathbf{D}\mathbf{U}^\top \mathbf{ y}_{i, :}
\]</span> where <span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix with diagonal elements given by <span class="math inline">\(d_{i} = \frac{\ell_i}{\noiseStd^2 + \ell_i^2}\)</span>.</p>
</section>
<section id="difficulty-for-probabilistic-approaches" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="nonlinear-mapping-3d-plot-figure" class="figure-frame">
<object class="svgplot 80%" data="../slides/diagrams/dimred/nonlinear-mapping-3d-plot.svg" width="center" style=" ">
</object>
</div>
</div>
<aside class="notes">
A two dimensional grid mapped into three dimensions to form a two dimensional manifold.
</aside>
</section>
<section id="difficulty-for-probabilistic-approaches-1" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="non-linear-mapping-2d-plot-figure" class="figure-frame">
<object class="svgplot 80%" data="../slides/diagrams/dimred/nonlinear-mapping-2d-plot.svg" width="center" style=" ">
</object>
</div>
</div>
<aside class="notes">
A one dimensional line mapped into two dimensions by two separate independent functions. Each point can be mapped exactly through the mappings.
</aside>
</section>
<section id="difficulty-for-probabilistic-approaches-2" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="gaussian-through-nonlinear-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/gaussian-through-nonlinear.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A Gaussian density over the input of a non linear function leads to a very non Gaussian output. Here the output is multimodal.
</aside>
</section>
<section id="getting-started-and-downloading-data" class="slide level2">
<h2>Getting Started and Downloading Data</h2>
<p>[which = [0,1,2,6,7,9] # which digits to work on data = pods.datasets.decampos_digits(which_digits=which) Y = data[‘Y’] labels = data[‘str_lbls’]}</p>
</section>
<section id="principal-component-analysis" class="slide level2">
<h2>Principal Component Analysis</h2>
<ul>
<li>What is the right shape <span class="math inline">\(n \times p\)</span> to use?</li>
</ul>
</section>
<section id="gaussian-process-latent-variable-model" class="slide level2">
<h2>Gaussian Process Latent Variable Model</h2>

</section>
<section id="bayesian-gplvm" class="slide level2">
<h2>Bayesian GPLVM</h2>

</section>
<section id="preoptimized-model" class="slide level2">
<h2>Preoptimized Model</h2>
</section>
<section id="multiview-learning-manifold-relevance-determination" class="slide level2">
<h2>Multiview Learning: Manifold Relevance Determination</h2>

</section>
<section id="interactive-demo-for-use-outside-the-notepad" class="slide level2">
<h2>Interactive Demo: For Use Outside the Notepad</h2>
</section>
<section id="observations" class="slide level2">
<h2>Observations</h2>
<ul>
<li>We tend to obtain more “strange” outputs when sampling from latent space areas away from the training inputs.</li>
<li>When sampling from the two dominant latent dimensions (the ones corresponding to large scales) we differentiate between all digits. Also note that projecting the latent space into the two dominant dimensions better separates the classes.</li>
<li>When sampling from less dominant latent dimensions the outputs vary in a more subtle way.</li>
</ul>
</section>
<section id="questions" class="slide level2">
<h2>Questions</h2>
<ul>
<li>Can you see a difference in the ARD parameters to the non Bayesian GPLVM?</li>
<li>How does the Bayesian GPLVM allow the ARD parameters of the RBF kernel magnify the two first dimensions?</li>
<li>Is Bayesian GPLVM better in differentiating between different kinds of digits?</li>
<li>Why does the starting noise variance have to be lower then the variance of the observed values?</li>
<li>How come we use the lowest variance when using a linear kernel, but the highest lengtscale when using an RBF kernel?</li>
</ul>
</section>
<section id="example-latent-doodle-space" class="slide level2">
<h2>Example: Latent Doodle Space</h2>
<div class="figure">
<div id="latent-doodle-space-figure" class="figure-frame">
<iframe width height src="https://player.vimeo.com/video/3235882#t=" frameborder="0" allow="autoplay; fullscreen" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
The latent doodle space idea of <span class="citation" data-cites="Baxter:doodle06">Baxter and Anjyo (2006)</span> manages to build a smooth mapping across very sparse data.
</aside>
</section>
<section id="example-latent-doodle-space-1" class="slide level2">
<h2>Example: Latent Doodle Space</h2>
<p><strong>Generalization with much less Data than Dimensions</strong></p>
<ul>
<li><p>Powerful uncertainly handling of GPs leads to surprising properties.</p></li>
<li><p>Non-linear models can be used where there are fewer data points than dimensions <em>without overfitting</em>.</p></li>
</ul>
<p><span style="text-align:right"><span class="citation" data-cites="Baxter:doodle06">(Baxter and Anjyo, 2006)</span></span></p>
</section>
<section id="continuous-character-control" class="slide level2">
<h2>Continuous Character Control</h2>
<p><span style="text-align:right"><span class="citation" data-cites="Levine:control12">Levine et al. (2012)</span></span> - Graph diffusion prior for enforcing connectivity between motions. <span class="math display">\[\log p(\mathbf{X}) = w_c \sum_{i,j} \log K_{ij}^d\]</span> with the graph diffusion kernel <span class="math inline">\(\mathbf{K}^d\)</span> obtain from <span class="math display">\[K_{ij}^d = \exp(\beta \mathbf{H})
    \qquad \text{with} \qquad \mathbf{H} = -\mathbf{T}^{-1/2} \mathbf{L} \mathbf{T}^{-1/2}\]</span> the graph Laplacian, and <span class="math inline">\(\mathbf{T}\)</span> is a diagonal matrix with <span class="math inline">\(T_{ii} = \sum_j w(\mathbf{ x}_i, \mathbf{ x}_j)\)</span>, <span class="math display">\[L_{ij} = \begin{cases} \sum_k w(\mathbf{ x}_i,\mathbf{ x}_k) &amp; \text{if $i=j$}
    \\
    -w(\mathbf{ x}_i,\mathbf{ x}_j) &amp;\text{otherwise.}
    \end{cases}\]</span> and <span class="math inline">\(w(\mathbf{ x}_i,\mathbf{ x}_j) = || \mathbf{ x}_i - \mathbf{ x}_j||^{-p}\)</span> measures similarity.</p>
</section>
<section id="character-control-results" class="slide level2">
<h2>Character Control: Results</h2>
<div class="figure">
<div id="charcter-control-gplvm-figure" class="figure-frame">
<iframe width="800" height="600" src="https://www.youtube.com/embed/hr3pdDl5IAg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
Character control in the latent space described the the GP-LVM <span class="citation" data-cites="Levine:control12">Levine et al. (2012)</span>
</aside>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Baxter:doodle06">
<p>Baxter, W.V., Anjyo, K.-I., 2006. Latent doodle space, in: EUROGRAPHICS. Vienna, Austria, pp. 477–485. <a href="https://doi.org/10.1111/j.1467-8659.2006.00967.x">https://doi.org/10.1111/j.1467-8659.2006.00967.x</a></p>
</div>
<div id="ref-Levine:control12">
<p>Levine, S., Wang, J.M., Haraux, A., Popović, Z., Koltun, V., 2012. Continuous character control with low-dimensional embeddings. ACM Transactions on Graphics (SIGGRAPH 2012) 31.</p>
</div>
</div>
</section></section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
