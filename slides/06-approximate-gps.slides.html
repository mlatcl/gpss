<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Approximate Gaussian Processes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="../assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Approximate Gaussian Processes</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="venue" style="text-align:center">Gaussian Process Summer School</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
</section>
<section id="approximations" class="slide level2">
<h2>Approximations</h2>
<div class="centered" style="">
<img class="" src="../slides/diagrams/sparse-gps-1.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="caption" style="">
Figure: Image credit: Kai Arulkumaran
</div>
</section>
<section id="approximations-1" class="slide level2">
<h2>Approximations</h2>
<div class="centered" style="">
<img class="" src="../slides/diagrams/sparse-gps-2.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="caption" style="">
Figure: Image credit: Kai Arulkumaran
</div>
</section>
<section id="approximations-2" class="slide level2">
<h2>Approximations</h2>
<div class="centered" style="">
<img class="" src="../slides/diagrams/sparse-gps-3.png" width="45%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="caption" style="">
Figure: Image credit: Kai Arulkumaran
</div>
</section>
<section id="approximations-3" class="slide level2">
<h2>Approximations</h2>
<div class="centered" style="">
<img class="" src="../slides/diagrams/sparse-gps-4.png" width="45%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="caption" style="">
Figure: Image credit: Kai Arulkumaran
</div>
</section>
<section id="approximate-gaussian-processes" class="slide level2">
<h2>Approximate Gaussian Processes</h2>
</section>
<section id="low-rank-motivation" class="slide level2">
<h2>Low Rank Motivation</h2>
<ul>
<li><p>Inference in a GP has the following demands:</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: right;">Complexity:</td>
<td style="text-align: left;"><span class="math inline">\(\mathcal{O}(n^3)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: right;">Storage:</td>
<td style="text-align: left;"><span class="math inline">\(\mathcal{O}(n^2)\)</span></td>
</tr>
</tbody>
</table></li>
<li><p>Inference in a low rank GP has the following demands:</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: right;">Complexity:</td>
<td style="text-align: left;"><span class="math inline">\(\mathcal{O}(nm^2)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: right;">Storage:</td>
<td style="text-align: left;"><span class="math inline">\(\mathcal{O}(nm)\)</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(m\)</span> is a user chosen parameter.</p></li>
</ul>
<p><small><span class="citation" data-cites="Snelson:pseudo05">Snelson and Ghahramani (n.d.)</span>,<span class="citation" data-cites="Quinonero:unifying05">Quiñonero Candela and Rasmussen (2005)</span>,<span class="citation" data-cites="Lawrence:larger07">Lawrence (n.d.)</span>,<span class="citation" data-cites="Titsias:variational09">Titsias (n.d.)</span>,<span class="citation" data-cites="Thang:unifying17">Bui et al. (2017)</span></small></p>
</section>
<section id="variational-compression" class="slide level2">
<h2>Variational Compression</h2>
<ul>
<li>Inducing variables are a compression of the real observations.</li>
<li>They are like pseudo-data. They can be in space of <span class="math inline">\(\mathbf{ f}\)</span> or a space that is related through a linear operator <span class="citation" data-cites="Alvarez:efficient10">(Álvarez et al., 2010)</span> — e.g. a gradient or convolution.</li>
</ul>
</section>
<section id="variational-compression-ii" class="slide level2">
<h2>Variational Compression II</h2>
<ul>
<li>Introduce <em>inducing</em> variables.</li>
<li>Compress information into the inducing variables and avoid the need to store all the data.</li>
<li>Allow for scaling e.g. stochastic variational <span class="citation" data-cites="Hensman:bigdata13">Hensman et al. (n.d.)</span> or parallelization <span class="citation" data-cites="Gal:distributed14">Gal et al. (n.d.)</span>,<span class="citation" data-cites="Dai:gpu14">Dai et al. (2014)</span>, <span class="citation" data-cites="Seeger:auto17">Seeger et al. (2017)</span></li>
</ul>
</section>
<section id="nonparametric-gaussian-processes" class="slide level2">
<h2>Nonparametric Gaussian Processes</h2>
<ul>
<li><p>We’ve seen how we go from parametric to non-parametric.</p></li>
<li><p>The limit implies infinite dimensional <span class="math inline">\(\mathbf{ w}\)</span>.</p></li>
<li><p>Gaussian processes are generally non-parametric: combine data with covariance function to get model.</p></li>
<li><p>This representation <em>cannot</em> be summarized by a parameter vector of a fixed size.</p></li>
</ul>
</section>
<section id="the-parametric-bottleneck" class="slide level2">
<h2>The Parametric Bottleneck</h2>
<ul>
<li><p>Parametric models have a representation that does not respond to increasing training set size.</p></li>
<li><p>Bayesian posterior distributions over parameters contain the information about the training data.</p>
<ul>
<li><p>Use Bayes’ rule from training data, <span class="math inline">\(p\left(\mathbf{ w}|\mathbf{ y}, \mathbf{X}\right)\)</span>,</p></li>
<li><p>Make predictions on test data <span class="math display">\[p\left(y_*|\mathbf{X}_*, \mathbf{ y}, \mathbf{X}\right) = \int
        p\left(y_*|\mathbf{ w},\mathbf{X}_*\right)p\left(\mathbf{ w}|\mathbf{ y},
          \mathbf{X})\text{d}\mathbf{ w}\right).\]</span></p></li>
</ul></li>
<li><p><span class="math inline">\(\mathbf{ w}\)</span> becomes a bottleneck for information about the training set to pass to the test set.</p></li>
<li><p>Solution: increase <span class="math inline">\(m\)</span> so that the bottleneck is so large that it no longer presents a problem.</p></li>
<li><p>How big is big enough for <span class="math inline">\(m\)</span>? Non-parametrics says <span class="math inline">\(m\rightarrow \infty\)</span>.</p></li>
</ul>
</section>
<section id="the-parametric-bottleneck-1" class="slide level2">
<h2>The Parametric Bottleneck</h2>
<ul>
<li>Now no longer possible to manipulate the model through the standard parametric form.</li>
</ul>
<div class="fragment">
<ul>
<li>However, it <em>is</em> possible to express <em>parametric</em> as GPs: <span class="math display">\[k\left(\mathbf{ x}_i,\mathbf{ x}_j\right)=\phi_:\left(\mathbf{ x}_i\right)^\top\phi_:\left(\mathbf{ x}_j\right).\]</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>These are known as degenerate covariance matrices.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Their rank is at most <span class="math inline">\(m\)</span>, non-parametric models have full rank covariance matrices.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Most well known is the “linear kernel”, <span class="math inline">\(k(\mathbf{ x}_i, \mathbf{ x}_j) = \mathbf{ x}_i^\top\mathbf{ x}_j\)</span>.</li>
</ul>
</div>
</section>
<section id="making-predictions" class="slide level2">
<h2>Making Predictions</h2>
<ul>
<li>For non-parametrics prediction at new points <span class="math inline">\(\mathbf{ f}_*\)</span> is made by conditioning on <span class="math inline">\(\mathbf{ f}\)</span> in the joint distribution.</li>
</ul>
<div class="fragment">
<ul>
<li>In GPs this involves combining the training data with the covariance function and the mean function.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Parametric is a special case when conditional prediction can be summarized in a <em>fixed</em> number of parameters.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Complexity of parametric model remains fixed regardless of the size of our training data set.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>For a non-parametric model the required number of parameters grows with the size of the training data.</li>
</ul>
</div>
</section>
<section id="information-capture" class="slide level2">
<h2>Information capture</h2>
<ul>
<li>Everything we want to do with a GP involves marginalising <span class="math inline">\(\mathbf{ f}\)</span>
<ul>
<li>Predictions</li>
<li>Marginal likelihood</li>
<li>Estimating covariance parameters</li>
</ul></li>
<li>The posterior of <span class="math inline">\(\mathbf{ f}\)</span> is the central object. This means inverting <span class="math inline">\(\mathbf{K}_{\mathbf{ f}\mathbf{ f}}\)</span>.</li>
</ul>
</section>
<section id="nystrom-method" class="slide level2">
<h2>Nystr"om Method</h2>
<img class="negate" src="../slides/diagrams/cov_approx.png" width="90%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
<div class="caption" style="">
Figure: Figure originally from presentation by Ed Snelson at NIPS
</div>
<p><span class="math display">\[
\mathbf{K}_{\mathbf{ f}\mathbf{ f}}\approx \mathbf{Q}_{\mathbf{ f}\mathbf{ f}}= \mathbf{K}_{\mathbf{ f}\mathbf{ u}}\mathbf{K}_{\mathbf{ u}\mathbf{ u}}^{-1}\mathbf{K}_{\mathbf{ u}\mathbf{ f}}
\]</span></p>
</section>
<section id="section" class="slide level2">
<h2></h2>
<table>
<tr>
<td width="30%">
<span class="math display">\[\mathbf{X},\,\mathbf{ y}\]</span>
</td>
<td width="70%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/nomenclature1.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<table>
<tr>
<td width="30%">
<span class="math display">\[\mathbf{X},\,\mathbf{ y}\]</span> <span class="math display">\[{\color{yellow} f(\mathbf{ x})} \sim {\mathcal GP}\]</span>
</td>
<td width="70%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/nomenclature2.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<table>
<tr>
<td width="30%">
<span class="math display">\[\mathbf{X},\,\mathbf{ y}\]</span> <span class="math display">\[f(\mathbf{ x}) \sim {\mathcal GP}\]</span><span class="math display">\[p({\color{yellow} \mathbf{ f}}) = \mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{ f}\mathbf{ f}}\right)\]</span>
</td>
<td width="70%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/nomenclature3.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<table>
<tr>
<td width="30%">
<span class="math display">\[
\mathbf{X},\,\mathbf{ y}\]</span> <span class="math display">\[f(\mathbf{ x}) \sim {\mathcal GP}
\]</span> <span class="math display">\[
p(\mathbf{ f}) = \mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{ f}\mathbf{ f}}\right)
\]</span> <span class="math display">\[p( \mathbf{ f}|\mathbf{ y},\mathbf{X})
\]</span>
</td>
<td width="70%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/nomenclature3a.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</section>
<section id="introducing-mathbf-u" class="slide level2">
<h2>Introducing <span class="math inline">\(\mathbf{ u}\)</span></h2>
<p>Take an extra <span class="math inline">\(m\)</span> points on the function, <span class="math inline">\(\mathbf{ u}= f(\mathbf{Z})\)</span>. <span class="math display">\[p(\mathbf{ y},\mathbf{ f},\mathbf{ u}) = p(\mathbf{ y}|\mathbf{ f}) p(\mathbf{ f}|\mathbf{ u}) p(\mathbf{ u})\]</span></p>
</section>
<section id="introducing-mathbf-u-1" class="slide level2">
<h2>Introducing <span class="math inline">\(\mathbf{ u}\)</span></h2>
<p><img class="negate" src="../slides/diagrams/cov_inducing_withX.png" width="60%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></p>
</section>
<section id="introducing-mathbf-u-2" class="slide level2">
<h2>Introducing <span class="math inline">\(\mathbf{ u}\)</span></h2>
<p>Take and extra <span class="math inline">\(M\)</span> points on the function, <span class="math inline">\(\mathbf{ u}= f(\mathbf{Z})\)</span>. <span class="math display">\[p(\mathbf{ y},\mathbf{ f},\mathbf{ u}) = p(\mathbf{ y}|\mathbf{ f}) p(\mathbf{ f}|\mathbf{ u}) p(\mathbf{ u})\]</span> <span class="math display">\[\begin{aligned}
    p(\mathbf{ y}|\mathbf{ f}) &amp;= \mathcal{N}\left(\mathbf{ y}|\mathbf{ f},\sigma^2 \mathbf{I}\right)\\
    p(\mathbf{ f}|\mathbf{ u}) &amp;= \mathcal{N}\left(\mathbf{ f}| \mathbf{K}_{\mathbf{ f}\mathbf{ u}}\mathbf{K}_{\mathbf{ u}\mathbf{ u}}^{-1}\mathbf{ u}, \tilde{\mathbf{K}}\right)\\
    p(\mathbf{ u}) &amp;= \mathcal{N}\left(\mathbf{ u}| \mathbf{0},\mathbf{K}_{\mathbf{ u}\mathbf{ u}}\right)
  \end{aligned}\]</span></p>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<table>
<tr>
<td width="30%">
<span class="math display">\[\mathbf{X},\,\mathbf{ y}\]</span> <span class="math display">\[f(\mathbf{ x}) \sim {\mathcal GP}\]</span> <span class="math display">\[p(\mathbf{ f}) = \mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{ f}\mathbf{ f}}\right)\]</span> <span class="math display">\[p(\mathbf{ f}|\mathbf{ y},\mathbf{X})\]</span>
</td>
<td width="70%">
<img class="negate" src="../slides/diagrams/nomenclature4" width="90%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</td>
</tr>
</table>
<p><span class="math display">\[
\begin{align}
                           &amp;\qquad\mathbf{Z}, \mathbf{ u}\\                      &amp;p({\color{red} \mathbf{ u}})  = \mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{ u}\mathbf{ u}}\right)\end{align}
\]</span></p>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<table>
<tr>
<td width="30%">
<span class="math display">\[\mathbf{X},\,\mathbf{ y}\]</span> <span class="math display">\[f(\mathbf{ x}) \sim {\mathcal GP}\]</span> <span class="math display">\[p(\mathbf{ f}) = \mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{ f}\mathbf{ f}}\right)\]</span> <span class="math display">\[p(\mathbf{ f}|\mathbf{ y},\mathbf{X})\]</span> <span class="math display">\[p(\mathbf{ u})  = \mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{ u}\mathbf{ u}}\right)\]</span> <span class="math display">\[\widetilde p({\color{red}\mathbf{ u}}|\mathbf{ y},\mathbf{X})\]</span>
</td>
<td width="70%">
<img class="negate" src="../slides/diagrams/nomenclature5.png" width="90%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</td>
</tr>
</table>
</section>
<section id="the-alternative-posterior" class="slide level2">
<h2>The alternative posterior</h2>
<p><span>Instead of doing</span> <span class="math display">\[
p(\mathbf{ f}|\mathbf{ y},\mathbf{X}) = \frac{p(\mathbf{ y}|\mathbf{ f})p(\mathbf{ f}|\mathbf{X})}{\int p(\mathbf{ y}|\mathbf{ f})p(\mathbf{ f}|\mathbf{X}){\text{d}\mathbf{ f}}}
\]</span> <span>We’ll do</span> <span class="math display">\[
p(\mathbf{ u}|\mathbf{ y},\mathbf{Z}) = \frac{p(\mathbf{ y}|\mathbf{ u})p(\mathbf{ u}|\mathbf{Z})}{\int p(\mathbf{ y}|\mathbf{ u})p(\mathbf{ u}|\mathbf{Z}){\text{d}\mathbf{ u}}}
\]</span> </p>
<!--Flexible Parametric Approximation-->
</section>
<section id="parametric-but-non-parametric" class="slide level2">
<h2>Parametric but Non-parametric</h2>
<ul>
<li>Augment with a vector of <em>inducing</em> variables, <span class="math inline">\(\mathbf{ u}\)</span>.</li>
<li>Form a variational lower bound on true likelihood.</li>
<li>Bound <em>factorizes</em> given inducing variables.</li>
<li>Inducing variables appear in bound similar to parameters in a parametric model.</li>
<li><em>But</em> number of inducing variables can be changed at run time.</li>
</ul>
</section>
<section id="inducing-variable-approximations" class="slide level2">
<h2>Inducing Variable Approximations</h2>
<ul>
<li>Date back to {<span class="citation" data-cites="Williams:nystrom00">Williams and Seeger (n.d.)</span>; <span class="citation" data-cites="Smola:sparsegp00">Smola and Bartlett (n.d.)</span>; <span class="citation" data-cites="Csato:sparse02">Csató and Opper (2002)</span>; <span class="citation" data-cites="Seeger:fast03">Seeger et al. (n.d.)</span>; <span class="citation" data-cites="Snelson:pseudo05">Snelson and Ghahramani (n.d.)</span>}. See {<span class="citation" data-cites="Quinonero:unifying05">Quiñonero Candela and Rasmussen (2005)</span>; <span class="citation" data-cites="Thang:unifying17">Bui et al. (2017)</span>} for reviews.</li>
<li>We follow variational perspective of {<span class="citation" data-cites="Titsias:variational09">Titsias (n.d.)</span>}.</li>
<li>This is an augmented variable method, followed by a collapsed variational approximation {<span class="citation" data-cites="King:klcorrection06">King and Lawrence (n.d.)</span>; <span class="citation" data-cites="Hensman:fast12">Hensman et al. (2012)</span>}.</li>
</ul>
</section>
<section id="augmented-variable-model-not-wrong-but-useful" class="slide level2">
<h2>Augmented Variable Model: Not Wrong but Useful?</h2>
<table>
<tr>
<td width="60%">
</td>
<td width="40%">
</td>
</tr>
</table>
</section>
<section id="variational-bound-on-pmathbf-ymathbf-u" class="slide level2">
<h2>Variational Bound on <span class="math inline">\(p(\mathbf{ y}|\mathbf{ u})\)</span></h2>
<p><span class="math display">\[
\begin{aligned}
    \log p(\mathbf{ y}|\mathbf{ u}) &amp; = \log \int p(\mathbf{ y}|\mathbf{ f}) p(\mathbf{ f}|\mathbf{ u}) \text{d}\mathbf{ f}\\ &amp; = \int q(\mathbf{ f}) \log \frac{p(\mathbf{ y}|\mathbf{ f}) p(\mathbf{ f}|\mathbf{ u})}{q(\mathbf{ f})}\text{d}\mathbf{ f}+ \text{KL}\left( q(\mathbf{ f})\,\|\,p(\mathbf{ f}|\mathbf{ y}, \mathbf{ u}) \right).
\end{aligned}
\]</span></p>
</section>
<section id="choose-form-for-qcdot" class="slide level2">
<h2>Choose form for <span class="math inline">\(q(\cdot)\)</span></h2>
<ul>
<li>Set <span class="math inline">\(q(\mathbf{ f})=p(\mathbf{ f}|\mathbf{ u})\)</span>, <span class="math display">\[
\log p(\mathbf{ y}|\mathbf{ u}) \geq \int p(\mathbf{ f}|\mathbf{ u}) \log p(\mathbf{ y}|\mathbf{ f})\text{d}\mathbf{ f}.
\]</span> <span class="math display">\[
p(\mathbf{ y}|\mathbf{ u}) \geq \exp \int p(\mathbf{ f}|\mathbf{ u}) \log p(\mathbf{ y}|\mathbf{ f})\text{d}\mathbf{ f}.
\]</span> <span style="text-align:right"><span class="citation" data-cites="Titsias:variational09">(Titsias, n.d.)</span></span></li>
</ul>
</section>
<section id="optimal-compression-in-inducing-variables" class="slide level2">
<h2>Optimal Compression in Inducing Variables</h2>
<ul>
<li><p>Maximizing lower bound minimizes the KL divergence (information gain): <span class="math display">\[
\text{KL}\left( p(\mathbf{ f}|\mathbf{ u})\,\|\,p(\mathbf{ f}|\mathbf{ y}, \mathbf{ u}) \right) = \int p(\mathbf{ f}|\mathbf{ u}) \log \frac{p(\mathbf{ f}|\mathbf{ u})}{p(\mathbf{ f}|\mathbf{ y}, \mathbf{ u})}\text{d}\mathbf{ u}
\]</span></p></li>
<li><p>This is minimized when the information stored about <span class="math inline">\(\mathbf{ y}\)</span> is stored already in <span class="math inline">\(\mathbf{ u}\)</span>.</p></li>
<li><p>The bound seeks an <em>optimal compression</em> from the <em>information gain</em> perspective.</p></li>
<li><p>If <span class="math inline">\(\mathbf{ u}= \mathbf{ f}\)</span> bound is exact (<span class="math inline">\(\mathbf{ f}\)</span> <span class="math inline">\(d\)</span>-separates <span class="math inline">\(\mathbf{ y}\)</span> from <span class="math inline">\(\mathbf{ u}\)</span>).</p></li>
</ul>
</section>
<section id="choice-of-inducing-variables" class="slide level2">
<h2>Choice of Inducing Variables</h2>
<ul>
<li>Free to choose whatever heuristics for the inducing variables.</li>
<li>Can quantify which heuristics perform better through checking lower bound.</li>
</ul>
<p><span class="math display">\[
\begin{bmatrix}
\mathbf{ f}\\
\mathbf{ u}
\end{bmatrix} \sim \mathcal{N}\left(\mathbf{0},\mathbf{K}\right)
\]</span> with <span class="math display">\[
\mathbf{K}=
\begin{bmatrix}
\mathbf{K}_{\mathbf{ f}\mathbf{ f}}&amp; \mathbf{K}_{\mathbf{ f}\mathbf{ u}}\\
\mathbf{K}_{\mathbf{ u}\mathbf{ f}}&amp; \mathbf{K}_{\mathbf{ u}\mathbf{ u}}
\end{bmatrix}
\]</span></p>
</section>
<section id="variational-compression-1" class="slide level2">
<h2>Variational Compression</h2>
<ul>
<li>Inducing variables are a compression of the real observations.</li>
<li>They are like pseudo-data. They can be in space of <span class="math inline">\(\mathbf{ f}\)</span> or a space that is related through a linear operator <span class="citation" data-cites="Alvarez:efficient10">(Álvarez et al., 2010)</span> — e.g. a gradient or convolution.</li>
</ul>
</section>
<section id="variational-compression-ii-1" class="slide level2">
<h2>Variational Compression II</h2>
<ul>
<li>Resulting algorithms reduce computational complexity.</li>
<li>Also allow deployment of more standard scaling techniques.</li>
<li>E.g. Stochastic variational inference <span class="citation" data-cites="Hoffman:stochastic12">Hoffman et al. (2012)</span></li>
<li>Allow for scaling e.g. stochastic variational <span class="citation" data-cites="Hensman:bigdata13">Hensman et al. (n.d.)</span> or parallelization <span class="citation" data-cites="Gal:distributed14 Dai:gpu14 Seeger:auto17">(Dai et al., 2014; Gal et al., n.d.; Seeger et al., 2017)</span></li>
</ul>
</section>
<section id="factorizing-likelihoods" class="slide level2">
<h2>Factorizing Likelihoods</h2>
<ul>
<li><p>If the likelihood, <span class="math inline">\(p(\mathbf{ y}|\mathbf{ f})\)</span>, factorizes     </p></li>
<li><p>&lt;8-&gt; Then the bound factorizes.</p></li>
<li><p>&lt;10-&gt; Now need a choice of distributions for <span class="math inline">\(\mathbf{ f}\)</span> and <span class="math inline">\(\mathbf{ y}|\mathbf{ f}\)</span> …</p></li>
</ul>
</section>
<section id="inducing-variables" class="slide level2">
<h2>Inducing Variables</h2>
<ul>
<li>Choose to go a different way.</li>
<li>Introduce a set of auxiliary variables, <span class="math inline">\(\mathbf{ u}\)</span>, which are <span class="math inline">\(m\)</span> in length.</li>
<li>They are like “artificial data”.</li>
<li>Used to <em>induce</em> a distribution: <span class="math inline">\(q(\mathbf{ u}|\mathbf{ y})\)</span></li>
</ul>
</section>
<section id="making-parameters-non-parametric" class="slide level2">
<h2>Making Parameters non-Parametric</h2>
<ul>
<li><p>Introduce variable set which is <em>finite</em> dimensional. <span class="math display">\[
p(\mathbf{ y}^*|\mathbf{ y}) \approx \int p(\mathbf{ y}^*|\mathbf{ u}) q(\mathbf{ u}|\mathbf{ y}) \text{d}\mathbf{ u}
\]</span></p></li>
<li><p>But dimensionality of <span class="math inline">\(\mathbf{ u}\)</span> can be changed to improve approximation.</p></li>
</ul>
</section>
<section id="variational-compression-2" class="slide level2">
<h2>Variational Compression</h2>
<ul>
<li>Model for our data, <span class="math inline">\(\mathbf{ y}\)</span>
<table>
<tr>
<td width>
<span class="math display">\[p(\mathbf{ y})\]</span>
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/py.svg" width style=" ">
</object>
</td>
</tr>
</table></li>
</ul>
</section>
<section id="variational-compression-3" class="slide level2">
<h2>Variational Compression</h2>
<ul>
<li>Prior density over <span class="math inline">\(\mathbf{ f}\)</span>. Likelihood relates data, <span class="math inline">\(\mathbf{ y}\)</span>, to <span class="math inline">\(\mathbf{ f}\)</span>.
<table>
<tr>
<td width>
<span class="math display">\[p(\mathbf{ y})=\int p(\mathbf{ y}|\mathbf{ f})p(\mathbf{ f})\text{d}\mathbf{ f}\]</span>
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/pygfpf.svg" width style=" ">
</object>
</td>
</tr>
</table></li>
</ul>
</section>
<section id="variational-compression-4" class="slide level2">
<h2>Variational Compression</h2>
<ul>
<li>Prior density over <span class="math inline">\(\mathbf{ f}\)</span>. Likelihood relates data, <span class="math inline">\(\mathbf{ y}\)</span>, to <span class="math inline">\(\mathbf{ f}\)</span>.
<table>
<tr>
<td width>
<span class="math display">\[p(\mathbf{ y})=\int p(\mathbf{ y}|\mathbf{ f})p(\mathbf{ u}|\mathbf{ f})p(\mathbf{ f})\text{d}\mathbf{ f}\text{d}\mathbf{ u}\]</span>
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/pygfpugfpf.svg" width style=" ">
</object>
</td>
</tr>
</table></li>
</ul>
</section>
<section id="variational-compression-5" class="slide level2">
<h2>Variational Compression</h2>
<table>
<tr>
<td width>
<span class="math display">\[p(\mathbf{ y})=\int \int p(\mathbf{ y}|\mathbf{ f})p(\mathbf{ f}|\mathbf{ u})\text{d}\mathbf{ f}p(\mathbf{ u})\text{d}\mathbf{ u}\]</span>
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/pygfpfgupu.svg" width style=" ">
</object>
</td>
</tr>
</table>
</section>
<section id="variational-compression-6" class="slide level2">
<h2>Variational Compression</h2>
<table>
<tr>
<td width>
<span class="math display">\[p(\mathbf{ y})=\int \int p(\mathbf{ y}|\mathbf{ f})p(\mathbf{ f}|\mathbf{ u})\text{d}\mathbf{ f}p(\mathbf{ u})\text{d}\mathbf{ u}\]</span>
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/pygfpfgupu2.svg" width style=" ">
</object>
</td>
</tr>
</table>
</section>
<section id="variational-compression-7" class="slide level2">
<h2>Variational Compression</h2>
<table>
<tr>
<td width>
<span class="math display">\[p(\mathbf{ y}|\mathbf{ u})=\int p(\mathbf{ y}|\mathbf{ f})p(\mathbf{ f}|\mathbf{ u})\text{d}\mathbf{ f}\]</span>
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/pygfpfgu.svg" width style=" ">
</object>
</td>
</tr>
</table>
</section>
<section id="variational-compression-8" class="slide level2">
<h2>Variational Compression</h2>
<table>
<tr>
<td width>
<span class="math display">\[p(\mathbf{ y}|\mathbf{ u})\]</span>
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/pygu.svg" width style=" ">
</object>
</td>
</tr>
</table>
</section>
<section id="variational-compression-9" class="slide level2">
<h2>Variational Compression</h2>
<table>
<tr>
<td width>
<span class="math display">\[p(\mathbf{ y}|\boldsymbol{ \theta})\]</span>
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/pygtheta.svg" width style=" ">
</object>
</td>
</tr>
</table>
</section>
<section id="compression" class="slide level2">
<h2>Compression</h2>
<ul>
<li>Replace true <span class="math inline">\(p(\mathbf{ u}|\mathbf{ y})\)</span> with approximation <span class="math inline">\(q(\mathbf{ u}|\mathbf{ y})\)</span>.</li>
<li>Minimize KL divergence between approximation and truth.</li>
<li>This is similar to the Bayesian posterior distribution.</li>
<li>But it’s placed over a set of ‘pseudo-observations’.</li>
</ul>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<p><span class="math display">\[\mathbf{ f}, \mathbf{ u}\sim \mathcal{N}\left(\mathbf{0},\begin{bmatrix}\mathbf{K}_{\mathbf{ f}\mathbf{ f}}&amp; \mathbf{K}_{\mathbf{ f}\mathbf{ u}}\\\mathbf{K}_{\mathbf{ u}\mathbf{ f}}&amp; \mathbf{K}_{\mathbf{ u}\mathbf{ u}}\end{bmatrix}\right)\]</span> <span class="math display">\[\mathbf{ y}|\mathbf{ f}= \prod_{i} \mathcal{N}\left(f,\sigma^2\right)\]</span></p>
<!--Variational Compression-->
</section>
<section id="gaussian-py_if_i" class="slide level2">
<h2>Gaussian <span class="math inline">\(p(y_i|f_i)\)</span></h2>
<p>For Gaussian likelihoods:  </p>
</section>
<section id="gaussian-process-over-mathbf-f-and-mathbf-u" class="slide level2">
<h2>Gaussian Process Over <span class="math inline">\(\mathbf{ f}\)</span> and <span class="math inline">\(\mathbf{ u}\)</span></h2>
<p>Define: <span class="math display">\[q_{i, i} = \text{var}_{p(f_i|\mathbf{ u})}\left( f_i \right) = \left&lt;f_i^2\right&gt;_{p(f_i|\mathbf{ u})} - \left&lt;f_i\right&gt;_{p(f_i|\mathbf{ u})}^2\]</span> We can write: <span class="math display">\[c_i = \exp\left(-{\frac{q_{i,i}}{2\sigma^2}}\right)\]</span> If joint distribution of <span class="math inline">\(p(\mathbf{ f}, \mathbf{ u})\)</span> is Gaussian then: <span class="math display">\[q_{i, i} = k_{i, i} - \mathbf{ k}_{i, \mathbf{ u}}^\top \mathbf{K}_{\mathbf{ u}, \mathbf{ u}}^{-1} \mathbf{ k}_{i, \mathbf{ u}}\]</span></p>
<p><span class="math inline">\(c_i\)</span> is not a function of <span class="math inline">\(\mathbf{ u}\)</span> but <em>is</em> a function of <span class="math inline">\(\mathbf{X}_\mathbf{ u}\)</span>.</p>
<!--frame end-->
<!--frame start-->
</section>
<section id="total-conditional-variance" class="slide level2">
<h2>Total Conditional Variance</h2>
<ul>
<li><p>The sum of <span class="math inline">\(q_{i,i}\)</span> is the <em>total conditional variance</em>.</p></li>
<li><p>If conditional density <span class="math inline">\(p(\mathbf{ f}|\mathbf{ u})\)</span> is Gaussian then it has covariance <span class="math display">\[\mathbf{Q} = \mathbf{K}_{\mathbf{ f}\mathbf{ f}} - \mathbf{K}_{\mathbf{ f}\mathbf{ u}}\mathbf{K}_{\mathbf{ u}\mathbf{ u}}^{-1} \mathbf{K}_{\mathbf{ u}\mathbf{ f}}\]</span></p></li>
<li><p><span class="math inline">\(\text{tr}\left(\mathbf{Q}\right) = \sum_{i}q_{i,i}\)</span> is known as total variance.</p></li>
<li><p>Because it is on conditional distribution we call it <em>total conditional variance</em>.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="capacity-of-a-density" class="slide level2">
<h2>Capacity of a Density</h2>
<ul>
<li><p>Measure the ’capacity of a density’.</p></li>
<li><p>Determinant of covariance represents ’volume’ of density.</p></li>
<li><p>log determinant is entropy: sum of <em>log</em> eigenvalues of covariance.</p></li>
<li><p>trace of covariance is total variance: sum of eigenvalues of covariance.</p></li>
<li><p><span class="math inline">\(\lambda &gt; \log \lambda\)</span> then total conditional variance upper bounds entropy.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="alternative-view" class="slide level2">
<h2>Alternative View</h2>
<p>Exponentiated total variance bounds determinant. <span class="math display">\[\det{\mathbf{Q}} &lt; \exp \text{tr}\left(\mathbf{Q}\right)\]</span> Because <span class="math display">\[\prod_{i=1}^k \lambda_i &lt; \prod_{i=1}^k \exp(\lambda_i)\]</span> where <span class="math inline">\(\{\lambda_i\}_{i=1}^k\)</span> are the <em>positive</em> eigenvalues of <span class="math inline">\(\mathbf{Q}\)</span> This in turn implies <span class="math display">\[\det{\mathbf{Q}} &lt; \prod_{i=1}^k \exp\left(q_{i,i}\right)\]</span></p>
<!--frame end-->
<!--frame start-->
</section>
<section id="communication-channel" class="slide level2">
<h2>Communication Channel</h2>
<ul>
<li><p>Conditional density <span class="math inline">\(p(\mathbf{ f}|\mathbf{ u})\)</span> can be seen as a <em>communication channel</em>.</p></li>
<li><p>Normally we have: <span> <span class="math display">\[\text{Transmitter} \stackrel{\mathbf{ u}}{\rightarrow} \begin{smallmatrix}p(\mathbf{ f}|\mathbf{ u}) \\ \text{Channel}\end{smallmatrix} \stackrel{\mathbf{ f}}{\rightarrow} \text{Receiver}\]</span></span> and we control <span class="math inline">\(p(\mathbf{ u})\)</span> (the source density).</p></li>
<li><p><em>Here</em> we can also control the transmission channel <span class="math inline">\(p(\mathbf{ f}|\mathbf{ u})\)</span>.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="lower-bound-on-likelihood" class="slide level2">
<h2>Lower Bound on Likelihood</h2>
<p>Substitute variational bound into marginal likelihood: <span class="math display">\[p(\mathbf{ y})\geq \prod_{i=1}^nc_i \int \mathcal{N}\left(\mathbf{ y}|\left&lt;\mathbf{ f}\right&gt;,\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{ u}\]</span> Note that: <span class="math display">\[\left&lt;\mathbf{ f}\right&gt;_{p(\mathbf{ f}|\mathbf{ u})} = \mathbf{K}_{\mathbf{ f}, \mathbf{ u}} \mathbf{K}_{\mathbf{ u}, \mathbf{ u}}^{-1}\mathbf{ u}\]</span> is <em>linearly</em> dependent on <span class="math inline">\(\mathbf{ u}\)</span>.</p>
<!--frame end-->
<!--frame start-->
</section>
<section id="deterministic-training-conditional" class="slide level2">
<h2>Deterministic Training Conditional</h2>
<p>Making the marginalization of <span class="math inline">\(\mathbf{ u}\)</span> straightforward. In the Gaussian case: <span class="math display">\[p(\mathbf{ u}) = \mathcal{N}\left(\mathbf{ u}|\mathbf{0},\mathbf{K}_{\mathbf{ u},\mathbf{ u}}\right)\]</span>      </p>
<!--frame end-->
</section>
<section id="variational-marginalisation-of-mathbf-f" class="slide level2">
<h2>Variational marginalisation of <span class="math inline">\(\mathbf{ f}\)</span></h2>
<p><span class="math display">\[\log p(\mathbf{ y}|\mathbf{ u}) = \log\int p(\mathbf{ y}|\mathbf{ f})p(\mathbf{ f}|\mathbf{ u},\mathbf{X})\text{d}\mathbf{ f}\]</span></p>
<p><span class="math display">\[\log p(\mathbf{ y}|\mathbf{ u}) = \log \mathbb{E}_{p(\mathbf{ f}|\mathbf{ u},\mathbf{X})}\left[p(\mathbf{ y}|\mathbf{ f})\right]\]</span>  <span class="math display">\[\log p(\mathbf{ y}|\mathbf{ u}) \geq  \mathbb{E}_{p(\mathbf{ f}|\mathbf{ u},\mathbf{X})}\left[\log p(\mathbf{ y}|\mathbf{ f})\right]\triangleq \log\widetilde p(\mathbf{ y}|\mathbf{ u})\]</span> </p>
<p><span> No inversion of <span class="math inline">\(\mathbf{K}_{\mathbf{ f}\mathbf{ f}}\)</span> required</span></p>
</section>
<section id="variational-marginalisation-of-mathbf-f-another-way" class="slide level2">
<h2>Variational marginalisation of <span class="math inline">\(\mathbf{ f}\)</span> (another way)</h2>
<p><span style="text-align:right"><span class="citation" data-cites="Titsias:variational09">Titsias (n.d.)</span></span> <span class="math display">\[p(\mathbf{ y}|\mathbf{ u}) = \frac{p(\mathbf{ y}|\mathbf{ f})p(\mathbf{ f}|\mathbf{ u})}{p(\mathbf{ f}|\mathbf{ y}, \mathbf{ u})}\]</span>  <span class="math display">\[\log p(\mathbf{ y}|\mathbf{ u}) = \log p(\mathbf{ y}|\mathbf{ f}) + \log \frac{p(\mathbf{ f}|\mathbf{ u})}{p(\mathbf{ f}|\mathbf{ y}, \mathbf{ u})}\]</span>  <span class="math display">\[\log p(\mathbf{ y}|\mathbf{ u}) = \bbE_{p(\mathbf{ f}|\mathbf{ u})}\big[\log p(\mathbf{ y}|\mathbf{ f})\big] + \bbE_{p(\mathbf{ f}|\mathbf{ u})}\big[\log \frac{p(\mathbf{ f}|\mathbf{ u})}{p(\mathbf{ f}|\mathbf{ y}, \mathbf{ u})}\big]\]</span>  <span class="math display">\[\log p(\mathbf{ y}|\mathbf{ u}) = \widetilde p(\mathbf{ y}|\mathbf{ u}) + \textsc{KL}[p(\mathbf{ f}|\mathbf{ u})||p(\mathbf{ f}|\mathbf{ y}, \mathbf{ u})]\]</span></p>
<p><span> No inversion of <span class="math inline">\(\mathbf{K}_{\mathbf{ f}\mathbf{ f}}\)</span> required</span></p>
</section>
<section id="a-lower-bound-on-the-likelihood" class="slide level2">
<h2>A Lower Bound on the Likelihood</h2>
<p><span class="math display">\[\widetilde p(\mathbf{ y}|\mathbf{ u})  = \prod_{i=1}^n\widetilde p(y_i|\mathbf{ u})\]</span> <span class="math display">\[\widetilde p(y|\mathbf{ u}) = \mathcal{N}\left(y|\mathbf{ k}_{fu}\mathbf{K}_{\mathbf{ u}\mathbf{ u}}^{-1}\mathbf{ u},\sigma^2\right) \,{\color{red}\exp\left\{-\tfrac{1}{2\sigma^2}\left(k_{ff}- \mathbf{ k}_{fu}\mathbf{K}_{\mathbf{ u}\mathbf{ u}}^{-1}\mathbf{ k}_{uf}\right)\right\}}\]</span></p>
<p><span>A straightforward likelihood approximation, and a penalty term</span></p>
</section>
<section id="now-we-can-marginalise-mathbf-u" class="slide level2">
<h2>Now we can marginalise <span class="math inline">\(\mathbf{ u}\)</span></h2>
<p><span class="math display">\[\widetilde p(\mathbf{ u}|\mathbf{ y},\mathbf{Z}) = \frac{\widetilde p(\mathbf{ y}|\mathbf{ u})p(\mathbf{ u}|\mathbf{Z})}{\int \widetilde p(\mathbf{ y}|\mathbf{ u})p(\mathbf{ u}|\mathbf{Z})\text{d}{\mathbf{ u}}}\]</span></p>
<ul>
<li><p>Computing the posterior costs <span class="math inline">\(\mathcal{O}(nm^2)\)</span></p></li>
<li><p>We also get a lower bound of the marginal likelihood</p></li>
</ul>
</section>
<section id="what-does-the-penalty-term-do" class="slide level2">
<h2>What does the penalty term do?</h2>
<p><span class="math display">\[{\color{red}\sum_{i=1}^n-\tfrac{1}{2\sigma^2}\left(k_{ff}- \mathbf{ k}_{fu}\mathbf{K}_{\mathbf{ u}\mathbf{ u}}^{-1}\mathbf{ k}_{uf}\right)}\]</span></p>
</section>
<section id="what-does-the-penalty-term-do-1" class="slide level2">
<h2>What does the penalty term do?</h2>
<p><span class="math display">\[{\color{red}\sum_{i=1}^n-\tfrac{1}{2\sigma^2}\left(k_{ff}- \mathbf{ k}_{fu}\mathbf{K}_{\mathbf{ u}\mathbf{ u}}^{-1}\mathbf{ k}_{uf}\right)}\]</span></p>
</section>
<section id="what-does-the-penalty-term-do-2" class="slide level2">
<h2>What does the penalty term do?</h2>
<!--![image](../../../gp/tex/diagrams/cov_approx){width="60.00000%"}-->
<!--![image](../../../gp/tex/diagrams/cov_approx_opt){width="60.00000%"}-->
</section>
<section id="how-good-is-the-inducing-approximation" class="slide level2">
<h2>How good is the inducing approximation?</h2>
<p><span>It’s easy to show that as <span class="math inline">\(\mathbf{Z}\to \mathbf{X}\)</span>:</span></p>
<ul>
<li><p><span class="math inline">\(\mathbf{ u}\to \mathbf{ f}\)</span> (and the posterior is exact)</p></li>
<li><p>The penalty term is zero.</p></li>
<li><p>The cost returns to <span class="math inline">\(\mathcal{O}(n^3)\)</span></p></li>
</ul>
<ul>
<li><p><br />
</p></li>
<li><p></p></li>
</ul>
</section>
<section id="predictions" class="slide level2">
<h2>Predictions</h2>

</section>
<section id="recap" class="slide level2">
<h2>Recap</h2>
<p><span>So far we:</span></p>
<ul>
<li><p>introduced <span class="math inline">\(\mathbf{Z}, \mathbf{ u}\)</span></p></li>
<li><p>approximated the intergral over <span class="math inline">\(\mathbf{ f}\)</span> variationally</p></li>
<li><p>captured the information in <span class="math inline">\(\widetilde p(\mathbf{ u}|\mathbf{ y})\)</span></p></li>
<li><p>obtained a lower bound on the marginal likeihood</p></li>
<li><p>saw the effect of the penalty term</p></li>
<li><p>prediction for new points</p></li>
</ul>
<p><span>Omitted details:</span></p>
<ul>
<li><p>optimization of the covariance parameters using the bound</p></li>
<li><p>optimization of Z (simultaneously)</p></li>
<li><p>the form of <span class="math inline">\(\widetilde p(\mathbf{ u}|\mathbf{ y})\)</span></p></li>
<li><p>historical approximations</p></li>
</ul>
</section>
<section id="other-approximations" class="slide level2">
<h2>Other approximations</h2>
<p><span>Subset selection</span> <span style="text-align:right"><span class="citation" data-cites="Lawrence:ivm02">Lawrence et al. (n.d.)</span></span></p>
<ul>
<li><p>Random or systematic</p></li>
<li><p>Set <span class="math inline">\(\mathbf{Z}\)</span> to subset of <span class="math inline">\(\mathbf{X}\)</span></p></li>
<li><p>Set <span class="math inline">\(\mathbf{ u}\)</span> to subset of <span class="math inline">\(\mathbf{ f}\)</span></p></li>
<li><p>Approximation to <span class="math inline">\(p(\mathbf{ y}|\mathbf{ u})\)</span>:</p>
<ul>
<li><p>$ p(_i) = p(_i_i) i$</p></li>
<li><p>$ p(_i) = 1  i$</p></li>
</ul></li>
</ul>
</section>
<section id="other-approximations-1" class="slide level2">
<h2>Other approximations</h2>
<p><span style="text-align:right"><span class="citation" data-cites="Quinonero:unifying05">Quiñonero Candela and Rasmussen (2005)</span></span> {Deterministic Training Conditional (DTC)}</p>
<ul>
<li><p>Approximation to <span class="math inline">\(p(\mathbf{ y}|\mathbf{ u})\)</span>:</p>
<ul>
<li>$ p(_i) = (_i, [_i])$</li>
</ul></li>
<li><p>As our variational formulation, but without penalty</p></li>
</ul>
<p>Optimization of <span class="math inline">\(\mathbf{Z}\)</span> is difficult</p>
</section>
<section id="other-approximations-2" class="slide level2">
<h2>Other approximations</h2>
<p><span>Fully Independent Training Conditional</span> <span style="text-align:right"><span class="citation" data-cites="Snelson:pseudo05">Snelson and Ghahramani (n.d.)</span></span></p>
<ul>
<li><p>Approximation to <span class="math inline">\(p(\mathbf{ y}|\mathbf{ u})\)</span>:</p></li>
<li><p>$ p() = _i p(_i) $</p></li>
</ul>
<p>Optimization of <span class="math inline">\(\mathbf{Z}\)</span> is still difficult, and there are some weird heteroscedatic effects</p>
</section>
<section id="selecting-data-dimensionality" class="slide level2">
<h2>Selecting Data Dimensionality</h2>
<ul>
<li>GP-LVM Provides probabilistic non-linear dimensionality reduction.</li>
<li>How to select the dimensionality?</li>
<li>Need to estimate marginal likelihood.</li>
<li>In standard GP-LVM it increases with increasing <span class="math inline">\(q\)</span>.</li>
</ul>
</section>
<section id="integrate-mapping-function-and-latent-variables" class="slide level2">
<h2>Integrate Mapping Function and Latent Variables</h2>
<table>
<tr>
<td width="40%">
<p><strong>Bayesian GP-LVM</strong></p>
<ul>
<li>Start with a standard GP-LVM.</li>
<li>Apply standard latent variable approach:
<ul>
<li>Define Gaussian prior over , <span class="math inline">\(\mathbf{Z}\)</span>.</li>
<li>Integrate out .</li>
<li>Unfortunately integration is intractable.</li>
</ul></li>
</ul>
</td>
<td width="60%">
<center>
{ }
</center>
</td>
</tr>
</table>
</section>
<section id="standard-variational-approach-fails" class="slide level2">
<h2>Standard Variational Approach Fails</h2>
<ul>
<li>Standard variational bound has the form: <span class="math display">\[
\mathcal{L}= \left&lt;\log p(\mathbf{ y}|\mathbf{Z})\right&gt;_{q(\mathbf{Z})} + \text{KL}\left( q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)
\]</span></li>
</ul>
</section>
<section id="standard-variational-approach-fails-1" class="slide level2">
<h2>Standard Variational Approach Fails</h2>
<ul>
<li>Requires expectation of <span class="math inline">\(\log p(\mathbf{ y}|\mathbf{Z})\)</span> under <span class="math inline">\(q(\mathbf{Z})\)</span>. <span class="math display">\[
\begin{align}
\log p(\mathbf{ y}|\mathbf{Z}) = &amp; -\frac{1}{2}\mathbf{ y}^\top\left(\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}+\sigma^2\mathbf{I}\right)^{-1}\mathbf{ y}\\ &amp; -\frac{1}{2}\log \det{\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}+\sigma^2 \mathbf{I}} -\frac{n}{2}\log 2\pi
\end{align}
\]</span> <span class="math inline">\(\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}\)</span> is dependent on <span class="math inline">\(\mathbf{Z}\)</span> and it appears in the inverse.</li>
</ul>
</section>
<section id="variational-bayesian-gp-lvm" class="slide level2">
<h2>Variational Bayesian GP-LVM</h2>
<ul>
<li>Consider collapsed variational bound, <span class="fragment" data-fragment-index="1"><small><span class="math display">\[
  p(\mathbf{ y})\geq \prod_{i=1}^nc_i \int \mathcal{N}\left(\mathbf{ y}|\left&lt;\mathbf{ f}\right&gt;,\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{ u}
\]</span></small></span> <span class="fragment" data-fragment-index="2"><small><span class="math display">\[
  p(\mathbf{ y}|\mathbf{Z})\geq \prod_{i=1}^nc_i \int \mathcal{N}\left(\mathbf{ y}|\left&lt;\mathbf{ f}\right&gt;_{p(\mathbf{ f}|\mathbf{ u}, \mathbf{Z})},\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{ u}
\]</span></small></span> <span class="fragment" data-fragment-index="3"><small><span class="math display">\[
    \int p(\mathbf{ y}|\mathbf{Z})p(\mathbf{Z}) \text{d}\mathbf{Z}\geq \int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{ y}|\left&lt;\mathbf{ f}\right&gt;_{p(\mathbf{ f}|\mathbf{ u}, \mathbf{Z})},\sigma^2\mathbf{I}\right) p(\mathbf{Z})\text{d}\mathbf{Z}p(\mathbf{ u}) \text{d}\mathbf{ u}
\]</span></small></span></li>
</ul>
</section>
<section id="variational-bayesian-gp-lvm-1" class="slide level2">
<h2>Variational Bayesian GP-LVM</h2>
<ul>
<li>Apply variational lower bound to the inner integral. <small><span class="math display">\[
  \begin{align}
  \int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{ y}|\left&lt;\mathbf{ f}\right&gt;_{p(\mathbf{ f}|\mathbf{ u}, \mathbf{Z})},\sigma^2\mathbf{I}\right) p(\mathbf{Z})\text{d}\mathbf{Z}\geq &amp; \left&lt;\sum_{i=1}^n\log  c_i\right&gt;_{q(\mathbf{Z})}\\ &amp; +\left&lt;\log\mathcal{N}\left(\mathbf{ y}|\left&lt;\mathbf{ f}\right&gt;_{p(\mathbf{ f}|\mathbf{ u}, \mathbf{Z})},\sigma^2\mathbf{I}\right)\right&gt;_{q(\mathbf{Z})}\\&amp; + \text{KL}\left( q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)    
  \end{align}
\]</span></small></li>
<li>Which is analytically tractable for Gaussian <span class="math inline">\(q(\mathbf{Z})\)</span> and some covariance functions.</li>
</ul>
</section>
<section id="required-expectations" class="slide level2">
<h2>Required Expectations</h2>
<ul>
<li>Need expectations under <span class="math inline">\(q(\mathbf{Z})\)</span> of: <small><span class="math display">\[
\log c_i = \frac{1}{2\sigma^2} \left[k_{i, i} - \mathbf{ k}_{i, \mathbf{ u}}^\top \mathbf{K}_{\mathbf{ u}, \mathbf{ u}}^{-1} \mathbf{ k}_{i, \mathbf{ u}}\right]
\]</span></small> and <small><span class="math display">\[
\log \mathcal{N}\left(\mathbf{ y}|\left&lt;\mathbf{ f}\right&gt;_{p(\mathbf{ f}|\mathbf{ u},\mathbf{Y})},\sigma^2\mathbf{I}\right) = -\frac{1}{2}\log 2\pi\sigma^2 - \frac{1}{2\sigma^2}\left(y_i - \mathbf{K}_{\mathbf{ f}, \mathbf{ u}}\mathbf{K}_{\mathbf{ u},\mathbf{ u}}^{-1}\mathbf{ u}\right)^2
\]</span></small></li>
</ul>
</section>
<section id="required-expectations-1" class="slide level2">
<h2>Required Expectations</h2>
<ul>
<li>This requires the expectations <span class="math display">\[
\left&lt;\mathbf{K}_{\mathbf{ f},\mathbf{ u}}\right&gt;_{q(\mathbf{Z})}
\]</span> and <span class="math display">\[
\left&lt;\mathbf{K}_{\mathbf{ f},\mathbf{ u}}\mathbf{K}_{\mathbf{ u},\mathbf{ u}}^{-1}\mathbf{K}_{\mathbf{ u},\mathbf{ f}}\right&gt;_{q(\mathbf{Z})}
\]</span> which can be computed analytically for some covariance functions <span class="citation" data-cites="Damianou:variational15">(Damianou et al., 2016)</span> or through sampling <span class="citation" data-cites="Damianou:thesis2015 Salimbeni:doubly2017">(Damianou, 2015; Salimbeni and Deisenroth, 2017)</span>.</li>
</ul>
</section>
<section id="variational-compression-10" class="slide level2">
<h2>Variational Compression</h2>
<p><span style="text-align:right"></p>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Andreas Damianou
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/andreas-damianou.png" clip-path="url(#clip0)"/>
</svg>
<p></span><br />
<span style="text-align:right"><span class="citation" data-cites="Damianou:deepgp13">Damianou and Lawrence (2013)</span></span><br />
</p>
<ul>
<li><p>Augment each layer with inducing variables <span class="math inline">\(\mathbf{ u}_i\)</span>.</p></li>
<li><p>Apply variational compression, <span class="math display">\[\begin{align}
      p(\mathbf{ y}, \{\mathbf{ h}_i\}_{i=1}^{\ell-1}|\{\mathbf{ u}_i\}_{i=1}^{\ell}, \mathbf{X}) \geq &amp; 
      \tilde p(\mathbf{ y}|\mathbf{ u}_{\ell}, \mathbf{ h}_{\ell-1})\prod_{i=2}^{\ell-1} \tilde p(\mathbf{ h}_i|\mathbf{ u}_i,\mathbf{ h}_{i-1}) \tilde p(\mathbf{ h}_1|\mathbf{ u}_i,\mathbf{X}) \nonumber \\
      &amp; \times
      \exp\left(\sum_{i=1}^\ell-\frac{1}{2\sigma^2_i}\text{tr}\left(\boldsymbol{ \Sigma}_{i}\right)\right)
      \label{eq:deep_structure}
    \end{align}\]</span> where <span class="math display">\[\tilde p(\mathbf{ h}_i|\mathbf{ u}_i,\mathbf{ h}_{i-1})
    = \mathcal{N}\left(\mathbf{ h}_i|\mathbf{K}_{\mathbf{ h}_{i}\mathbf{ u}_{i}}\mathbf{K}_{\mathbf{ u}_i\mathbf{ u}_i}^{-1}\mathbf{ u}_i,\sigma^2_i\mathbf{I}\right).\]</span></p></li>
</ul>
</section>
<section id="nested-variational-compression" class="slide level2">
<h2>Nested Variational Compression</h2>
<p><span style="text-align:right"></p>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip1">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
James Hensman
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/james-hensman.png" clip-path="url(#clip1)"/>
</svg>
<p></span><br />
<span style="text-align:right"><span class="citation" data-cites="Hensman:nested14">Hensman and Lawrence (2014)</span></span></p>
<ul>
<li><p>By sustaining explicity distributions over inducing variables James Hensman has developed a nested variant of variational compression.</p></li>
<li><p>Exciting thing: it mathematically looks like a deep neural network, but with inducing variables in the place of basis functions.</p></li>
<li><p>Additional complexity control term in the objective function.</p></li>
</ul>
</section>
<section id="nested-bound" class="slide level2">
<h2>Nested Bound</h2>
<p><span class="math display">\[\begin{align}
    \log p(\mathbf{ y}|\mathbf{X})  \geq &amp;
    %
    -\frac{1}{\sigma_1^2} \text{tr}\left(\boldsymbol{ \Sigma}_1\right)
    % 
    -\sum_{i=2}^\ell\frac{1}{2\sigma_i^2} \left(\psi_{i}
    % 
    - \text{tr}\left({\boldsymbol \Phi}_{i}\mathbf{K}_{\mathbf{ u}_{i} \mathbf{ u}_{i}}^{-1}\right)\right) \nonumber \\
    %
    &amp; - \sum_{i=1}^{\ell}\text{KL}\left( q(\mathbf{ u}_i)\,\|\,p(\mathbf{ u}_i) \right) \nonumber \\
    %
    &amp; - \sum_{i=2}^{\ell}\frac{1}{2\sigma^2_{i}}\text{tr}\left(({\boldsymbol
        \Phi}_i - {\boldsymbol \Psi}_i^\top{\boldsymbol \Psi}_i)
      \mathbf{K}_{\mathbf{ u}_{i}
        \mathbf{ u}_{i}}^{-1}
      \left&lt;\mathbf{ u}_{i}\mathbf{ u}_{i}^\top\right&gt;_{q(\mathbf{ u}_{i})}\mathbf{K}_{\mathbf{ u}_{i}\mathbf{ u}_{i}}^{-1}\right) \nonumber \\
    %
    &amp; + {\only&lt;2&gt;{\color{cyan}}\log \mathcal{N}\left(\mathbf{ y}|{\boldsymbol
        \Psi}_{\ell}\mathbf{K}_{\mathbf{ u}_{\ell}
        \mathbf{ u}_{\ell}}^{-1}{\mathbf
        m}_\ell,\sigma^2_\ell\mathbf{I}\right)}
    \label{eq:deep_bound}
  \end{align}\]</span></p>
</section>
<section id="required-expectations-2" class="slide level2">
<h2>Required Expectations</h2>
<p><span class="math display">\[{\only&lt;1&gt;{\color{cyan}}\log \mathcal{N}\left(\mathbf{ y}|{\only&lt;2-&gt;{\color{yellow}}{\boldsymbol
          \Psi}_{\ell}}\mathbf{K}_{\mathbf{ u}_{\ell}
          \mathbf{ u}_{\ell}}^{-1}{\mathbf
          m}_\ell,\sigma^2_\ell\mathbf{I}\right)}\]</span> where   </p>
</section>
<section id="gaussian-py_if_i-1" class="slide level2">
<h2>Gaussian <span class="math inline">\(p(y_i|f_i)\)</span></h2>
<p>For Gaussian likelihoods:  </p>
</section>
<section id="gaussian-process-over-mathbf-f-and-mathbf-u-1" class="slide level2">
<h2>Gaussian Process Over <span class="math inline">\(\mathbf{ f}\)</span> and <span class="math inline">\(\mathbf{ u}\)</span></h2>
<p>Define: <span class="math display">\[q_{i, i} = \text{var}_{p(f_i|\mathbf{ u})}\left( f_i \right) = \left&lt;f_i^2\right&gt;_{p(f_i|\mathbf{ u})} - \left&lt;f_i\right&gt;_{p(f_i|\mathbf{ u})}^2\]</span> We can write: <span class="math display">\[c_i = \exp\left(-{\frac{q_{i,i}}{2\sigma^2}}\right)\]</span> If joint distribution of <span class="math inline">\(p(\mathbf{ f}, \mathbf{ u})\)</span> is Gaussian then: <span class="math display">\[q_{i, i} = k_{i, i} - \mathbf{ k}_{i, \mathbf{ u}}^\top \mathbf{K}_{\mathbf{ u}, \mathbf{ u}}^{-1} \mathbf{ k}_{i, \mathbf{ u}}\]</span></p>
<p><span class="math inline">\(c_i\)</span> is not a function of <span class="math inline">\(\mathbf{ u}\)</span> but <em>is</em> a function of <span class="math inline">\(\mathbf{X}_\mathbf{ u}\)</span>.</p>
</section>
<section id="lower-bound-on-likelihood-1" class="slide level2">
<h2>Lower Bound on Likelihood</h2>
<p>Substitute variational bound into marginal likelihood: <span class="math display">\[p(\mathbf{ y})\geq \prod_{i=1}^nc_i \int \mathcal{N}\left(\mathbf{ y}|\left&lt;\mathbf{ f}\right&gt;,\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{ u}\]</span> Note that: <span class="math display">\[\left&lt;\mathbf{ f}\right&gt;_{p(\mathbf{ f}|\mathbf{ u})} = \mathbf{K}_{\mathbf{ f}, \mathbf{ u}} \mathbf{K}_{\mathbf{ u}, \mathbf{ u}}^{-1}\mathbf{ u}\]</span> is <em>linearly</em> dependent on <span class="math inline">\(\mathbf{ u}\)</span>.</p>
</section>
<section id="deterministic-training-conditional-1" class="slide level2">
<h2>Deterministic Training Conditional</h2>
<p>Making the marginalization of <span class="math inline">\(\mathbf{ u}\)</span> straightforward. In the Gaussian case: <span class="math display">\[p(\mathbf{ u}) = \mathcal{N}\left(\mathbf{ u}|\mathbf{0},\mathbf{K}_{\mathbf{ u},\mathbf{ u}}\right)\]</span>      </p>
</section>
<section id="efficient-computation" class="slide level2">
<h2>Efficient Computation</h2>
<ul>
<li>Thang and Turner paper</li>
</ul>
</section>
<section id="other-limitations" class="slide level2">
<h2>Other Limitations</h2>
<ul>
<li>Joint Gaussianity is analytic, but not flexible.</li>
</ul>
</section>
<section id="gpy-a-gaussian-process-framework-in-python" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
GPy is a BSD licensed software code base for implementing Gaussian process models in Python. It is designed for teaching and modelling. We welcome contributions which can be made through the Github repository <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</aside>
<center>
<a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</center>
</section>
<section id="gpy-a-gaussian-process-framework-in-python-1" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<ul>
<li>BSD Licensed software base.</li>
<li>Wide availability of libraries, ‘modern’ scripting language.</li>
<li>Allows us to set projects to undergraduates in Comp Sci that use GPs.</li>
<li>Available through GitHub <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></li>
<li>Reproducible Research with Jupyter Notebook.</li>
</ul>
</section>
<section id="features" class="slide level2">
<h2>Features</h2>
<ul>
<li>Probabilistic-style programming (specify the model, not the algorithm).</li>
<li>Non-Gaussian likelihoods.</li>
<li>Multivariate outputs.</li>
<li>Dimensionality reduction.</li>
<li>Approximations for large data sets.</li>
</ul>
</section>
<section id="full-gaussian-process-fit" class="slide level2">
<h2>Full Gaussian Process Fit</h2>
<div class="figure">
<div id="sparse-demo-full-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-full-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Full Gaussian process fitted to the data set.
</aside>
</section>
<section id="inducing-variable-fit" class="slide level2">
<h2>Inducing Variable Fit</h2>
<div class="figure">
<div id="sparse-demo-constrained-inducing-6-unlearned-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-constrained-inducing-6-unlearned-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Sparse Gaussian process fitted with six inducing variables, no optimization of parameters or inducing variables.
</aside>
</section>
<section id="inducing-variable-param-optimize" class="slide level2">
<h2>Inducing Variable Param Optimize</h2>
<div class="figure">
<div id="sparse-demo-constrained-inducing-6-learned-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-constrained-inducing-6-learned-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fitted with inducing variables fixed and parameters optimized
</aside>
</section>
<section id="inducing-variable-full-optimize" class="slide level2">
<h2>Inducing Variable Full Optimize</h2>
<div class="figure">
<div id="sparse-demo-unconstrained-inducing-6-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-unconstrained-inducing-6-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fitted with location of inducing variables and parameters both optimized
</aside>
</section>
<section id="eight-optimized-inducing-variables" class="slide level2">
<h2>Eight Optimized Inducing Variables</h2>
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-sparse-inducing-8-gp.svg" width style=" ">
</object>
</section>
<section id="full-gaussian-process-fit-1" class="slide level2">
<h2>Full Gaussian Process Fit</h2>
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-full-gp.svg" width style=" ">
</object>
<div class="figure">
<div id="sparse-demo-sparse-inducing-8-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-sparse-inducing-8-gp.svg" width="80%" style=" ">
</object>
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-full-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Comparison of the full Gaussian process fit with a sparse Gaussian process using eight inducing varibles. Both inducing variables and parameters are optimized.
</aside>
<!-- SECTION Non Gaussian Likelihoods -->
</section>
<section id="non-gaussian-likelihoods" class="slide level2">
<h2>Non Gaussian Likelihoods</h2>
<p>[Gaussian processes model functions. If our observation is a corrupted version of this function and the corruption process is <em>also</em> Gaussian, it is trivial to account for this. However, there are many circumstances where our observation may be non Gaussian. In these cases we need to turn to approximate inference techniques. As a simple illustration, we’ll use a dataset of binary observations of the language that is spoken in different regions of East-Timor. First we will load the data and a couple of libraries to visualize it.}</p>
</section>
<section id="robust-regression-a-running-example" class="slide level2">
<h2>Robust Regression: A Running Example</h2>
</section>
<section id="olympic-marathon-data" class="slide level2">
<h2>Olympic Marathon Data</h2>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardised distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level2">
<h2>Olympic Marathon Data</h2>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Olympic marathon pace times since 1892.
</aside>
<!-- SECTION Sparse GP Classification -->
</section>
<section id="sparse-gp-classification" class="slide level2">
<h2>Sparse GP Classification</h2>
</section>
<section id="section-7" class="slide level2">
<h2></h2>
<div class="figure">
<div id="olivetti-glasses-image-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="'../slides/diagrams/datasets/olivetti-glasses-image.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Image from the Oivetti glasses data sets.
</aside>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Alvarez:efficient10">
<p>Álvarez, M.A., Luengo, D., Titsias, M.K., Lawrence, N.D., 2010. Efficient multioutput Gaussian processes through variational inducing kernels, in:. pp. 25–32.</p>
</div>
<div id="ref-Thang:unifying17">
<p>Bui, T.D., Yan, J., Turner, R.E., 2017. A unifying framework for Gaussian process pseudo-point approximations using power expectation propagation. Journal of Machine Learning Research 18, 1–72.</p>
</div>
<div id="ref-Csato:sparse02">
<p>Csató, L., Opper, M., 2002. Sparse on-line Gaussian processes. Neural Computation 14, 641–668.</p>
</div>
<div id="ref-Dai:gpu14">
<p>Dai, Z., Damianou, A., Hensman, J., Lawrence, N.D., 2014. Gaussian process models with parallelization and GPU acceleration.</p>
</div>
<div id="ref-Damianou:thesis2015">
<p>Damianou, A., 2015. Deep Gaussian processes and variational propagation of uncertainty (PhD thesis). University of Sheffield.</p>
</div>
<div id="ref-Damianou:deepgp13">
<p>Damianou, A., Lawrence, N.D., 2013. Deep Gaussian processes, in:. pp. 207–215.</p>
</div>
<div id="ref-Damianou:variational15">
<p>Damianou, A., Titsias, M.K., Lawrence, N.D., 2016. Variational inference for latent variables and uncertain inputs in Gaussian processes. Journal of Machine Learning Research 17.</p>
</div>
<div id="ref-Gal:distributed14">
<p>Gal, Y., Wilk, M. van der, Rasmussen, C.E., n.d. Distributed variational inference in sparse Gaussian process regression and latent variable models, in:.</p>
</div>
<div id="ref-Hensman:bigdata13">
<p>Hensman, J., Fusi, N., Lawrence, N.D., n.d. Gaussian processes for big data, in:.</p>
</div>
<div id="ref-Hensman:nested14">
<p>Hensman, J., Lawrence, N.D., 2014. Nested variational compression in deep Gaussian processes. University of Sheffield.</p>
</div>
<div id="ref-Hensman:fast12">
<p>Hensman, J., Rattray, M., Lawrence, N.D., 2012. Fast variational inference in the conjugate exponential family, in:.</p>
</div>
<div id="ref-Hoffman:stochastic12">
<p>Hoffman, M., Blei, D.M., Wang, C., Paisley, J., 2012. Stochastic variational inference, arXiv preprint arXiv:1206.7051.</p>
</div>
<div id="ref-King:klcorrection06">
<p>King, N.J., Lawrence, N.D., n.d. Fast variational inference for Gaussian Process models through KL-correction, in:. pp. 270–281.</p>
</div>
<div id="ref-Lawrence:larger07">
<p>Lawrence, N.D., n.d. Learning for larger datasets with the Gaussian process latent variable model, in:. pp. 243–250.</p>
</div>
<div id="ref-Lawrence:ivm02">
<p>Lawrence, N.D., Seeger, M., Herbrich, R., n.d. Fast sparse Gaussian process methods: The informative vector machine, in:. pp. 625–632.</p>
</div>
<div id="ref-Quinonero:unifying05">
<p>Quiñonero Candela, J., Rasmussen, C.E., 2005. A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Research 6, 1939–1959.</p>
</div>
<div id="ref-Salimbeni:doubly2017">
<p>Salimbeni, H., Deisenroth, M., 2017. Doubly stochastic variational inference for deep Gaussian processes, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural Information Processing Systems 30. Curran Associates, Inc., pp. 4591–4602.</p>
</div>
<div id="ref-Seeger:auto17">
<p>Seeger, M.W., Hetzel, A., Dai, Z., Lawrence, N.D., 2017. Auto-differentiating linear algebra. CoRR abs/1710.08717.</p>
</div>
<div id="ref-Seeger:fast03">
<p>Seeger, M., Williams, C.K.I., Lawrence, N.D., n.d. Fast forward selection to speed up sparse Gaussian process regression, in:.</p>
</div>
<div id="ref-Smola:sparsegp00">
<p>Smola, A.J., Bartlett, P.L., n.d. Sparse greedy Gaussian process regression, in:. pp. 619–625.</p>
</div>
<div id="ref-Snelson:pseudo05">
<p>Snelson, E., Ghahramani, Z., n.d. Sparse Gaussian processes using pseudo-inputs, in:.</p>
</div>
<div id="ref-Titsias:variational09">
<p>Titsias, M.K., n.d. Variational learning of inducing variables in sparse Gaussian processes, in:. pp. 567–574.</p>
</div>
<div id="ref-Williams:nystrom00">
<p>Williams, C.K.I., Seeger, M., n.d. Using the Nyström method to speed up kernel machines, in:. pp. 682–688.</p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
