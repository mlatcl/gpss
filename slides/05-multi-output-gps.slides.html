<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Multi-output Gaussian Processes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="https://inverseprobability.com/assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Multi-output Gaussian Processes</h1>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
</section>
<section id="gpy-a-gaussian-process-framework-in-python" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/gpss/./slides/diagrams//gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
GPy is a BSD licensed software code base for implementing Gaussian process models in Python. It is designed for teaching and modelling. We welcome contributions which can be made through the GitHub repository <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</aside>
<center>
<a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</center>
</section>
<section id="gpy-a-gaussian-process-framework-in-python-1" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<ul>
<li>BSD Licensed software base.</li>
<li>Wide availability of libraries, ‘modern’ scripting language.</li>
<li>Allows us to set projects to undergraduates in Comp Sci that use GPs.</li>
<li>Available through GitHub <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></li>
<li>Reproducible Research with Jupyter Notebook.</li>
</ul>
</section>
<section id="features" class="slide level2">
<h2>Features</h2>
<ul>
<li>Probabilistic-style programming (specify the model, not the algorithm).</li>
<li>Non-Gaussian likelihoods.</li>
<li>Multivariate outputs.</li>
<li>Dimensionality reduction.</li>
<li>Approximations for large data sets.</li>
</ul>
</section>
<section id="simple-kalman-filter" class="slide level2">
<h2>Simple Kalman Filter</h2>
<ul>
<li><p>We have state vector <span class="math inline">\(\mathbf{X}= \left[\mathbf{ x}_1  \dots \mathbf{ x}_q\right] \in \mathbb{R}^{T \times q}\)</span> and if each state evolves independently we have <span class="math display">\[      
\begin{align*}
  p(\mathbf{X}) &amp;= \prod_{i=1}^qp(\mathbf{ x}_{:, i}) \\
 p(\mathbf{ x}_{:, i}) &amp;= \mathcal{N}\left(\mathbf{ x}_{:, i}|\mathbf{0},\mathbf{K}\right).
\end{align*}
\]</span></p></li>
<li><p>We want to obtain outputs through: <span class="math display">\[
\mathbf{ y}_{i, :} = \mathbf{W}\mathbf{ x}_{i, :}
\]</span></p></li>
</ul>
</section>
<section id="stacking-and-kronecker-products" class="slide level2">
<h2>Stacking and Kronecker Products</h2>
<ul>
<li>Represent with a ‘stacked’ system: <span class="math display">\[
p(\mathbf{ x}) = \mathcal{N}\left(\mathbf{ x}|\mathbf{0},\mathbf{I}\otimes \mathbf{K}\right)
\]</span> where the stacking is placing each column of <span class="math inline">\(\mathbf{X}\)</span> one on top of another as <span class="math display">\[
\mathbf{ x}= \begin{bmatrix}
      \mathbf{ x}_{:, 1}\\
      \mathbf{ x}_{:, 2}\\
      \vdots\\
      \mathbf{ x}_{:, q}
    \end{bmatrix}
\]</span></li>
</ul>
</section>
<section id="kronecker-product" class="slide level2">
<h2>Kronecker Product</h2>
<div class="figure">
<div id="kronecker-illustrate-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//kern/kronecker_illustrate.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Illustration of the Kronecker product.
</aside>
</section>
<section id="kronecker-product-1" class="slide level2">
<h2>Kronecker Product</h2>
<div class="figure">
<div id="kronecker-ik-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//kern/kronecker_IK.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Kronecker product between two matrices.
</aside>
</section>
<section id="stacking-and-kronecker-products-1" class="slide level2">
<h2>Stacking and Kronecker Products</h2>
<ul>
<li>Represent with a ‘stacked’ system: <span class="math display">\[p
(\mathbf{ x}) = \mathcal{N}\left(\mathbf{ x}|\mathbf{0},\mathbf{I}\otimes \mathbf{K}\right)
\]</span> where the stacking is placing each column of <span class="math inline">\(\mathbf{X}\)</span> one on top of another as <span class="math display">\[
\mathbf{ x}= \begin{bmatrix}
      \mathbf{ x}_{:, 1}\\
      \mathbf{ x}_{:, 2}\\
      \vdots\\
      \mathbf{ x}_{:, q}
    \end{bmatrix}
    \]</span></li>
</ul>
</section>
<section id="column-stacking" class="slide level2">
<h2>Column Stacking</h2>
<p>For this stacking the marginal distribution over <em>time</em> is given by the block diagonals.</p>
</section>
<section id="section" class="slide level2">
<h2></h2>
<script>
showDivs(0, 'kronecker-IK-highlighted');
</script>
<p><small></small> <input id="range-kronecker-IK-highlighted" type="range" min="0" max="5" value="0" onchange="setDivs('kronecker-IK-highlighted')" oninput="setDivs('kronecker-IK-highlighted')"> <button onclick="plusDivs(-1, 'kronecker-IK-highlighted')">❮</button> <button onclick="plusDivs(1, 'kronecker-IK-highlighted')">❯</button></p>
<div class="kronecker-IK-highlighted" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//kern/kronecker_IK_highlighted001.svg" width="80%" style=" ">
</object>
</div>
<div class="kronecker-IK-highlighted" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//kern/kronecker_IK_highlighted002.svg" width="80%" style=" ">
</object>
</div>
<div class="kronecker-IK-highlighted" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//kern/kronecker_IK_highlighted003.svg" width="80%" style=" ">
</object>
</div>
<div class="kronecker-IK-highlighted" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//kern/kronecker_IK_highlighted004.svg" width="80%" style=" ">
</object>
</div>
<div class="kronecker-IK-highlighted" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//kern/kronecker_IK_highlighted005.svg" width="80%" style=" ">
</object>
</div>
</section>
<section id="two-ways-of-stacking" class="slide level2">
<h2>Two Ways of Stacking</h2>
<p>Can also stack each row of <span class="math inline">\(\mathbf{X}\)</span> to form column vector: <span class="math display">\[\mathbf{ x}= \begin{bmatrix}
      \mathbf{ x}_{1, :}\\
      \mathbf{ x}_{2, :}\\
      \vdots\\
      \mathbf{ x}_{T, :}
    \end{bmatrix}\]</span> <span class="math display">\[p(\mathbf{ x}) = \mathcal{N}\left(\mathbf{ x}|\mathbf{0},\mathbf{K}\otimes \mathbf{I}\right)\]</span></p>
</section>
<section id="row-stacking" class="slide level2">
<h2>Row Stacking</h2>
<p>For this stacking the marginal distribution over the latent <em>dimensions</em> is given by the block diagonals.</p>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<script>
showDivs(1, 'kronecker-ki-highlighted');
</script>
<p><small></small> <input id="range-kronecker-ki-highlighted" type="range" min="1" max="5" value="1" onchange="setDivs('kronecker-ki-highlighted')" oninput="setDivs('kronecker-ki-highlighted')"> <button onclick="plusDivs(-1, 'kronecker-ki-highlighted')">❮</button> <button onclick="plusDivs(1, 'kronecker-ki-highlighted')">❯</button></p>
<div class="kronecker-ki-highlighted" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//kern/kronecker_KI_highlighted001.svg" width="80%" style=" ">
</object>
</div>
<div class="kronecker-ki-highlighted" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//kern/kronecker_KI_highlighted002.svg" width="80%" style=" ">
</object>
</div>
<div class="kronecker-ki-highlighted" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//kern/kronecker_KI_highlighted003.svg" width="80%" style=" ">
</object>
</div>
<div class="kronecker-ki-highlighted" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//kern/kronecker_KI_highlighted004.svg" width="80%" style=" ">
</object>
</div>
<div class="kronecker-ki-highlighted" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//kern/kronecker_KI_highlighted005.svg" width="80%" style=" ">
</object>
</div>
</section>
<section id="mapping-from-latent-process-to-observed" class="slide level2">
<h2>Mapping from Latent Process to Observed</h2>
<div class="figure">
<div id="kronecker-ki-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//kern/kronecker_KI.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Mapping from latent process to observed
</aside>
</section>
<section id="observed-process" class="slide level2">
<h2>Observed Process</h2>
<p>The observations are related to the latent points by a linear mapping matrix, <span class="math display">\[
\mathbf{ y}_{i, :} = \mathbf{W}\mathbf{ x}_{i, :} + \boldsymbol{ \epsilon}_{i, :}
\]</span> <span class="math display">\[
\boldsymbol{ \epsilon}\sim \mathcal{N}\left(\mathbf{0},\sigma^2\mathbf{I}\right)
\]</span></p>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="figure">
<div id="kronecker-wx-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//kern/kronecker_WX.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
</section>
<section id="output-covariance" class="slide level2">
<h2>Output Covariance</h2>
<p>This leads to a covariance of the form <span class="math display">\[
(\mathbf{I}\otimes \mathbf{W}) (\mathbf{K}\otimes \mathbf{I}) (\mathbf{I}\otimes \mathbf{W}^\top) + \mathbf{I}\sigma^2
\]</span> Using <span class="math inline">\((\mathbf{A}\otimes\mathbf{B}) (\mathbf{C}\otimes\mathbf{D}) = \mathbf{A}\mathbf{C} \otimes \mathbf{B}\mathbf{D}\)</span> This leads to <span class="math display">\[
\mathbf{K}\otimes {\mathbf{W}}{\mathbf{W}}^\top + \mathbf{I}\sigma^2
\]</span> or <span class="math display">\[
\mathbf{ y}\sim \mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top \otimes \mathbf{K}+ \mathbf{I}\sigma^2\right)
\]</span></p>
</section>
<section id="kernels-for-vector-valued-outputs-a-review" class="slide level2">
<h2>Kernels for Vector Valued Outputs: A Review</h2>
</section>
<section id="kronecker-structure-gps" class="slide level2">
<h2>Kronecker Structure GPs</h2>
<ul>
<li><p>This Kronecker structure leads to several published models. <span class="math display">\[
(\mathbf{K}(\mathbf{ x},\mathbf{ x}^\prime))_{i,i^\prime}=k(\mathbf{ x},\mathbf{ x}^\prime)k_T(i,i^\prime),
\]</span> where <span class="math inline">\(k\)</span> has <span class="math inline">\(\mathbf{ x}\)</span> and <span class="math inline">\(k_T\)</span> has <span class="math inline">\(n\)</span> as inputs.</p></li>
<li><p>Can think of multiple output covariance functions as covariances with augmented input.</p></li>
<li><p>Alongside <span class="math inline">\(\mathbf{ x}\)</span> we also input the <span class="math inline">\(i\)</span> associated with the <em>output</em> of interest.</p></li>
</ul>
</section>
<section id="separable-covariance-functions" class="slide level2">
<h2>Separable Covariance Functions</h2>
<ul>
<li><p>Taking <span class="math inline">\(\mathbf{B}= {\mathbf{W}}{\mathbf{W}}^\top\)</span> we have a matrix expression across outputs. <span class="math display">\[\mathbf{K}(\mathbf{ x},\mathbf{ x}^\prime)=k(\mathbf{ x},\mathbf{ x}^\prime)\mathbf{B},\]</span> where <span class="math inline">\(\mathbf{B}\)</span> is a <span class="math inline">\(p\times p\)</span> symmetric and positive semi-definite matrix.</p></li>
<li><p><span class="math inline">\(\mathbf{B}\)</span> is called the <em>coregionalization</em> matrix.</p></li>
<li><p>We call this class of covariance functions <em>separable</em> due to their product structure.</p></li>
</ul>
</section>
<section id="sum-of-separable-covariance-functions" class="slide level2">
<h2>Sum of Separable Covariance Functions</h2>
<ul>
<li><p>In the same spirit a more general class of kernels is given by <span class="math display">\[\mathbf{K}(\mathbf{ x},\mathbf{ x}^\prime)=\sum_{j=1}^qk_{j}(\mathbf{ x},\mathbf{ x}^\prime)\mathbf{B}_{j}.\]</span></p></li>
<li><p>This can also be written as <span class="math display">\[\mathbf{K}(\mathbf{X}, \mathbf{X}) = \sum_{j=1}^q\mathbf{B}_{j}\otimes k_{j}(\mathbf{X}, \mathbf{X}),\]</span></p></li>
<li><p>This is like several Kalman filter-type models added together, but each one with a different set of latent functions.</p></li>
<li><p>We call this class of kernels sum of separable kernels (SoS kernels).</p></li>
</ul>
</section>
<section id="geostatistics" class="slide level2">
<h2>Geostatistics</h2>
<ul>
<li><p>Use of GPs in Geostatistics is called kriging.</p></li>
<li><p>These multi-output GPs pioneered in geostatistics: prediction over vector-valued output data is known as <em>cokriging</em>.</p></li>
<li><p>The model in geostatistics is known as the <em>linear model of coregionalization</em> (LMC, <span class="citation" data-cites="Journel:miningBook78">Journel and Huijbregts (1978)</span> <span class="citation" data-cites="Goovaerts:book97">Goovaerts (1997)</span>).</p></li>
<li><p>Most machine learning multitask models can be placed in the context of the LMC model.</p></li>
</ul>
</section>
<section id="weighted-sum-of-latent-functions" class="slide level2">
<h2>Weighted sum of Latent Functions</h2>
<ul>
<li><p>In the linear model of coregionalization (LMC) outputs are expressed as linear combinations of independent random functions.</p></li>
<li><p>In the LMC, each component <span class="math inline">\(f_i\)</span> is expressed as a linear sum <span class="math display">\[f_i(\mathbf{ x}) = \sum_{j=1}^q{w}_{i,{j}}{u}_{j}(\mathbf{ x}).\]</span> where the latent functions are independent and have covariance functions <span class="math inline">\(k_{j}(\mathbf{ x},\mathbf{ x}^\prime)\)</span>.</p></li>
<li><p>The processes <span class="math inline">\(\{f_j(\mathbf{ x})\}_{j=1}^q\)</span> are independent for <span class="math inline">\(q\neq {j}^\prime\)</span>.</p></li>
</ul>
</section>
<section id="kalman-filter-special-case" class="slide level2">
<h2>Kalman Filter Special Case</h2>
<ul>
<li><p>The Kalman filter is an example of the LMC where <span class="math inline">\({u}_i(\mathbf{ x}) \rightarrow {x}_i(t)\)</span>.</p></li>
<li><p>I.e. we’ve moved form time input to a more general input space.</p></li>
<li><p>In matrix notation:</p>
<ol type="1">
<li>Kalman filter <span class="math display">\[\mathbf{F}= {\mathbf{W}}\mathbf{X}\]</span></li>
<li>LMC <span class="math display">\[\mathbf{F}= {\mathbf{W}}{\mathbf{U}}\]</span> where the rows of these matrices <span class="math inline">\({\mathbf{F}}\)</span>, <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\({\mathbf{U}}\)</span> each contain <span class="math inline">\(q\)</span> samples from their corresponding functions at a different time (Kalman filter) or spatial location (LMC).</li>
</ol></li>
</ul>
</section>
<section id="intrinsic-coregionalization-model" class="slide level2">
<h2>Intrinsic Coregionalization Model</h2>
<ul>
<li><p>If one covariance used for latent functions (like in Kalman filter).</p></li>
<li><p>This is called the intrinsic coregionalization model (ICM, <span class="citation" data-cites="Goovaerts:book97">Goovaerts (1997)</span>).</p></li>
<li><p>The kernel matrix corresponding to a dataset <span class="math inline">\(\mathbf{X}\)</span> takes the form <span class="math display">\[
\mathbf{K}(\mathbf{X}, \mathbf{X}) =  \mathbf{B}\otimes k(\mathbf{X}, \mathbf{X}).
\]</span></p></li>
</ul>
</section>
<section id="autokrigeability" class="slide level2">
<h2>Autokrigeability</h2>
<ul>
<li><p>If outputs are noise-free, maximum likelihood is equivalent to independent fits of <span class="math inline">\(\mathbf{B}\)</span> and <span class="math inline">\(k(\mathbf{ x}, \mathbf{ x}^\prime)\)</span> <span class="citation" data-cites="Helterbrand:universalCR94">(Helterbrand and Cressie, 1994)</span>.</p></li>
<li><p>In geostatistics this is known as autokrigeability <span class="citation" data-cites="Wackernagel:multivariate03">(Wackernagel, 2003)</span>.</p></li>
<li><p>In multitask learning its the cancellation of intertask transfer <span class="citation" data-cites="Bonilla:multi07">(Bonilla et al., n.d.)</span>.</p></li>
</ul>
</section>
<section id="intrinsic-coregionalization-model-1" class="slide level2">
<h2>Intrinsic Coregionalization Model</h2>
<p><span class="math display">\[
\mathbf{K}(\mathbf{X}, \mathbf{X}) =  \mathbf{ w}\mathbf{ w}^\top  \otimes k(\mathbf{X}, \mathbf{X}).
\]</span></p>
<p><span class="math display">\[
\mathbf{ w}= \begin{bmatrix} 1 \\ 5\end{bmatrix}
\]</span> <span class="math display">\[
\mathbf{B}= \begin{bmatrix} 1 &amp; 5\\ 5&amp;25\end{bmatrix}
\]</span></p>
<!--![image](../../../multigp/tex/diagrams/icmCovarianceImage)![image](../../../multigp/tex/diagrams/icmCovarianceSample1)![image](../../../multigp/tex/diagrams/icmCovarianceSample2)![image](../../../multigp/tex/diagrams/icmCovarianceSample3)![image](../../../multigp/tex/diagrams/icmCovarianceSample4)-->
</section>
<section id="intrinsic-coregionalization-model-covariance" class="slide level2">
<h2>Intrinsic Coregionalization Model Covariance</h2>
<center>
<span class="math display">\[k(i, j, \mathbf{ x}, \mathbf{ x}^\prime) = b_{i,j} k(\mathbf{ x}, \mathbf{ x}^\prime)\]</span>
</center>
<div class="figure">
<div id="icm-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/icm_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/icm_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Intrinsic coregionalization model covariance function.
</aside>
</section>
<section id="intrinsic-coregionalization-model-2" class="slide level2">
<h2>Intrinsic Coregionalization Model</h2>
<p><span class="math display">\[
\mathbf{K}(\mathbf{X}, \mathbf{X}) =  \mathbf{B}\otimes k(\mathbf{X}, \mathbf{X}).
\]</span></p>
<p><span class="math display">\[
\mathbf{B}= \begin{bmatrix} 1 &amp; 0.5\\ 0.5&amp; 1.5\end{bmatrix}
\]</span></p>
<!--![image](../../../multigp/tex/diagrams/icm2CovarianceImage)![image](../../../multigp/tex/diagrams/icm2CovarianceSample1)![image](../../../multigp/tex/diagrams/icm2CovarianceSample2)![image](../../../multigp/tex/diagrams/icm2CovarianceSample3)![image](../../../multigp/tex/diagrams/icm2CovarianceSample4)-->
</section>
<section id="lmc-samples" class="slide level2">
<h2>LMC Samples</h2>
<p><span class="math display">\[\mathbf{K}(\mathbf{X}, \mathbf{X}) = \mathbf{B}_1 \otimes k_1(\mathbf{X}, \mathbf{X}) + \mathbf{B}_2 \otimes k_2(\mathbf{X}, \mathbf{X})\]</span></p>
<p><span class="math display">\[\mathbf{B}_1 = \begin{bmatrix} 1.4 &amp; 0.5\\ 0.5&amp; 1.2\end{bmatrix}\]</span> <span class="math display">\[{\ell}_1 = 1\]</span> <span class="math display">\[\mathbf{B}_2 = \begin{bmatrix} 1 &amp; 0.5\\ 0.5&amp; 1.3\end{bmatrix}\]</span> <span class="math display">\[{\ell}_2 = 0.2\]</span></p>
<!--![image](../../../multigp/tex/diagrams/lmc2CovarianceImage)![image](../../../multigp/tex/diagrams/lmc2CovarianceSample1)![image](../../../multigp/tex/diagrams/lmc2CovarianceSample2)![image](../../../multigp/tex/diagrams/lmc2CovarianceSample3)![image](../../../multigp/tex/diagrams/lmc2CovarianceSample4)-->
</section>
<section id="lmc-in-machine-learning-and-statistics" class="slide level2">
<h2>LMC in Machine Learning and Statistics</h2>
<ul>
<li><p>Used in machine learning for GPs for multivariate regression and in statistics for computer emulation of expensive multivariate computer codes.</p></li>
<li><p>Imposes the correlation of the outputs explicitly through the set of coregionalization matrices.</p></li>
<li><p>Setting <span class="math inline">\(\mathbf{B}= \mathbf{I}_p\)</span> assumes outputs are conditionally independent given the parameters <span class="math inline">\(\boldsymbol{ \theta}\)</span>. <span class="citation" data-cites="Minka:learningtolearn97 Lawrence:learning04 Kai:multitask05">(Lawrence and Platt, 2004; Minka and Picard, 1997; Yu et al., 2005)</span>.</p></li>
<li><p>More recent approaches for multiple output modeling are different versions of the linear model of coregionalization.</p></li>
</ul>
</section>
<section id="semiparametric-latent-factor-model" class="slide level2">
<h2>Semiparametric Latent Factor Model</h2>
<ul>
<li><p>Coregionalization matrices are rank 1 <span class="citation" data-cites="Teh:semiparametric05">Teh et al. (n.d.)</span>. rewrite equation as <span class="math display">\[\mathbf{K}(\mathbf{X}, \mathbf{X}) = \sum_{j=1}^q\mathbf{ w}_{:, {j}}\mathbf{ w}^{\top}_{:, {j}} \otimes k_{j}(\mathbf{X}, \mathbf{X}).\]</span></p></li>
<li><p>Like the Kalman filter, but each latent function has a <em>different</em> covariance.</p></li>
<li><p>Authors suggest using an exponentiated quadratic characteristic length-scale for each input dimension.</p></li>
</ul>
</section>
<section id="semi-parametric-latent-factor-covariance" class="slide level2">
<h2>Semi Parametric Latent Factor Covariance</h2>

</section>
<section id="semiparametric-latent-factor-model-samples" class="slide level2">
<h2>Semiparametric Latent Factor Model Samples</h2>
<p><span class="math display">\[
\mathbf{K}(\mathbf{X}, \mathbf{X}) = \mathbf{ w}_{:, 1}\mathbf{ w}_{:, 1}^\top \otimes k_1(\mathbf{X}, \mathbf{X}) + \mathbf{ w}_{:, 2} \mathbf{ w}_{:, 2}^\top \otimes k_2(\mathbf{X}, \mathbf{X})
\]</span></p>
<p><span class="math display">\[
\mathbf{ w}_1 = \begin{bmatrix} 0.5 \\ 1\end{bmatrix}
\]</span> <span class="math display">\[
\mathbf{ w}_2 = \begin{bmatrix} 1 \\ 0.5\end{bmatrix}
\]</span></p>
<!--![image](../../../multigp/tex/diagrams/slfmCovarianceImage)![image](../../../multigp/tex/diagrams/slfmCovarianceSample1)![image](../../../multigp/tex/diagrams/slfmCovarianceSample2)![image](../../../multigp/tex/diagrams/slfmCovarianceSample3)![image](../../../multigp/tex/diagrams/slfmCovarianceSample4)-->
</section>
<section id="gaussian-processes-for-multi-task-multi-output-and-multi-class" class="slide level2">
<h2>Gaussian processes for Multi-task, Multi-output and Multi-class</h2>
<ul>
<li><p><span class="citation" data-cites="Bonilla:multi07">Bonilla et al. (n.d.)</span> suggest ICM for multitask learning.</p></li>
<li><p>Use a PPCA form for <span class="math inline">\(\mathbf{B}\)</span>: similar to our Kalman filter example.</p></li>
<li><p>Refer to the autokrigeability effect as the cancellation of inter-task transfer.</p></li>
<li><p>Also discuss the similarities between the multi-task GP and the ICM, and its relationship to the SLFM and the LMC.</p></li>
</ul>
</section>
<section id="multitask-classification" class="slide level2">
<h2>Multitask Classification</h2>
<ul>
<li><p>Mostly restricted to the case where the outputs are conditionally independent given the hyperparameters <span class="math inline">\(\boldsymbol{\phi}\)</span> <span class="citation" data-cites="Minka:learningtolearn97 Williams:multiclass98 Lawrence:learning04 Seeger:multiple04 Kai:multitask05 Rasmussen:book06">(Lawrence and Platt, 2004; Minka and Picard, 1997; Rasmussen and Williams, 2006; Seeger and Jordan, 2004; Williams and Barber, 1998; Yu et al., 2005)</span>.</p></li>
<li><p>Intrinsic coregionalization model has been used in the multiclass scenario. <span class="citation" data-cites="Skolidis:multiclass11">Skolidis and Sanguinetti (2011)</span> use the intrinsic coregionalization model for classification, by introducing a probit noise model as the likelihood.</p></li>
<li><p>Posterior distribution is no longer analytically tractable: approximate inference is required.</p></li>
</ul>
</section>
<section id="computer-emulation" class="slide level2">
<h2>Computer Emulation</h2>
<ul>
<li><p>A statistical model used as a surrogate for a computationally expensive computer model.</p></li>
<li><p><span class="citation" data-cites="Higdon:high08">Higdon et al. (2008)</span> use the linear model of coregionalization to model images representing the evolution of the implosion of steel cylinders.</p></li>
<li><p>In <span class="citation" data-cites="Conti:multi09">Conti and O’Hagan (2009)</span> use the ICM to model a vegetation model: called the Sheffield Dynamic Global Vegetation Model <span class="citation" data-cites="Woodward:vegetation98">Woodward et al. (1998)</span>.</p></li>
</ul>
</section>
<section id="modelling-multiple-outputs" class="slide level2">
<h2>Modelling Multiple Outputs</h2>
</section>
<section id="running-example" class="slide level2">
<h2>Running Example</h2>
</section>
<section id="olympic-sprint-data" class="slide level2">
<h2>Olympic Sprint Data</h2>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Sprints for Men and Women</li>
<li>100m, 200m, 400m</li>
<li>In early years of olympics not all events run.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="https://mlatcl.github.io/gpss/./slides/diagrams//ml/100m_final_start.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a> by <a href="https://www.staff.ncl.ac.uk/d.j.wilkinson/">Darren Wilkinson</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-sprint-data-1" class="slide level2">
<h2>Olympic Sprint Data</h2>
<div class="figure">
<div id="olympic-sprint-data-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//datasets/olympic-sprint-data.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Olympic sprint gold medal winning times from <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span>.
</aside>
</section>
<section id="gaussian-process-fit" class="slide level2">
<h2>Gaussian Process Fit</h2>
</section>
<section id="olympic-marathon-data-gp" class="slide level2">
<h2>Olympic Marathon Data GP</h2>
<div class="figure">
<div id="olympic-sprint-gp-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/olympic-sprint-gp.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the Olympic Sprint data.
</aside>
</section>
<section id="olympic-marathon-data-lmc-gp" class="slide level2">
<h2>Olympic Marathon Data LMC GP</h2>
<div class="figure">
<div id="olympic-sprint-lmc-gp-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/olympic-sprint-lmc-gp.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the Olympic Sprint data.
</aside>
</section>
<section id="olympic-marathon-data-icm-gp" class="slide level2">
<h2>Olympic Marathon Data ICM GP</h2>
<div class="figure">
<div id="olympic-sprint-icm-gp-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/olympic-sprint-icm-gp.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the Olympic Sprint data.
</aside>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Bonilla:multi07" class="csl-entry" role="doc-biblioentry">
Bonilla, E.V., Chai, K.M., Williams, C.K.I., n.d. Multi-task <span>G</span>aussian process prediction.
</div>
<div id="ref-Conti:multi09" class="csl-entry" role="doc-biblioentry">
Conti, S., O’Hagan, A., 2009. Bayesian emulation of complex multi-output and dynamic computer models. Journal of Statistical Planning and Inference 140, 640–651. <a href="https://doi.org/doi:10.1016/j.jspi.2009.08.006">https://doi.org/doi:10.1016/j.jspi.2009.08.006</a>
</div>
<div id="ref-Goovaerts:book97" class="csl-entry" role="doc-biblioentry">
Goovaerts, P., 1997. <span>G</span>eostatistics <span>F</span>or <span>N</span>atural <span>R</span>esources <span>E</span>valuation. Oxford University Press.
</div>
<div id="ref-Helterbrand:universalCR94" class="csl-entry" role="doc-biblioentry">
Helterbrand, J.D., Cressie, N.A.C., 1994. Universal cokriging under intrinsic coregionalization. Mathematical Geology 26, 205–226.
</div>
<div id="ref-Higdon:high08" class="csl-entry" role="doc-biblioentry">
Higdon, D.M., Gattiker, J., Williams, B., Rightley, M., 2008. Computer model calibration using high dimensional output. Journal of the American Statistical Association 103, 570–583.
</div>
<div id="ref-Journel:miningBook78" class="csl-entry" role="doc-biblioentry">
Journel, A.G., Huijbregts, C.J., 1978. Mining geostatistics. Academic Press, London.
</div>
<div id="ref-Lawrence:learning04" class="csl-entry" role="doc-biblioentry">
Lawrence, N.D., Platt, J.C., 2004. Learning to learn with the informative vector machine. pp. 512–519. <a href="https://doi.org/10.1145/1015330.1015382">https://doi.org/10.1145/1015330.1015382</a>
</div>
<div id="ref-Minka:learningtolearn97" class="csl-entry" role="doc-biblioentry">
Minka, T.P., Picard, R.W., 1997. Learning how to learn is learning with point sets.
</div>
<div id="ref-Rasmussen:book06" class="csl-entry" role="doc-biblioentry">
Rasmussen, C.E., Williams, C.K.I., 2006. Gaussian processes for machine learning. mit, Cambridge, MA.
</div>
<div id="ref-Rogers:book11" class="csl-entry" role="doc-biblioentry">
Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC Press.
</div>
<div id="ref-Seeger:multiple04" class="csl-entry" role="doc-biblioentry">
Seeger, M., Jordan, M.I., 2004. Sparse <span>G</span>aussian <span>P</span>rocess classification with multiple classes (No. 661). Department of Statistics, University of California at Berkeley.
</div>
<div id="ref-Skolidis:multiclass11" class="csl-entry" role="doc-biblioentry">
Skolidis, G., Sanguinetti, G., 2011. Bayesian multitask classification with <span>G</span>aussian process priors. IEEE Transactions on Neural Networks 22, 2011–2021.
</div>
<div id="ref-Teh:semiparametric05" class="csl-entry" role="doc-biblioentry">
Teh, Y.W., Seeger, M., Jordan, M.I., n.d. Semiparametric latent factor models. pp. 333–340.
</div>
<div id="ref-Wackernagel:multivariate03" class="csl-entry" role="doc-biblioentry">
Wackernagel, H., 2003. Multivariate geostatistics: An introduction with applications, 3rd ed. springer.
</div>
<div id="ref-Williams:multiclass98" class="csl-entry" role="doc-biblioentry">
Williams, C.K.I., Barber, D., 1998. Bayesian <span>C</span>lassification with <span>G</span>aussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence 20, 1342–1351.
</div>
<div id="ref-Woodward:vegetation98" class="csl-entry" role="doc-biblioentry">
Woodward, I., Lomas, M.R., Betts, R.A., 1998. Vegetation-climate feedbacks in a greenhouse world. Philosophical Transactions: Biological Sciences 353, 29–39.
</div>
<div id="ref-Kai:multitask05" class="csl-entry" role="doc-biblioentry">
Yu, K., Tresp, V., Schwaighofer, A., 2005. Learning <span>G</span>aussian processes from multiple tasks, in: Proceedings of the 22nd International Conference on Machine Learning (ICML 2005). pp. 1012–1019.
</div>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
