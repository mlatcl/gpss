<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Unsupervised Learning with Gaussian Processes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="../assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Unsupervised Learning with Gaussian Processes</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="venue" style="text-align:center">Gaussian Process Summer School</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
</section>
<section id="gpy-a-gaussian-process-framework-in-python" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
GPy is a BSD licensed software code base for implementing Gaussian process models in Python. It is designed for teaching and modelling. We welcome contributions which can be made through the Github repository <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</aside>
<center>
<a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</center>
</section>
<section id="gpy-a-gaussian-process-framework-in-python-1" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<ul>
<li>BSD Licensed software base.</li>
<li>Wide availability of libraries, ‘modern’ scripting language.</li>
<li>Allows us to set projects to undergraduates in Comp Sci that use GPs.</li>
<li>Available through GitHub <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></li>
<li>Reproducible Research with Jupyter Notebook.</li>
</ul>
</section>
<section id="features" class="slide level2">
<h2>Features</h2>
<ul>
<li>Probabilistic-style programming (specify the model, not the algorithm).</li>
<li>Non-Gaussian likelihoods.</li>
<li>Multivariate outputs.</li>
<li>Dimensionality reduction.</li>
<li>Approximations for large data sets.</li>
</ul>
<!-- SECTION Probabilistic PCA -->
</section>
<section id="probabilistic-pca" class="slide level2">
<h2>Probabilistic PCA</h2>
<p><span class="math display">\[
\boldsymbol{\Sigma} = \noiseStd^2 \mathbf{I}.
\]</span></p>
<p><span class="math display">\[
p(\mathbf{Y}|\mathbf{W}, \noiseStd^2)
= \prod_{i=1}^n\mathcal{N}\left(\mathbf{ y}_{i, :}|\mathbf{0},\mathbf{W}\mathbf{W}^\top + \noiseStd^2 \mathbf{I}\right)
\]</span></p>
<p><span class="math display">\[
\mathbf{W}= \mathbf{U}\mathbf{L} \mathbf{R}^\top
\]</span></p>
<p><span class="math display">\[
\mathbf{S} = \sum_{i=1}^n(\mathbf{ y}_{i, :} - \boldsymbol{ \mu})(\mathbf{ y}_{i,
:} - \boldsymbol{ \mu})^\top,
\]</span></p>
<p><span class="math display">\[
\ell_i = \sqrt{\lambda_i - \noiseStd^2}
\]</span></p>
</section>
<section id="python-implementation-of-probabilistic-pca" class="slide level2">
<h2>Python Implementation of Probabilistic PCA</h2>
<p>We will now implement this algorithm in python.</p>
<!-- SECTION Posterior for Principal Component Analysis -->
</section>
<section id="posterior-for-principal-component-analysis" class="slide level2">
<h2>Posterior for Principal Component Analysis</h2>
<p><span class="math display">\[
p(\mathbf{ z}_{i, :} | \mathbf{ y}_{i, :})
\]</span></p>
<p><span class="math display">\[
p(\mathbf{ z}_{i, :} | \mathbf{ y}_{i, :}) \propto p(\mathbf{ y}_{i,
:}|\mathbf{W}, \mathbf{ z}_{i, :}, \noiseStd^2) p(\mathbf{ z}_{i, :})
\]</span></p>
<p><span class="math display">\[
\log p(\mathbf{ z}_{i, :} | \mathbf{ y}_{i, :}) = \log p(\mathbf{ y}_{i, :}|\mathbf{W},
\mathbf{ z}_{i, :}, \noiseStd^2) + \log p(\mathbf{ z}_{i, :}) + \text{const}
\]</span></p>
<p><span class="math display">\[
\log p(\mathbf{ z}_{i, :} | \mathbf{ y}_{i, :}) =
-\frac{1}{2\noiseStd^2} (\mathbf{ y}_{i, :} - \mathbf{W}\mathbf{ z}_{i,
:})^\top(\mathbf{ y}_{i, :} - \mathbf{W}\mathbf{ z}_{i, :}) - \frac{1}{2}
\mathbf{ z}_{i, :}^\top \mathbf{ z}_{i, :} + \text{const}
\]</span></p>
</section>
<section id="exercise-0" class="slide level2">
<h2>Exercise 0</h2>
<p>Multiply out the terms in the brackets. Then collect the quadratic term and the linear terms together. Show that the posterior has the form <span class="math display">\[
\mathbf{ z}_{i, :} | \mathbf{W}\sim \mathcal{N}\left(\boldsymbol{ \mu}_x,\mathbf{C}_x\right)
\]</span> where <span class="math display">\[
\mathbf{C}_x = \left(\noiseStd^{-2}
\mathbf{W}^\top\mathbf{W}+ \mathbf{I}\right)^{-1}
\]</span> and <span class="math display">\[
\boldsymbol{ \mu}_x
= \mathbf{C}_x \noiseStd^{-2}\mathbf{W}^\top \mathbf{ y}_{i, :} 
\]</span> Compare this to the posterior for the Bayesian linear regression from last week, do they have similar forms? What matches and what differs?</p>
</section>
<section id="python-implementation-of-the-posterior" class="slide level2">
<h2>Python Implementation of the Posterior</h2>
<p>Now let’s implement the system in code.</p>
</section>
<section id="exercise-1" class="slide level2">
<h2>Exercise 1</h2>
<p>Use the values for <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\noiseStd^2\)</span> you have computed, along with the data set <span class="math inline">\(\mathbf{Y}\)</span> to compute the posterior density over <span class="math inline">\(\mathbf{Z}\)</span>. Write a function of the form</p>
<p>python mu_x, C_x = posterior(Y, W, sigma2)} where <code>mu_x</code> and <code>C_x</code> are the posterior mean and posterior covariance for the given <span class="math inline">\(\mathbf{Y}\)</span>.</p>
<p>Don’t forget to subtract the mean of the data <code>Y</code> inside your function before computing the posterior: remember we assumed at the beginning of our analysis that the data had been centred (i.e. the mean was removed).}{20}</p>
</section>
<section id="numerically-stable-and-efficient-version" class="slide level2">
<h2>Numerically Stable and Efficient Version</h2>
<p>Just as we saw for  and  computation of a matrix such as <span class="math inline">\(\mathbf{Y}^\top\mathbf{Y}\)</span> (or its centred version) can be a bad idea in terms of loss of numerical accuracy. Fortunately, we can find the eigenvalues and eigenvectors of the matrix <span class="math inline">\(\mathbf{Y}^\top\mathbf{Y}\)</span> without direct computation of the matrix. This can be done with the <a href="http://en.wikipedia.org/wiki/Singular_value_decomposition"><em>singular value decomposition</em></a>. The singular value decompsition takes a matrix, <span class="math inline">\(\mathbf{Z}\)</span> and represents it in the form, <span class="math display">\[
\mathbf{Z} = \mathbf{U}\boldsymbol{\Lambda}\mathbf{V}^\top
\]</span> where <span class="math inline">\(\mathbf{U}\)</span> is a matrix of orthogonal vectors in the columns, meaning <span class="math inline">\(\mathbf{U}^\top\mathbf{U} = \mathbf{I}\)</span>. It has the same number of rows and columns as <span class="math inline">\(\mathbf{Z}\)</span>. The matrices <span class="math inline">\(\mathbf{\Lambda}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> are both square with dimensionality given by the number of columns of <span class="math inline">\(\mathbf{Z}\)</span>. The matrix <span class="math inline">\(\mathbf{\Lambda}\)</span> is <em>diagonal</em> and <span class="math inline">\(\mathbf{V}\)</span> is an orthogonal matrix so <span class="math inline">\(\mathbf{V}^\top\mathbf{V} = \mathbf{V}\mathbf{V}^\top = \mathbf{I}\)</span>. The eigenvalues of the matrix <span class="math inline">\(\mathbf{Y}^\top\mathbf{Y}\)</span> are then given by the singular values of the matrix <span class="math inline">\(\mathbf{Y}^\top\)</span> squared and the eigenvectors are given by <span class="math inline">\(\mathbf{U}\)</span>.</p>
</section>
<section id="solution-for-mathbfw" class="slide level2">
<h2>Solution for <span class="math inline">\(\mathbf{W}\)</span></h2>
<p>Given the singular value decomposition of <span class="math inline">\(\mathbf{Y}\)</span> then we have <span class="math display">\[
\mathbf{W}=
\mathbf{U}\mathbf{L}\mathbf{R}^\top
\]</span> where <span class="math inline">\(\mathbf{R}\)</span> is an arbitrary rotation matrix. This implies that the posterior is given by <span class="math display">\[
\mathbf{C}_x =
\left[\noiseStd^{-2}\mathbf{R}\mathbf{L}^2\mathbf{R}^\top + \mathbf{I}\right]^{-1}
\]</span> because <span class="math inline">\(\mathbf{U}^\top \mathbf{U} = \mathbf{I}\)</span>. Since, by convention, we normally take <span class="math inline">\(\mathbf{R} = \mathbf{I}\)</span> to ensure that the principal components are orthonormal we can write <span class="math display">\[
\mathbf{C}_x = \left[\noiseStd^{-2}\mathbf{L}^2 +
\mathbf{I}\right]^{-1}
\]</span> which implies that <span class="math inline">\(\mathbf{C}_x\)</span> is actually diagonal with elements given by <span class="math display">\[
c_i = \frac{\noiseStd^2}{\noiseStd^2 + \ell^2_i}
\]</span> and allows us to write <span class="math display">\[
\boldsymbol{ \mu}_x = [\mathbf{L}^2 + \noiseStd^2
\mathbf{I}]^{-1} \mathbf{L} \mathbf{U}^\top \mathbf{ y}_{i, :}
\]</span> <span class="math display">\[
\boldsymbol{ \mu}_x = \mathbf{D}\mathbf{U}^\top \mathbf{ y}_{i, :}
\]</span> where <span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix with diagonal elements given by <span class="math inline">\(d_{i} = \frac{\ell_i}{\noiseStd^2 + \ell_i^2}\)</span>.</p>
</section>
<section id="difficulty-for-probabilistic-approaches" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="nonlinear-mapping-3d-plot-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/nonlinear-mapping-3d-plot.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A two dimensional grid mapped into three dimensions to form a two dimensional manifold.
</aside>
</section>
<section id="difficulty-for-probabilistic-approaches-1" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="non-linear-mapping-2d-plot-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/nonlinear-mapping-2d-plot.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A one dimensional line mapped into two dimensions by two separate independent functions. Each point can be mapped exactly through the mappings.
</aside>
</section>
<section id="difficulty-for-probabilistic-approaches-2" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="gaussian-through-nonlinear-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/gaussian-through-nonlinear.svg" width="100%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A Gaussian density over the input of a non linear function leads to a very non Gaussian output. Here the output is multimodal.
</aside>
</section>
<section id="getting-started-and-downloading-data" class="slide level2">
<h2>Getting Started and Downloading Data</h2>
</section>
<section id="principal-component-analysis" class="slide level2">
<h2>Principal Component Analysis</h2>
<ul>
<li>What is the right shape <span class="math inline">\(n\times p\)</span> to use?</li>
</ul>
</section>
<section id="gaussian-process-latent-variable-model" class="slide level2">
<h2>Gaussian Process Latent Variable Model</h2>

</section>
<section id="example-latent-doodle-space" class="slide level2">
<h2>Example: Latent Doodle Space</h2>
<div class="figure">
<div id="latent-doodle-space-figure" class="figure-frame">
<iframe width height src="https://player.vimeo.com/video/3235882#t=" frameborder="0" allow="autoplay; fullscreen" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
The latent doodle space idea of <span class="citation" data-cites="Baxter:doodle06">Baxter and Anjyo (2006)</span> manages to build a smooth mapping across very sparse data.
</aside>
</section>
<section id="example-latent-doodle-space-1" class="slide level2">
<h2>Example: Latent Doodle Space</h2>
<p><strong>Generalization with much less Data than Dimensions</strong></p>
<ul>
<li><p>Powerful uncertainly handling of GPs leads to surprising properties.</p></li>
<li><p>Non-linear models can be used where there are fewer data points than dimensions <em>without overfitting</em>.</p></li>
</ul>
<p><span style="text-align:right"><span class="citation" data-cites="Baxter:doodle06">(Baxter and Anjyo, 2006)</span></span></p>
</section>
<section id="example-continuous-character-control" class="slide level2">
<h2>Example: Continuous Character Control</h2>
<p><span style="text-align:right"><span class="citation" data-cites="Levine:control12">Levine et al. (2012)</span></span> - Graph diffusion prior for enforcing connectivity between motions. <span class="math display">\[\log p(\mathbf{X}) = w_c \sum_{i,j} \log K_{ij}^d\]</span> with the graph diffusion kernel <span class="math inline">\(\mathbf{K}^d\)</span> obtain from <span class="math display">\[K_{ij}^d = \exp(\beta \mathbf{H})
    \qquad \text{with} \qquad \mathbf{H} = -\mathbf{T}^{-1/2} \mathbf{L} \mathbf{T}^{-1/2}\]</span> the graph Laplacian, and <span class="math inline">\(\mathbf{T}\)</span> is a diagonal matrix with <span class="math inline">\(T_{ii} = \sum_j w(\mathbf{ x}_i, \mathbf{ x}_j)\)</span>, <span class="math display">\[L_{ij} = \begin{cases} \sum_k w(\mathbf{ x}_i,\mathbf{ x}_k) &amp; \text{if $i=j$}
    \\
    -w(\mathbf{ x}_i,\mathbf{ x}_j) &amp;\text{otherwise.}
    \end{cases}\]</span> and <span class="math inline">\(w(\mathbf{ x}_i,\mathbf{ x}_j) = || \mathbf{ x}_i - \mathbf{ x}_j||^{-p}\)</span> measures similarity.</p>
</section>
<section id="character-control-results" class="slide level2">
<h2>Character Control: Results</h2>
<div class="figure">
<div id="charcter-control-gplvm-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/hr3pdDl5IAg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
Character control in the latent space described the the GP-LVM <span class="citation" data-cites="Levine:control12">Levine et al. (2012)</span>
</aside>
</section>
<section id="data-for-blastocyst-development-in-mice-single-cell-taqman-arrays" class="slide level2">
<h2>Data for Blastocyst Development in Mice: Single Cell TaqMan Arrays</h2>
</section>
<section id="principal-component-analysis-1" class="slide level2">
<h2>Principal Component Analysis</h2>
</section>
<section id="pca-result" class="slide level2">
<h2>PCA Result</h2>
<div class="figure">
<div id="singlecell-data-pca-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/singlecell-data-pca.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
First two principal compoents of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data.
</aside>
</section>
<section id="gp-lvm-on-the-data" class="slide level2">
<h2>GP-LVM on the Data</h2>
<div class="figure">
<div id="singlecell-gplvm-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gplvm/singlecell-gplvm.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data with the GP-LVM.
</aside>
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="figure">
<div id="singlecell-gplvm-ard-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gplvm/singlecell-gplvm-ard.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The ARD parameters of the GP-LVM for the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data.
</aside>
</section>
<section id="blastocyst-data-isomap" class="slide level2">
<h2>Blastocyst Data: Isomap</h2>
<div class="figure">
<div id="singlecell-isomap-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/singlecell-isomap.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data with Isomap.
</aside>
</section>
<section id="blastocyst-data-locally-linear-embedding" class="slide level2">
<h2>Blastocyst Data: Locally Linear Embedding</h2>
<div class="figure">
<div id="singlecell-lle-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/singlecell-lle.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data with a locally linear embedding.
</aside>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></p></li>
<li><p>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></p></li>
<li><p>blog posts:</p>
<p><a href="http://inverseprobability.com/2014/07/01/open-data-science">Open Data Science</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Baxter:doodle06">
<p>Baxter, W.V., Anjyo, K.-I., 2006. Latent doodle space, in: EUROGRAPHICS. Vienna, Austria, pp. 477–485. <a href="https://doi.org/10.1111/j.1467-8659.2006.00967.x">https://doi.org/10.1111/j.1467-8659.2006.00967.x</a></p>
</div>
<div id="ref-Guo:fate10">
<p>Guo, G., Huss, M., Tong, G.Q., Wang, C., Sun, L.L., Clarke, N.D., Robsonemail, P., 2010. Resolution of cell fate decisions revealed by single-cell gene expression analysis from zygote to blastocyst. Developmental Cell 18, 675–685. <a href="https://doi.org/10.1016/j.devcel.2010.02.012">https://doi.org/10.1016/j.devcel.2010.02.012</a></p>
</div>
<div id="ref-Levine:control12">
<p>Levine, S., Wang, J.M., Haraux, A., Popović, Z., Koltun, V., 2012. Continuous character control with low-dimensional embeddings. ACM Transactions on Graphics (SIGGRAPH 2012) 31.</p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
