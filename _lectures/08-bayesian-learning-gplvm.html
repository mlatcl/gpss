---
title: "Bayesian Learning of GP-LVM"
venue: "Gaussian Process Summer School"
abstract: "null"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: University of Cambridge
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orchid: 
time: "null"
week: 0
session: 8
reveal: 08-bayesian-learning-gplvm.slides.html
ipynb: 08-bayesian-learning-gplvm.ipynb
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<ul>
<li>GP-LVM Provides probabilistic non-linear dimensionality reduction.</li>
<li>How to select the dimensionality?</li>
<li>Need to estimate marginal likelihood.</li>
<li>In standard GP-LVM it increases with increasing <span class="math inline">$\latentDim$</span>.</li>
</ul>
<table>
<tr>
<td width="40%">
<p><strong>Bayesian GP-LVM</strong></p>
<ul>
<li>Start with a standard GP-LVM.</li>
<li>Apply standard latent variable approach:
<ul>
<li>Define Gaussian prior over , <span class="math inline">$\latentMatrix$</span>.</li>
<li>Integrate out .</li>
<li>Unfortunately integration is intractable.</li>
</ul></li>
</ul>
</td>
<td width="60%">
<center>
{ }
</center>
</td>
</tr>
</table>
<h2 id="standard-variational-approach-fails">Standard Variational Approach Fails</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/variational-bayes-gplvm-long.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/variational-bayes-gplvm-long.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<ul>
<li>Standard variational bound has the form: <br /><span class="math display">$$
\likelihoodBound = \expDist{\log p(\dataVector|\latentMatrix)}{q(\latentMatrix)} + \KL{q(\latentMatrix)}{p(\latentMatrix)}
$$</span><br /></li>
</ul>
<p>The standard variational approach would require the expectation of <span class="math inline">$\log p(\dataVector|\latentMatrix)$</span> under <span class="math inline">$q(\latentMatrix)$</span>. <br /><span class="math display">$$
  \begin{align}
  \log p(\dataVector|\latentMatrix) = &amp; -\frac{1}{2}\dataVector^\top\left(\kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector}+\dataStd^2\eye\right)^{-1}\dataVector \\ &amp; -\frac{1}{2}\log \det{\kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector}+\dataStd^2 \eye} -\frac{\numData}{2}\log 2\pi
  \end{align}
  $$</span><br /> But this is extremely difficult to compute because <span class="math inline">$\kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector}$</span> is dependent on <span class="math inline">$\latentMatrix$</span> and it appears in the inverse.</p>
<h2 id="variational-bayesian-gp-lvm">Variational Bayesian GP-LVM</h2>
<p>The alternative approach is to consider the collapsed variational bound (used for low rank (sparse is a misnomer) Gaussian process approximations. <br /><span class="math display">$$
    p(\dataVector)\geq \prod_{i=1}^\numData c_i \int \gaussianDist{\dataVector}{\expSamp{\mappingFunctionVector}}{\dataStd^2\eye}p(\inducingVector) \text{d}\inducingVector
  $$</span><br /> <br /><span class="math display">$$
    p(\dataVector|\latentMatrix )\geq \prod_{i=1}^\numData c_i \int \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye}p(\inducingVector) \text{d}\inducingVector
  $$</span><br /> <br /><span class="math display">$$
      \int p(\dataVector|\latentMatrix)p(\latentMatrix) \text{d}\latentMatrix \geq \int \prod_{i=1}^\numData c_i \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye} p(\latentMatrix)\text{d}\latentMatrix p(\inducingVector) \text{d}\inducingVector
  $$</span><br /></p>
<p>To integrate across <span class="math inline">$\latentMatrix$</span> we apply the lower bound to the inner integral. <br /><span class="math display">$$
    \begin{align}
    \int \prod_{i=1}^\numData c_i \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye} p(\latentMatrix)\text{d}\latentMatrix \geq &amp; \expDist{\sum_{i=1}^\numData\log  c_i}{q(\latentMatrix)}\\ &amp; +\expDist{\log\gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye}}{q(\latentMatrix)}\\&amp; + \KL{q(\latentMatrix)}{p(\latentMatrix)}    
    \end{align}
  $$</span><br /> * Which is analytically tractable for Gaussian <span class="math inline">$q(\latentMatrix)$</span> and some covariance functions.</p>
<ul>
<li><p>Need expectations under <span class="math inline">$q(\latentMatrix)$</span> of: <br /><span class="math display">$$
\log c_i = \frac{1}{2\dataStd^2} \left[\kernelScalar_{i, i} - \kernelVector_{i, \inducingVector}^\top \kernelMatrix_{\inducingVector, \inducingVector}^{-1} \kernelVector_{i, \inducingVector}\right]
$$</span><br /> and <br /><span class="math display">$$
\log \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector,\dataMatrix)}}{\dataStd^2\eye} = -\frac{1}{2}\log 2\pi\dataStd^2 - \frac{1}{2\dataStd^2}\left(\dataScalar_i - \kernelMatrix_{\mappingFunctionVector, \inducingVector}\kernelMatrix_{\inducingVector,\inducingVector}^{-1}\inducingVector\right)^2
$$</span><br /></p></li>
<li><p>This requires the expectations <br /><span class="math display">$$
\expDist{\kernelMatrix_{\mappingFunctionVector,\inducingVector}}{q(\latentMatrix)}
$$</span><br /> and <br /><span class="math display">$$
\expDist{\kernelMatrix_{\mappingFunctionVector,\inducingVector}\kernelMatrix_{\inducingVector,\inducingVector}^{-1}\kernelMatrix_{\inducingVector,\mappingFunctionVector}}{q(\latentMatrix)}
$$</span><br /> which can be computed analytically for some covariance functions <span class="citation" data-cites="Damianou:variational15">(Damianou, Titsias, and Lawrence 2016)</span> or through sampling <span class="citation" data-cites="Damianou:thesis2015 Salimbeni:doubly2017">(Damianou 2015; Salimbeni and Deisenroth 2017)</span>.</p></li>
</ul>
<h2 id="manifold-relevance-determination">Manifold Relevance Determination</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/mrd-gplvm.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/mrd-gplvm.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<h1 id="modeling-multiple-views">Modeling Multiple ‘Views’</h1>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/shared-intro.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/shared-intro.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<ul>
<li>Single space to model correlations between two different data sources, e.g., images &amp; text, image &amp; pose.</li>
<li>Shared latent spaces: {}</li>
<li>Effective when the `views’ are correlated.</li>
<li>But not all information is shared between both `views’.</li>
<li>PCA applied to concatenated data vs CCA applied to data.</li>
</ul>
<h1 id="shared-private-factorization">Shared-Private Factorization</h1>
<ul>
<li>In real scenarios, the `views’ are neither fully independent, nor fully correlated.</li>
<li>Shared models
<ul>
<li>either allow information relevant to a single view to be mixed in the shared signal,</li>
<li>or are unable to model such private information.</li>
</ul></li>
<li>Solution: Model shared and private information <span class="citation" data-cites="Klami:group11">Virtanen, Klami, and Kaski (n.d.)</span>,<span class="citation" data-cites="Ek:ambiguity08">Ek et al. (2008)</span>,<span class="citation" data-cites="Leen:gplvmcca06">Leen and Fyfe (2006)</span>,<span class="citation" data-cites="Klami:local07">Klami and Kaski (n.d.)</span>,<span class="citation" data-cites="Klami:probabilistic08">Klami and Kaski (2008)</span>,<span class="citation" data-cites="Tucker:battery58">Tucker (1958)</span></li>
<li>Probabilistic CCA is case when dimensionality of <span class="math inline"><strong>Z</strong></span> matches <span class="math inline">$\dataMatrix^{(i)}$</span> (cf Inter Battery Factor Analysis {}).</li>
</ul>
<!--frame failure start-->
<h1 id="manifold-relevance-determination-1">Manifold Relevance Determination</h1>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/mrd-intro.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/mrd-intro.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<pre><code>\def\layersep{2cm}
\begin{center}
  \begin{tikzpicture}[node distance=\layersep]</code></pre>
<p> = [text width=4em, text centered] % Draw the input layer nodes / in {1,…,8} % This is the same as writing / in {1/1,2/2,3/3,4/4} (Y-) at (, 0) {<span class="math inline">$\dataScalar_\x$</span>};</p>
<pre><code>% Draw the hidden layer nodes
\foreach \name / \x in {1,...,6}
    \path[xshift=1cm]
        node[latent] (X-\name) at (\x cm, \layersep) {$\latentScalar_\x$};

% Connect every node in the latent layer with every node in the
% data layer.
\foreach \source in {1,...,6}
    \foreach \dest in {1,...,8}
        \draw[-&gt;] (X-\source) -- (Y-\dest);



%  Annotate the layers
\node[annot,left of=X-1, node distance=1cm] (ls) {Latent space};
\node[annot,left of=Y-1, node distance=1cm] (ds) {Data space};</code></pre>
<p>\end{tikzpicture} \end{center}</p>
<!--frame failure end-->
<!--frame failure start-->
<h1 id="shared-gp-lvm">Shared GP-LVM</h1>
<pre><code>\def\layersep{2cm}
\begin{center}
  \begin{tikzpicture}[node distance=\layersep]</code></pre>
<p> = [text width=4em, text centered] % Draw the input layer nodes / in {1,…,4} % This is the same as writing / in {1/1,2/2,3/3,4/4} (Y-) at (, 0) {<span class="math inline">$\dataScalar^{(1)}_\x$</span>};</p>
<pre><code>\foreach \name / \x in {1,...,4}
% This is the same as writing \foreach \name / \x in {1/1,2/2,3/3,4/4}
    \node[obs] (Z-\name) at (\x+5, 0) {$\dataScalar^{(2)}_\x$};

% Draw the hidden layer nodes
\foreach \name / \x in {1,...,6}
    \path[xshift=2cm]
        node[latent] (X-\name) at (\x cm, \layersep) {$\latentScalar_\x$};

% Connect every node in the latent layer with every node in the
% data layer.
\foreach \source in {1,...,6}
    \foreach \dest in {1,...,4}
        \draw[-&gt;] (X-\source) -- (Y-\dest);

\foreach \source in {1,...,6}
    \foreach \dest in {1,...,4}
        \draw[-&gt;] (X-\source) -- (Z-\dest);


%  Annotate the layers
\node[annot,left of=X-1, node distance=1cm] (ls) {Latent space};
\node[annot,left of=Y-1, node distance=1cm] (ds) {Data space};</code></pre>
<p>\end{tikzpicture} Separate ARD parameters for mappings to <span class="math inline">$\dataMatrix^{(1)}$</span> and <span class="math inline">$\dataMatrix^{(2)}$</span>. \end{center}</p>
<h1 id="manifold-relevance-determination-results">Manifold Relevance Determination Results</h1>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/mrd-results.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/mrd-results.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="figure">
<div id="yale-faces-0-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gplvm/andreas-deep-talk-0.svg" width="90%" style=" ">
</object>
</div>
<div id="yale-faces-0-magnify" class="magnify" onclick="magnifyFigure(&#39;yale-faces-0&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="yale-faces-0-caption" class="caption-frame">
<p>Figure: The Yale Faces data set shows different people under different lighting conditions</p>
</div>
</div>
<div class="figure">
<div id="yale-faces-1-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gplvm/andreas-deep-talk-1.svg" width="90%" style=" ">
</object>
</div>
<div id="yale-faces-1-magnify" class="magnify" onclick="magnifyFigure(&#39;yale-faces-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="yale-faces-1-caption" class="caption-frame">
<p>Figure: ARD Demonstrates not all of the latent space is used.</p>
</div>
</div>
<div class="figure">
<div id="yale-faces-2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gplvm/andreas-deep-talk-2.svg" width="90%" style=" ">
</object>
</div>
<div id="yale-faces-2-magnify" class="magnify" onclick="magnifyFigure(&#39;yale-faces-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="yale-faces-2-caption" class="caption-frame">
<p>Figure: Other applications include inferring pose from silhouette</p>
</div>
</div>
<div class="figure">
<div id="manifold-relevance-determination-figure" class="figure-frame">
<iframe width="800" height="600" src="https://www.youtube.com/embed/UvLI8o8z4IU?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="manifold-relevance-determination-magnify" class="magnify" onclick="magnifyFigure(&#39;manifold-relevance-determination&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="manifold-relevance-determination-caption" class="caption-frame">
<p>Figure: A short video description of the Manifold Relevance Determination method as published at ICML 2012</p>
</div>
</div>
<!--frame end-->
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Damianou:thesis2015">
<p>Damianou, Andreas. 2015. “Deep Gaussian Processes and Variational Propagation of Uncertainty.” PhD thesis, University of Sheffield.</p>
</div>
<div id="ref-Damianou:variational15">
<p>Damianou, Andreas, Michalis K. Titsias, and Neil D. Lawrence. 2016. “Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes.” <em>Journal of Machine Learning Research</em> 17.</p>
</div>
<div id="ref-Ek:ambiguity08">
<p>Ek, Carl Henrik, Jon Rihan, Philip H. S. Torr, Gregory Rogez, and Neil D. Lawrence. 2008. “Ambiguity Modeling in Latent Spaces.” In <em>Machine Learning for Multimodal Interaction (Mlmi 2008)</em>, edited by Andrei Popescu-Belis and Rainer Stiefelhagen, 62–73. LNCS. Springer-Verlag.</p>
</div>
<div id="ref-Klami:probabilistic08">
<p>Klami, Aarto, and Sami Kaski. 2008. “Probabilistic Approach to Detecting Dependencies Between Data Sets.” <em>Neurocomputing</em> 72: 39–46.</p>
</div>
<div id="ref-Klami:local07">
<p>———. n.d. “Local Dependent Components Analysis.” In.</p>
</div>
<div id="ref-Leen:gplvmcca06">
<p>Leen, Gayle, and Colin Fyfe. 2006. “A Gaussian Process Latent Variable Model Formulation of Canonical Correlation Analysis.” In. Bruges (Belgium).</p>
</div>
<div id="ref-Salimbeni:doubly2017">
<p>Salimbeni, Hugh, and Marc Deisenroth. 2017. “Doubly Stochastic Variational Inference for Deep Gaussian Processes.” In <em>Advances in Neural Information Processing Systems 30</em>, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 4591–4602. Curran Associates, Inc. <a href="http://papers.nips.cc/paper/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.pdf">http://papers.nips.cc/paper/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.pdf</a>.</p>
</div>
<div id="ref-Tucker:battery58">
<p>Tucker, Ledyard R. 1958. “An Inter-Battery Method of Factor Analysis.” <em>Psychometrika</em> 23 (2): 111–36.</p>
</div>
<div id="ref-Klami:group11">
<p>Virtanen, S., Aarto Klami, and Sami Kaski. n.d. “Bayesian CCA via Group Sparsity.” In.</p>
</div>
</div>

