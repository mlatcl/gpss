---
title: "Covariance Functions and Hyperparameter Optimizaton"
venue: "Gaussian Process Summer School"
abstract: "<p>In this talk we review multi-output Gaussian processes. Introducing them initially through a Kalman filter representation of a GP.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: 
  institute: 
  twitter: 
  gscholar: 
  orchid: 
time: "null"
week: 0
session: 3
reveal: 03-covariance-functions.slides.html
ipynb: 03-covariance-functions.ipynb
layout: lecture
categories:
- notes
---



<!-- To compile -->
<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h2 id="the-importance-of-the-covariance-function">The Importance of the Covariance Function</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-covariance-function-importance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-covariance-function-importance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The covariance function encapsulates our assumptions about the data. The equations for the distribution of the prediction function, given the training observations, are highly sensitive to the covariation between the test locations and the training locations as expressed by the matrix <span class="math inline">$\kernelMatrix_*$</span>. We defined a matrix <span class="math inline"><strong>A</strong></span> which allowed us to express our conditional mean in the form, <br /><span class="math display">$$
\meanVector_\mappingFunction = \mathbf{A}^\top \dataVector,
$$</span><br /> where <span class="math inline">$\dataVector$</span> were our <em>training observations</em>. In other words our mean predictions are always a linear weighted combination of our <em>training data</em>. The weights are given by computing the covariation between the training and the test data (<span class="math inline">$\kernelMatrix_*$</span>) and scaling it by the inverse covariance of the training data observations, <span class="math inline">$\left[\kernelMatrix + \dataStd^2 \eye\right]^{-1}$</span>. This inverse is the main computational object that needs to be resolved for a Gaussian process. It has a computational burden which is <span class="math inline">$O(\numData^3)$</span> and a storage burden which is <span class="math inline">$O(\numData^2)$</span>. This makes working with Gaussian processes computationally intensive for the situation where <span class="math inline">$\numData&gt;10,000$</span>.</p>
<div class="figure">
<div id="intro-to-gps-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/ewJ3AxKclOg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="intro-to-gps-magnify" class="magnify" onclick="magnifyFigure(&#39;intro-to-gps&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="intro-to-gps-caption" class="caption-frame">
<p>Figure: Introduction to Gaussian processes given by Neil Lawrence at the 2014 Gaussian process Winter School at the University of Sheffield.</p>
</div>
</div>
<h2 id="improving-the-numerics">Improving the Numerics</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-numerics-and-optimization.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-numerics-and-optimization.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>In practice we shouldn’t be using matrix inverse directly to solve the GP system. One more stable way is to compute the <em>Cholesky decomposition</em> of the kernel matrix. The log determinant of the covariance can also be derived from the Cholesky decomposition.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">from</span> mlai <span class="im">import</span> update_inverse</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>GP.update_inverse <span class="op">=</span> update_inverse</span></code></pre></div>
<h2 id="capacity-control">Capacity Control</h2>
<p>Gaussian processes are sometimes seen as part of a wider family of methods known as kernel methods. Kernel methods are also based around covariance functions, but in the field they are known as Mercer kernels. Mercer kernels have interpretations as inner products in potentially infinite dimensional Hilbert spaces. This interpretation arises because, if we take <span class="math inline"><em>α</em> = 1</span>, then the kernel can be expressed as <br /><span class="math display">$$
\kernelMatrix = \basisMatrix\basisMatrix^\top 
$$</span><br /> which imples the elements of the kernel are given by, <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \basisVector(\inputVector)^\top \basisVector(\inputVector^\prime).
$$</span><br /> So we see that the kernel function is developed from an inner product between the basis functions. Mercer’s theorem tells us that any valid <em>positive definite function</em> can be expressed as this inner product but with the caveat that the inner product could be <em>infinite length</em>. This idea has been used quite widely to <em>kernelize</em> algorithms that depend on inner products. The kernel functions are equivalent to covariance functions and they are parameterized accordingly. In the kernel modeling community it is generally accepted that kernel parameter estimation is a difficult problem and the normal solution is to cross validate to obtain parameters. This can cause difficulties when a large number of kernel parameters need to be estimated. In Gaussian process modelling kernel parameter estimation (in the simplest case proceeds) by maximum likelihood. This involves taking gradients of the likelihood with respect to the parameters of the covariance function.</p>
<h2 id="gradients-of-the-likelihood">Gradients of the Likelihood</h2>
<p>The easiest conceptual way to obtain the gradients is a two step process. The first step involves taking the gradient of the likelihood with respect to the covariance function, the second step involves considering the gradient of the covariance function with respect to its parameters.</p>
<h2 id="overall-process-scale">Overall Process Scale</h2>
<p>In general we won’t be able to find parameters of the covariance function through fixed point equations, we will need to do gradient based optimization.</p>
<h2 id="capacity-control-and-data-fit">Capacity Control and Data Fit</h2>
<p>The objective function can be decomposed into two terms, a capacity control term, and a data fit term. The capacity control term is the log determinant of the covariance. The data fit term is the matrix inner product between the data and the inverse covariance.</p>
<h2 id="learning-covariance-parameters">Learning Covariance Parameters</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Can we determine covariance parameters from the data?</p>
<p><br /><span class="math display">$$
\gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}{\det{\kernelMatrix}^{\frac{1}{2}}}}{\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}
$$</span><br /></p>
<p><br /><span class="math display">$$
\begin{aligned}
    \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}\color{blue}{\det{\kernelMatrix}^{\frac{1}{2}}}}\color{red}{\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}
\end{aligned}
$$</span><br /></p>
<p><br /><span class="math display">$$
\begin{aligned}
    \log \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=&amp;\color{blue}{-\frac{1}{2}\log\det{\kernelMatrix}}\color{red}{-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}} \\ &amp;-\frac{\numData}{2}\log2\pi
\end{aligned}
$$</span><br /></p>
<p><br /><span class="math display">$$
\errorFunction(\parameterVector) = \color{blue}{\frac{1}{2}\log\det{\kernelMatrix}} + \color{red}{\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}}
$$</span><br /></p>
<h2 id="capacity-control-through-the-determinant">Capacity Control through the Determinant</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize-capacity.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize-capacity.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The parameters are <em>inside</em> the covariance function (matrix).  <br /><span class="math display">$$\kernelScalar_{i, j} = \kernelScalar(\inputVals_i, \inputVals_j; \parameterVector)$$</span><br /></p>
<p><span> <br /><span class="math display">$$\kernelMatrix = \rotationMatrix \eigenvalueMatrix^2 \rotationMatrix^\top$$</span><br /></span></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>gpoptimizePlot1</span></code></pre></div>
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp-optimize-eigen.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<span class="math inline">$\eigenvalueMatrix$</span> represents distance on axes. <span class="math inline">$\rotationMatrix$</span> gives rotation.
</td>
</tr>
</table>
<ul>
<li><span class="math inline">$\eigenvalueMatrix$</span> is <em>diagonal</em>, <span class="math inline">$\rotationMatrix^\top\rotationMatrix = \eye$</span>.</li>
<li>Useful representation since <span class="math inline">$\det{\kernelMatrix} = \det{\eigenvalueMatrix^2} = \det{\eigenvalueMatrix}^2$</span>.</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>diagrams <span class="op">=</span> <span class="st">&#39;./gp/&#39;</span></span></code></pre></div>
<div class="figure">
<div id="gp-optimise-determinant-figure-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant009.svg" width="80%" style=" ">
</object>
</div>
<div id="gp-optimise-determinant-figure-magnify" class="magnify" onclick="magnifyFigure(&#39;gp-optimise-determinant-figure&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-optimise-determinant-figure-caption" class="caption-frame">
<p>Figure: The determinant of the covariance is dependent only on the eigenvalues. It represents the ‘footprint’ of the Gaussian.</p>
</div>
</div>
<div class="figure">
<div id="gp-optimise-quadratic-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/diagrams/gp-optimise-quadratic002.svg" width="80%" style=" ">
</object>
</div>
<div id="gp-optimise-quadratic-magnify" class="magnify" onclick="magnifyFigure(&#39;gp-optimise-quadratic&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-optimise-quadratic-caption" class="caption-frame">
<p>Figure: The data fit term of the Gaussian process is a quadratic loss centered around zero. This has eliptical contours, the principal axes of which are given by the covariance matrix.</p>
</div>
</div>
<h2 id="quadratic-data-fit">Quadratic Data Fit</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize-data-fit.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize-data-fit.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<h2 id="data-fit-term">Data Fit Term</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize-data-fit-capacity.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize-data-fit-capacity.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="figure">
<div id="gp-optimise-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise006.svg" width="100%" style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise010.svg" width="100%" style=" ">
</object>
</td>
</tr>
</table>
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise016.svg" width="100%" style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise021.svg" width="100%" style=" ">
</object>
</td>
</tr>
</table>
</div>
<div id="gp-optimise-magnify" class="magnify" onclick="magnifyFigure(&#39;gp-optimise&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-optimise-caption" class="caption-frame">
<p>Figure: Variation in the data fit term, the capacity term and the negative log likelihood for different lengthscales.</p>
</div>
</div>
<h2 id="exponentiated-quadratic-covariance">Exponentiated Quadratic Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/eq-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/eq-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The exponentiated quadratic covariance, also known as the Gaussian covariance or the RBF covariance and the squared exponential. Covariance between two points is related to the negative exponential of the squared distnace between those points. This covariance function can be derived in a few different ways: as the infinite limit of a radial basis function neural network, as diffusion in the heat equation, as a Gaussian filter in <em>Fourier space</em> or as the composition as a series of linear filters applied to a base function.</p>
<p>The covariance takes the following form, <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector-\inputVector^\prime}^2}{2\lengthScale^2}\right)
$$</span><br /> where <span class="math inline">ℓ</span> is the <em>length scale</em> or <em>time scale</em> of the process and <span class="math inline"><em>α</em></span> represents the overall process variance.</p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector-\inputVector^\prime}^2}{2\lengthScale^2}\right)$$</span><br />
</center>
<div class="figure">
<div id="eq-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/eq_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/eq_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="eq-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;eq-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="eq-covariance-plot-caption" class="caption-frame">
<p>Figure: The exponentiated quadratic covariance function.</p>
</div>
</div>
<h2 id="brownian-covariance">Brownian Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/brownian-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/brownian-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="im">from</span> mlai <span class="im">import</span> brownian_cov</span></code></pre></div>
<p>Brownian motion is also a Gaussian process. It follows a Gaussian random walk, with diffusion occuring at each time point driven by a Gaussian input. This implies it is both Markov and Gaussian. The covariance function for Brownian motion has the form <br /><span class="math display">$$
\kernelScalar(t, t^\prime)=\alpha \min(t, t^\prime)
$$</span><br /></p>
<center>
<br /><span class="math display">$$\kernelScalar(t, t^\prime)=\alpha \min(t, t^\prime)$$</span><br />
</center>
<div class="figure">
<div id="brownian-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/brownian_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/brownian_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="brownian-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;brownian-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="brownian-covariance-plot-caption" class="caption-frame">
<p>Figure: Brownian motion covariance function.</p>
</div>
</div>
<h2 id="polynomial-covariance">Polynomial Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/poly-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/poly-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha(w \inputVector^\top\inputVector^\prime + b)^d$$</span><br />
</center>
<div class="figure">
<div id="polynomial-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/polynomial_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/polynomial_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="polynomial-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;polynomial-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="polynomial-covariance-plot-caption" class="caption-frame">
<p>Figure: Polynomial covariance function.</p>
</div>
</div>
<h2 id="periodic-covariance">Periodic Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/periodic-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/periodic-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha\exp\left(\frac{-2\sin(\pi rw)^2}{\lengthScale^2}\right)$$</span><br />
</center>
<div class="figure">
<div id="periodic-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/periodic_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/periodic_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="periodic-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;periodic-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="periodic-covariance-plot-caption" class="caption-frame">
<p>Figure: Periodic covariance function.</p>
</div>
</div>
<h2 id="sinc-covariance">Sinc Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/sinc-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/sinc-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Another approach to developing covariance function exploits Bochner’s theorem <span class="citation" data-cites="Bochner:book59">Bochner (1959)</span>. Bochner’s theorem tells us that any positve filter in Fourier space implies has an associated Gaussian process with a stationary covariance function. The covariance function is the <em>inverse Fourier transform</em> of the filter applied in Fourier space.</p>
<p>For example, in signal processing, <em>band limitations</em> are commonly applied as an assumption. For example, we may believe that no frequency above <span class="math inline"><em>w</em> = 2</span> exists in the signal. This is equivalent to a rectangle function being applied as a the filter in Fourier space.</p>
<p>The inverse Fourier transform of the rectangle function is the <span class="math inline">sinc( ⋅ )</span> function. So the sinc is a valid covariance function, and it represents <em>band limited</em> signals.</p>
<p>Note that other covariance functions we’ve introduced can also be interpreted in this way. For example, the exponentiated quadratic covariance function can be Fourier transformed to see what the implied filter in Fourier space is. The Fourier transform of the exponentiated quadratic is an exponentiated quadratic, so the standard EQ-covariance implies a EQ filter in Fourier space.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="im">from</span> mlai <span class="im">import</span> sinc_cov</span></code></pre></div>
<h2 id="mlp-covariance">MLP Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/mlp-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/mlp-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="im">from</span> mlai <span class="im">import</span> mlp_cov</span></code></pre></div>
<p>The multi-layer perceptron (MLP) covariance, also known as the neural network covariance or the arcsin covariance, is derived by considering the infinite limit of a neural network.</p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \arcsin\left(\frac{w \inputVector^\top \inputVector^\prime + b}{\sqrt{\left(w \inputVector^\top \inputVector + b + 1\right)\left(w \left.\inputVector^\prime\right.^\top \inputVector^\prime + b + 1\right)}}\right)$$</span><br />
</center>
<div class="figure">
<div id="mlp-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/mlp_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/mlp_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="mlp-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;mlp-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mlp-covariance-plot-caption" class="caption-frame">
<p>Figure: The multi-layer perceptron covariance function. This is derived by considering the infinite limit of a neural network with probit activation functions.</p>
</div>
</div>
<h2 id="relu-covariance">RELU Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/relu-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/relu-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="im">from</span> mlai <span class="im">import</span> relu_cov</span></code></pre></div>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = 
\alpha \arcsin\left(\frac{w \inputVector^\top \inputVector^\prime + b}
{\sqrt{\left(w \inputVector^\top \inputVector + b + 1\right)
\left(w \left.\inputVector^\prime\right.^\top \inputVector^\prime + b + 1\right)}}\right)$$</span><br />
</center>
<div class="figure">
<div id="relu-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/relu_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/relu_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="relu-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;relu-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="relu-covariance-plot-caption" class="caption-frame">
<p>Figure: Rectified linear unit covariance function.</p>
</div>
</div>
<h3 id="covariance-functions">Covariance Functions</h3>
<h4 id="where-did-this-covariance-matrix-come-from">Where did this covariance matrix come from?</h4>
<p><strong>Markov Process</strong></p>
<p><strong>Visualization of inverse covariance (precision).</strong></p>
<ul>
<li><p>Precision matrix is sparse: only neighbours in matrix are non-zero.</p></li>
<li><p>This reflects <em>conditional</em> independencies in data.</p></li>
<li><p>In this case <em>Markov</em> structure.</p></li>
</ul>
<h3 id="covariance-functions-1">Covariance Functions</h3>
<h4 data-transition="none" id="where-did-this-covariance-matrix-come-from-1">Where did this covariance matrix come from?</h4>
<p><strong>Exponentiated Quadratic</strong></p>
<p><strong>Visualization of inverse covariance (precision).</strong></p>
<table>
<tr>
<td width="50%">
<ul>
<li>Precision matrix is not sparse.</li>
<li>Each point is dependent on all the others.</li>
<li>In this case non-Markovian.</li>
</ul>
</td>
<td width="50%">
rbfprecisionSample
</td>
</tr>
</table>
<h3 data-transition="none" id="covariance-functions-2">Covariance Functions</h3>
<p><strong>Markov Process</strong></p>
<p><strong>Visualization of inverse covariance (precision).</strong></p>
<table>
<tr>
<td width="50%">
<ul>
<li>Precision matrix is sparse: only neighbours in matrix are non-zero.</li>
<li>This reflects <em>conditional</em> independencies in data.</li>
<li>In this case <em>Markov</em> structure.</li>
</ul>
</td>
<td width="50%">
markovprecisionPlot
</td>
</tr>
</table>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Bochner:book59">
<p>Bochner, Salomon. 1959. <em>Lectures on Fourier Integrals</em>. Princeton University Press. <a href="http://books.google.co.uk/books?id=-vU02QewWK8C">http://books.google.co.uk/books?id=-vU02QewWK8C</a>.</p>
</div>
</div>

