---
title: "Covariance Functions and Hyperparameter Optimizaton"
venue: "Gaussian Process Summer School"
abstract: "<p>In this talk we review multi-output Gaussian processes. Introducing them initially through a Kalman filter representation of a GP.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: 
  institute: 
  twitter: 
  gscholar: 
  orcid: 
time: "null"
week: 0
session: 3
reveal: 03-covariance-functions.slides.html
ipynb: 03-covariance-functions.ipynb
layout: lecture
categories:
- notes
---



<!-- To compile -->
<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h2 id="setup">Setup</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gpss/includes/gpss-notebook-setup.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gpss/includes/gpss-notebook-setup.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>First we download some libraries and files to support the notebook.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> urllib.request</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py&#39;</span>,<span class="st">&#39;mlai.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py&#39;</span>,<span class="st">&#39;teaching_plots.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py&#39;</span>,<span class="st">&#39;gp_tutorial.py&#39;</span>)</span></code></pre></div>
<h2 id="pods">pods</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/pods-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/pods-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>In Sheffield we created a suite of software tools for ‘Open Data Science’. Open data science is an approach to sharing code, models and data that should make it easier for companies, health professionals and scientists to gain access to data science techniques.</p>
<p>You can also check this blog post on <a href="http://inverseprobability.com/2014/07/01/open-data-science">Open Data Science</a>.</p>
<p>The software can be installed using</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="op">%</span>pip install <span class="op">--</span>upgrade git<span class="op">+</span>https:<span class="op">//</span>github.com<span class="op">/</span>sods<span class="op">/</span>ods</span></code></pre></div>
<p>from the command prompt where you can access your python installation.</p>
<p>The code is also available on github: <a href="https://github.com/sods/ods" class="uri">https://github.com/sods/ods</a></p>
<p>Once <code>pods</code> is installed, it can be imported in the usual manner.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">import</span> pods</span></code></pre></div>
<h2 id="gpy-a-gaussian-process-framework-in-python">GPy: A Gaussian Process Framework in Python</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Gaussian processes are a flexible tool for non-parametric analysis with uncertainty. The GPy software was started in Sheffield to provide a easy to use interface to GPs. One which allowed the user to focus on the modelling rather than the mathematics.</p>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gpy-software-magnify" class="magnify" onclick="magnifyFigure(&#39;gpy-software&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-software-caption" class="caption-frame">
<p>Figure: GPy is a BSD licensed software code base for implementing Gaussian process models in Python. It is designed for teaching and modelling. We welcome contributions which can be made through the Github repository <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></p>
</div>
</div>
<p>GPy is a BSD licensed software code base for implementing Gaussian process models in python. This allows GPs to be combined with a wide variety of software libraries.</p>
<p>The software itself is available on <a href="https://github.com/SheffieldML/GPy">GitHub</a> and the team welcomes contributions.</p>
<p>The aim for GPy is to be a probabilistic-style programming language, i.e. you specify the model rather than the algorithm. As well as a large range of covariance functions the software allows for non-Gaussian likelihoods, multivariate outputs, dimensionality reduction and approximations for larger data sets.</p>
<p>The documentation for GPy can be found <a href="https://gpy.readthedocs.io/en/latest/">here</a>.</p>
<h2 id="the-importance-of-the-covariance-function">The Importance of the Covariance Function</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-covariance-function-importance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-covariance-function-importance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The covariance function encapsulates our assumptions about the data. The equations for the distribution of the prediction function, given the training observations, are highly sensitive to the covariation between the test locations and the training locations as expressed by the matrix <span class="math inline">$\kernelMatrix_*$</span>. We defined a matrix <span class="math inline"><strong>A</strong></span> which allowed us to express our conditional mean in the form, <br /><span class="math display">$$
\meanVector_\mappingFunction = \mathbf{A}^\top \dataVector,
$$</span><br /> where <span class="math inline">$\dataVector$</span> were our <em>training observations</em>. In other words our mean predictions are always a linear weighted combination of our <em>training data</em>. The weights are given by computing the covariation between the training and the test data (<span class="math inline">$\kernelMatrix_*$</span>) and scaling it by the inverse covariance of the training data observations, <span class="math inline">$\left[\kernelMatrix + \dataStd^2 \eye\right]^{-1}$</span>. This inverse is the main computational object that needs to be resolved for a Gaussian process. It has a computational burden which is <span class="math inline">$O(\numData^3)$</span> and a storage burden which is <span class="math inline">$O(\numData^2)$</span>. This makes working with Gaussian processes computationally intensive for the situation where <span class="math inline">$\numData&gt;10,000$</span>.</p>
<div class="figure">
<div id="intro-to-gps-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/ewJ3AxKclOg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="intro-to-gps-magnify" class="magnify" onclick="magnifyFigure(&#39;intro-to-gps&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="intro-to-gps-caption" class="caption-frame">
<p>Figure: Introduction to Gaussian processes given by Neil Lawrence at the 2014 Gaussian process Winter School at the University of Sheffield.</p>
</div>
</div>
<h2 id="improving-the-numerics">Improving the Numerics</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-numerics-and-optimization.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-numerics-and-optimization.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>In practice we shouldn’t be using matrix inverse directly to solve the GP system. One more stable way is to compute the <em>Cholesky decomposition</em> of the kernel matrix. The log determinant of the covariance can also be derived from the Cholesky decomposition.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="im">from</span> mlai <span class="im">import</span> update_inverse</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>GP.update_inverse <span class="op">=</span> update_inverse</span></code></pre></div>
<h2 id="capacity-control">Capacity Control</h2>
<p>Gaussian processes are sometimes seen as part of a wider family of methods known as kernel methods. Kernel methods are also based around covariance functions, but in the field they are known as Mercer kernels. Mercer kernels have interpretations as inner products in potentially infinite dimensional Hilbert spaces. This interpretation arises because, if we take <span class="math inline"><em>α</em> = 1</span>, then the kernel can be expressed as <br /><span class="math display">$$
\kernelMatrix = \basisMatrix\basisMatrix^\top 
$$</span><br /> which imples the elements of the kernel are given by, <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \basisVector(\inputVector)^\top \basisVector(\inputVector^\prime).
$$</span><br /> So we see that the kernel function is developed from an inner product between the basis functions. Mercer’s theorem tells us that any valid <em>positive definite function</em> can be expressed as this inner product but with the caveat that the inner product could be <em>infinite length</em>. This idea has been used quite widely to <em>kernelize</em> algorithms that depend on inner products. The kernel functions are equivalent to covariance functions and they are parameterized accordingly. In the kernel modeling community it is generally accepted that kernel parameter estimation is a difficult problem and the normal solution is to cross validate to obtain parameters. This can cause difficulties when a large number of kernel parameters need to be estimated. In Gaussian process modelling kernel parameter estimation (in the simplest case proceeds) by maximum likelihood. This involves taking gradients of the likelihood with respect to the parameters of the covariance function.</p>
<h2 id="gradients-of-the-likelihood">Gradients of the Likelihood</h2>
<p>The easiest conceptual way to obtain the gradients is a two step process. The first step involves taking the gradient of the likelihood with respect to the covariance function, the second step involves considering the gradient of the covariance function with respect to its parameters.</p>
<h2 id="overall-process-scale">Overall Process Scale</h2>
<p>In general we won’t be able to find parameters of the covariance function through fixed point equations, we will need to do gradient based optimization.</p>
<h2 id="capacity-control-and-data-fit">Capacity Control and Data Fit</h2>
<p>The objective function can be decomposed into two terms, a capacity control term, and a data fit term. The capacity control term is the log determinant of the covariance. The data fit term is the matrix inner product between the data and the inverse covariance.</p>
<h2 id="learning-covariance-parameters">Learning Covariance Parameters</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Can we determine covariance parameters from the data?</p>
<p><br /><span class="math display">$$
\gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}{\det{\kernelMatrix}^{\frac{1}{2}}}}{\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}
$$</span><br /></p>
<p><br /><span class="math display">$$
\begin{aligned}
    \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}\color{blue}{\det{\kernelMatrix}^{\frac{1}{2}}}}\color{red}{\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}
\end{aligned}
$$</span><br /></p>
<p><br /><span class="math display">$$
\begin{aligned}
    \log \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=&amp;\color{blue}{-\frac{1}{2}\log\det{\kernelMatrix}}\color{red}{-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}} \\ &amp;-\frac{\numData}{2}\log2\pi
\end{aligned}
$$</span><br /></p>
<p><br /><span class="math display">$$
\errorFunction(\parameterVector) = \color{blue}{\frac{1}{2}\log\det{\kernelMatrix}} + \color{red}{\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}}
$$</span><br /></p>
<h2 id="capacity-control-through-the-determinant">Capacity Control through the Determinant</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize-capacity.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize-capacity.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The parameters are <em>inside</em> the covariance function (matrix).  <br /><span class="math display">$$\kernelScalar_{i, j} = \kernelScalar(\inputVals_i, \inputVals_j; \parameterVector)$$</span><br /></p>
<p><span> <br /><span class="math display">$$\kernelMatrix = \rotationMatrix \eigenvalueMatrix^2 \rotationMatrix^\top$$</span><br /></span></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>gpoptimizePlot1</span></code></pre></div>
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp-optimize-eigen.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<span class="math inline">$\eigenvalueMatrix$</span> represents distance on axes. <span class="math inline">$\rotationMatrix$</span> gives rotation.
</td>
</tr>
</table>
<ul>
<li><span class="math inline">$\eigenvalueMatrix$</span> is <em>diagonal</em>, <span class="math inline">$\rotationMatrix^\top\rotationMatrix = \eye$</span>.</li>
<li>Useful representation since <span class="math inline">$\det{\kernelMatrix} = \det{\eigenvalueMatrix^2} = \det{\eigenvalueMatrix}^2$</span>.</li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>diagrams <span class="op">=</span> <span class="st">&#39;./gp/&#39;</span></span></code></pre></div>
<div class="figure">
<div id="gp-optimise-determinant-figure-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant009.svg" width="80%" style=" ">
</object>
</div>
<div id="gp-optimise-determinant-figure-magnify" class="magnify" onclick="magnifyFigure(&#39;gp-optimise-determinant-figure&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-optimise-determinant-figure-caption" class="caption-frame">
<p>Figure: The determinant of the covariance is dependent only on the eigenvalues. It represents the ‘footprint’ of the Gaussian.</p>
</div>
</div>
<div class="figure">
<div id="gp-optimise-quadratic-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/diagrams/gp-optimise-quadratic002.svg" width="80%" style=" ">
</object>
</div>
<div id="gp-optimise-quadratic-magnify" class="magnify" onclick="magnifyFigure(&#39;gp-optimise-quadratic&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-optimise-quadratic-caption" class="caption-frame">
<p>Figure: The data fit term of the Gaussian process is a quadratic loss centered around zero. This has eliptical contours, the principal axes of which are given by the covariance matrix.</p>
</div>
</div>
<h2 id="quadratic-data-fit">Quadratic Data Fit</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize-data-fit.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize-data-fit.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<h2 id="data-fit-term">Data Fit Term</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize-data-fit-capacity.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize-data-fit-capacity.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="figure">
<div id="gp-optimise-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise006.svg" width="100%" style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise010.svg" width="100%" style=" ">
</object>
</td>
</tr>
</table>
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise016.svg" width="100%" style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise021.svg" width="100%" style=" ">
</object>
</td>
</tr>
</table>
</div>
<div id="gp-optimise-magnify" class="magnify" onclick="magnifyFigure(&#39;gp-optimise&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-optimise-caption" class="caption-frame">
<p>Figure: Variation in the data fit term, the capacity term and the negative log likelihood for different lengthscales.</p>
</div>
</div>
<h2 id="exponentiated-quadratic-covariance">Exponentiated Quadratic Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/eq-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/eq-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The exponentiated quadratic covariance, also known as the Gaussian covariance or the RBF covariance and the squared exponential. Covariance between two points is related to the negative exponential of the squared distnace between those points. This covariance function can be derived in a few different ways: as the infinite limit of a radial basis function neural network, as diffusion in the heat equation, as a Gaussian filter in <em>Fourier space</em> or as the composition as a series of linear filters applied to a base function.</p>
<p>The covariance takes the following form, <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector-\inputVector^\prime}^2}{2\lengthScale^2}\right)
$$</span><br /> where <span class="math inline">ℓ</span> is the <em>length scale</em> or <em>time scale</em> of the process and <span class="math inline"><em>α</em></span> represents the overall process variance.</p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector-\inputVector^\prime}^2}{2\lengthScale^2}\right)$$</span><br />
</center>
<div class="figure">
<div id="eq-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/eq_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/eq_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="eq-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;eq-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="eq-covariance-plot-caption" class="caption-frame">
<p>Figure: The exponentiated quadratic covariance function.</p>
</div>
</div>
<h2 id="where-did-this-covariance-matrix-come-from">Where Did This Covariance Matrix Come From?</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/computing-rbf-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/computing-rbf-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p><br /><span class="math display">$$
k(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\left\Vert \inputVector - \inputVector^\prime\right\Vert^2_2}{2\lengthScale^2}\right)$$</span><br /></p>
<div class="figure">
<div id="computing-eq-three-covariance2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/kern/computing_eq_three_covariance016.svg" width="80%" style=" ">
</object>
</div>
<div id="computing-eq-three-covariance2-magnify" class="magnify" onclick="magnifyFigure(&#39;computing-eq-three-covariance2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="computing-eq-three-covariance2-caption" class="caption-frame">
<p>Figure: Entrywise fill in of the covariance matrix from the covariance function.</p>
</div>
</div>
<div class="figure">
<div id="computing-eq-four-covariance2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/kern/computing_eq_four_covariance027.svg" width="80%" style=" ">
</object>
</div>
<div id="computing-eq-four-covariance2-magnify" class="magnify" onclick="magnifyFigure(&#39;computing-eq-four-covariance2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="computing-eq-four-covariance2-caption" class="caption-frame">
<p>Figure: Entrywise fill in of the covariance matrix from the covariance function.</p>
</div>
</div>
<div class="figure">
<div id="computing-eq-three-2-covariance2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/kern/computing_eq_three_2_covariance016.svg" width="80%" style=" ">
</object>
</div>
<div id="computing-eq-three-2-covariance2-magnify" class="magnify" onclick="magnifyFigure(&#39;computing-eq-three-2-covariance2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="computing-eq-three-2-covariance2-caption" class="caption-frame">
<p>Figure: Entrywise fill in of the covariance matrix from the covariance function.</p>
</div>
</div>
<h2 id="brownian-covariance">Brownian Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/brownian-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/brownian-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a></span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="im">from</span> mlai <span class="im">import</span> brownian_cov</span></code></pre></div>
<p>Brownian motion is also a Gaussian process. It follows a Gaussian random walk, with diffusion occuring at each time point driven by a Gaussian input. This implies it is both Markov and Gaussian. The covariance function for Brownian motion has the form <br /><span class="math display">$$
\kernelScalar(t, t^\prime)=\alpha \min(t, t^\prime)
$$</span><br /></p>
<center>
<br /><span class="math display">$$\kernelScalar(t, t^\prime)=\alpha \min(t, t^\prime)$$</span><br />
</center>
<div class="figure">
<div id="brownian-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/brownian_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/brownian_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="brownian-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;brownian-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="brownian-covariance-plot-caption" class="caption-frame">
<p>Figure: Brownian motion covariance function.</p>
</div>
</div>
<h2 id="where-did-this-covariance-matrix-come-from-1">Where did this covariance matrix come from?</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/precision-matrices.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/precision-matrices.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p><strong>Markov Process</strong></p>
<p><strong>Visualization of inverse covariance (precision).</strong></p>
<ul>
<li><p>Precision matrix is sparse: only neighbours in matrix are non-zero.</p></li>
<li><p>This reflects <em>conditional</em> independencies in data.</p></li>
<li><p>In this case <em>Markov</em> structure.</p></li>
</ul>
<h2 id="where-did-this-covariance-matrix-come-from-2">Where did this covariance matrix come from?</h2>
<p><strong>Exponentiated Quadratic</strong></p>
<p><strong>Visualization of inverse covariance (precision).</strong></p>
<table>
<tr>
<td width="50%">
<ul>
<li>Precision matrix is not sparse.</li>
<li>Each point is dependent on all the others.</li>
<li>In this case non-Markovian.</li>
</ul>
</td>
<td width="50%">
rbfprecisionSample
</td>
</tr>
</table>
<h2 id="covariance-functions">Covariance Functions</h2>
<p><strong>Markov Process</strong></p>
<p><strong>Visualization of inverse covariance (precision).</strong></p>
<table>
<tr>
<td width="50%">
<ul>
<li>Precision matrix is sparse: only neighbours in matrix are non-zero.</li>
<li>This reflects <em>conditional</em> independencies in data.</li>
<li>In this case <em>Markov</em> structure.</li>
</ul>
</td>
<td width="50%">
markovprecisionPlot
</td>
</tr>
</table>
<h2 id="exponential-covariance">Exponential Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/ou-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/ou-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The expontential covariance, in one dimension this is also known as the Ornstein Uhlenbeck covariance, and in multiple dimensions it’s also the Mater 1/2 covaraince. It has an interpretation as a stochastic differential equation with a linear drift term (equivalent to a quadratic potential). The drift keeps the covariance stationary (unlike the Brownian motion covariance). It also has an interpretation as a Cauchy filter in Fourier space <span class="citation" data-cites="Stein:interpolation99">(Stein 1999)</span> (from Bochner’s theorem).</p>
<p>The covariance takes the following form, <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector-\inputVector^\prime}}{\lengthScale}\right)
$$</span><br /> where <span class="math inline">ℓ</span> is the <em>length scale</em> or <em>time scale</em> of the process and <span class="math inline"><em>α</em></span> represents the overall process variance.</p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector-\inputVector^\prime}}{\lengthScale}\right)$$</span><br />
</center>
<div class="figure">
<div id="ou-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/ou_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/ou_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="ou-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;ou-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="ou-covariance-plot-caption" class="caption-frame">
<p>Figure: The exponential covariance function.</p>
</div>
</div>
<h2 id="basis-function-covariance">Basis Function Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/basis-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/basis-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The fixed basis function covariance just comes from the properties of a multivariate Gaussian, if we decide <br /><span class="math display">$$
\mappingFunctionVector=\basisMatrix\mappingVector
$$</span><br /> and then we assume <br /><span class="math display">$$
\mappingVector \sim \gaussianSamp{\zerosVector}{\alpha\eye}
$$</span><br /> then it follows from the properties of a multivariate Gaussian that <br /><span class="math display">$$
\mappingFunctionVector \sim \gaussianSamp{\zerosVector}{\alpha\basisMatrix\basisMatrix^\top}
$$</span><br /> meaning that the vector of observations from the function is jointly distributed as a Gaussian process and the covariance matrix is <span class="math inline">$\kernelMatrix = \alpha\basisMatrix \basisMatrix^\top$</span>, each element of the covariance matrix can then be found as the inner product between two rows of the basis funciton matrix.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="im">from</span> mlai <span class="im">import</span> basis_cov</span></code></pre></div>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="im">from</span> mlai <span class="im">import</span> radial</span></code></pre></div>
<center>
<br /><span class="math display">$$\kernel(\inputVector, \inputVector^\prime) = \basisVector(\inputVector)^\top \basisVector(\inputVector^\prime)$$</span><br />
</center>
<div class="figure">
<div id="basis-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/basis_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/basis_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="basis-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;basis-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="basis-covariance-plot-caption" class="caption-frame">
<p>Figure: A covariance function based on a non-linear basis given by <span class="math inline">$\basisVector(\inputVector)$</span>.</p>
</div>
</div>
<h2 id="degenerate-covariance-functions">Degenerate Covariance Functions</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/rbf-basis-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/rbf-basis-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Any linear basis function can also be incorporated into a covariance function. For example, an RBF network is a type of neural network with a set of radial basis functions. Meaning, the basis funciton is radially symmetric. These basis functions take the form, <br /><span class="math display">$$
\basisFunction_k(\inputScalar) = \exp\left(-\frac{\ltwoNorm{\inputScalar-\meanScalar_k}^{2}}{\lengthScale^{2}}\right).
$$</span><br /> Given a set of parameters, <br /><span class="math display">$$
\meanVector = \begin{bmatrix} -1 \\ 0 \\ 1\end{bmatrix},
$$</span><br /> we can construct the corresponding covariance function, which has the form, <br /><span class="math display">$$
\kernelScalar\left(\inputVals,\inputVals^{\prime}\right)=\alpha\basisVector(\inputVals)^\top \basisVector(\inputVals^\prime).
$$</span><br /></p>
<h2 id="bochners-theoerem">Bochners Theoerem</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/boechners-theorem.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/boechners-theorem.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p><br /><span class="math display"><em>Q</em>(<em>t</em>) = ∫<sub>ℝ</sub><em>e</em><sup> − <em>i</em><em>t</em><em>x</em></sup>d<em>μ</em>(<em>x</em>).</span><br /></p>
<p>Imagine we are given data and we wish to generalize from it. Without making further assumptions, we have no more information than the given data set. We can think of this ssomewhat like a weighted sum of Dirac delta functions. The Dirac delta function is defined to be a function with an integral of one, which is zero at all places apart from zero, where it is infinite. Given observations at particular times (or locations) <span class="math inline">$\inputVector_i$</span> we can think of our observations as being a function, <br /><span class="math display">$$
\mappingFunction(\inputVector) = \sum_{i=1}^\numData y_i \delta(\inputVector-\inputVector_i),
$$</span><br /> This function is highly discontinuous, imagine if we wished to smooth it by filtering in Fourier space. The Fourier transform of a function is given by, <br /><span class="math display">$$
F(\boldsymbol{\omega}) = \int_{-\infty}^\infty \mappingFunction(\inputVector) \exp\left(-i2\pi \boldsymbol{\omega}^\top \inputVector\right) \text{d} \inputVector
$$</span><br /> and since our function is a series of delta functions the the transform is easy to compute, <br /><span class="math display">$$
F(\boldsymbol{\omega}) = \sum_{i=1}^\numData y_i\exp\left(-i 2\pi \boldsymbol{\omega}^\top \inputVector_i\right)
$$</span><br /> which has a real part given by a weighted sum of cosines and a complex part given by a weighted sum of sines.</p>
<p>One theorem that gives insight into covariances is Bochner’s theorem. Bochner’s theorem states that any positive filter in Fourier space gives rise to a valid covariance function. Further, it gives a relationship between the filter and the form of the covariance function. The form of the covariance is given by the <a href="http://en.wikipedia.org/wiki/Fourier_transform">Fourier transform</a> of the filter, with the argument of the transform being replaced by the distance between the points.</p>
<p>Fourier space is a transformed space of the original function to a new basis. The transformation occurs through a convolution with a sine and cosine basis. Given a function of time <span class="math inline">$\mappingFunction(t)$</span> the Fourier transform moves it to a weighted linear sum of a sine and cosine basis, <br /><span class="math display">$$
F(\omega) = \int_{-\infty}^\infty \mappingFunction(t) \left[\cos(2\pi \omega t) - i \sin(2\pi \omega t) \right]\text{d} t
$$</span><br /> where is the imaginary basis, <span class="math inline">$i=\sqrt{-1}$</span>. Through Euler’s formula, <br /><span class="math display">exp (<em>i</em><em>x</em>) = cos <em>x</em> + <em>i</em>sin <em>x</em></span><br /> we can re-express this form as <br /><span class="math display">$$
F(\omega) = \int_{-\infty}^\infty \mappingFunction(t) \exp(-i 2\pi\omega)\text{d} t
$$</span><br /> which is a standard form for the Fourier transform. Fourier’s theorem was that the <em>inverse</em> transform can also be expressed in a similar form so we have <br /><span class="math display">$$
\mappingFunction(t) = \int_{-\infty}^\infty F(\omega) \exp(2\pi\omega)\text{d} \omega.
$$</span><br /> Although we’ve introduced the transform in the context of time Fourier’s interest was an analytical theory of heat and the transform can be applied to a multidimensional spatial function, <span class="math inline">$\mappingFunction(\inputVector)$</span>.</p>
<iframe frameborder="0" scrolling="no" style="border:0px" src="http://books.google.co.uk/books?id=TDQJAAAAIAAJ&amp;pg=PA525&amp;output=embed" width="700" height="500">
</iframe>
<h2 id="sinc-covariance">Sinc Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/sinc-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/sinc-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Another approach to developing covariance function exploits Bochner’s theorem <span class="citation" data-cites="Bochner:book59">Bochner (1959)</span>. Bochner’s theorem tells us that any positve filter in Fourier space implies has an associated Gaussian process with a stationary covariance function. The covariance function is the <em>inverse Fourier transform</em> of the filter applied in Fourier space.</p>
<p>For example, in signal processing, <em>band limitations</em> are commonly applied as an assumption. For example, we may believe that no frequency above <span class="math inline"><em>w</em> = 2</span> exists in the signal. This is equivalent to a rectangle function being applied as a the filter in Fourier space.</p>
<p>The inverse Fourier transform of the rectangle function is the <span class="math inline">sinc( ⋅ )</span> function. So the sinc is a valid covariance function, and it represents <em>band limited</em> signals.</p>
<p>Note that other covariance functions we’ve introduced can also be interpreted in this way. For example, the exponentiated quadratic covariance function can be Fourier transformed to see what the implied filter in Fourier space is. The Fourier transform of the exponentiated quadratic is an exponentiated quadratic, so the standard EQ-covariance implies a EQ filter in Fourier space.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a></span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="im">from</span> mlai <span class="im">import</span> sinc_cov</span></code></pre></div>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \text{sinc}\left(\pi w r\right)$$</span><br />
</center>
<div class="figure">
<div id="sinc-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/sinc_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/sinc_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="sinc-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;sinc-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sinc-covariance-plot-caption" class="caption-frame">
<p>Figure: Sinc covariance function.</p>
</div>
</div>
<h2 id="matérn-32-covariance">Matérn 3/2 Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/matern32-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/matern32-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The Matérn 3/2 <span class="citation" data-cites="Stein:interpolation99">(Stein 1999)</span> covariance is which is once differentiable, it arises from applying a Student-<span class="math inline"><em>t</em></span> based filter in Fourier space with three degrees of freedom.</p>
<p>The covariance takes the following form, <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \left(1+\frac{\sqrt{3}\ltwoNorm{\inputVector-\inputVector^\prime}}{\lengthScale}\right)\exp\left(-\frac{\sqrt{3}\ltwoNorm{\inputVector-\inputVector^\prime}}{\lengthScale}\right)
$$</span><br /> where <span class="math inline">ℓ</span> is the <em>length scale</em> or <em>time scale</em> of the process and <span class="math inline"><em>α</em></span> represents the overall process variance.</p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \left(1+\frac{\sqrt{3}\ltwoNorm{\inputVector-\inputVector^\prime}}{\lengthScale}\right)\exp\left(-\frac{\sqrt{3}\ltwoNorm{\inputVector-\inputVector^\prime}}{\lengthScale}\right)$$</span><br />
</center>
<div class="figure">
<div id="matern32-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/matern32_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/matern32_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="matern32-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;matern32-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="matern32-covariance-plot-caption" class="caption-frame">
<p>Figure: The Matérn 3/2 covariance function.</p>
</div>
</div>
<h2 id="matérn-52-covariance">Matérn 5/2 Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/matern52-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/matern52-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The Matérn 5/2 <span class="citation" data-cites="Stein:interpolation99">(Stein 1999)</span> covariance is which is once differentiable, it arises from applying a Student-<span class="math inline"><em>t</em></span> based filter in Fourier space with five degrees of freedom.</p>
<p>The covariance takes the following form, <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \left(1+\frac{\sqrt{5}\ltwoNorm{\inputVector-\inputVector^\prime}}{\lengthScale} + \frac{5\ltwoNorm{\inputVector-\inputVector^\prime}^2}{3\lengthScale^2}\right)\exp\left(-\frac{\sqrt{5}\ltwoNorm{\inputVector-\inputVector^\prime}}{\lengthScale}\right)
$$</span><br /> where <span class="math inline">ℓ</span> is the <em>length scale</em> or <em>time scale</em> of the process and <span class="math inline"><em>α</em></span> represents the overall process variance.</p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \left(1+\frac{\sqrt{5}\ltwoNorm{\inputVector-\inputVector^\prime}}{\lengthScale} + \frac{5\ltwoNorm{\inputVector-\inputVector^\prime}^2}{3\lengthScale^2}\right)\exp\left(-\frac{\sqrt{5}\ltwoNorm{\inputVector-\inputVector^\prime}}{\lengthScale}\right)$$</span><br />
</center>
<div class="figure">
<div id="matern52-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/matern52_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/matern52_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="matern52-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;matern52-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="matern52-covariance-plot-caption" class="caption-frame">
<p>Figure: The Matérn 5/2 covariance function.</p>
</div>
</div>
<h2 id="rational-quadratic-covariance">Rational Quadratic Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/ratquad-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/ratquad-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The rational quadratic covariance function is derived by a continuous mixture of exponentiated quadratic covariance funcitons, where the lengthscale is given by an inverse gamma distribution. The resulting covariance is infinitely smooth (in terms of differentiability) but has a family of length scales present. As <span class="math inline"><em>a</em></span> gets larger, the exponentiated quadratic covariance funciton is recovered.</p>
<p>The covariance takes the following form, <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \left(1+\frac{\ltwoNorm{\inputVector-\inputVector^\prime}^2}{2 a \lengthScale^2}\right)^{-a}
$$</span><br /> where <span class="math inline">ℓ</span> is the <em>length scale</em> or <em>time scale</em> of the process and <span class="math inline"><em>α</em></span> represents the overall process variance and <span class="math inline"><em>a</em></span> represents shape parameter of the inverse Gamma used to create the scale mixture.</p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \left(1+\frac{\ltwoNorm{\inputVector-\inputVector^\prime}^2}{2 a \lengthScale^2}\right)^{-a}$$</span><br />
</center>
<div class="figure">
<div id="ratquad-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/ratquad_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/ratquad_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="ratquad-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;ratquad-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="ratquad-covariance-plot-caption" class="caption-frame">
<p>Figure: The rational quadratic covariance function.</p>
</div>
</div>
<h2 id="polynomial-covariance">Polynomial Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/poly-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/poly-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha(w \inputVector^\top\inputVector^\prime + b)^d$$</span><br />
</center>
<div class="figure">
<div id="polynomial-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/polynomial_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/polynomial_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="polynomial-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;polynomial-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="polynomial-covariance-plot-caption" class="caption-frame">
<p>Figure: Polynomial covariance function.</p>
</div>
</div>
<h2 id="periodic-covariance">Periodic Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/periodic-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/periodic-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha\exp\left(\frac{-2\sin(\pi rw)^2}{\lengthScale^2}\right)$$</span><br />
</center>
<div class="figure">
<div id="periodic-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/periodic_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/periodic_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="periodic-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;periodic-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="periodic-covariance-plot-caption" class="caption-frame">
<p>Figure: Periodic covariance function.</p>
</div>
</div>
<h2 id="mlp-covariance">MLP Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/mlp-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/mlp-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a></span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="im">from</span> mlai <span class="im">import</span> mlp_cov</span></code></pre></div>
<p>The multi-layer perceptron (MLP) covariance, also known as the neural network covariance or the arcsin covariance, is derived by considering the infinite limit of a neural network.</p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \arcsin\left(\frac{w \inputVector^\top \inputVector^\prime + b}{\sqrt{\left(w \inputVector^\top \inputVector + b + 1\right)\left(w \left.\inputVector^\prime\right.^\top \inputVector^\prime + b + 1\right)}}\right)$$</span><br />
</center>
<div class="figure">
<div id="mlp-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/mlp_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/mlp_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="mlp-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;mlp-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mlp-covariance-plot-caption" class="caption-frame">
<p>Figure: The multi-layer perceptron covariance function. This is derived by considering the infinite limit of a neural network with probit activation functions.</p>
</div>
</div>
<h2 id="relu-covariance">RELU Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/relu-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/relu-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a></span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="im">from</span> mlai <span class="im">import</span> relu_cov</span></code></pre></div>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = 
\alpha \arcsin\left(\frac{w \inputVector^\top \inputVector^\prime + b}
{\sqrt{\left(w \inputVector^\top \inputVector + b + 1\right)
\left(w \left.\inputVector^\prime\right.^\top \inputVector^\prime + b + 1\right)}}\right)$$</span><br />
</center>
<div class="figure">
<div id="relu-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/relu_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/relu_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="relu-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;relu-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="relu-covariance-plot-caption" class="caption-frame">
<p>Figure: Rectified linear unit covariance function.</p>
</div>
</div>
<h2 id="additive-covariance">Additive Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/add-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/add-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>An additive covariance function is derived from considering the result of summing two Gaussian processes together. If the first Gaussian process is <span class="math inline"><em>g</em>( ⋅ )</span>, governed by covariance <span class="math inline">$\kernelScalar_g(\cdot, \cdot)$</span> and the second process is <span class="math inline"><em>h</em>( ⋅ )</span>, governed by covariance <span class="math inline">$\kernelScalar_h(\cdot, \cdot)$</span> then the combined process <span class="math inline"><em>f</em>( ⋅ ) = <em>g</em>( ⋅ ) + <em>h</em>( ⋅ )</span> is govererned by a covariance function, <br /><span class="math display">$$
\kernelScalar_f(\inputVector, \inputVector^\prime) = \kernelScalar_g(\inputVector, \inputVector^\prime) + \kernelScalar_h(\inputVector, \inputVector^\prime)
$$</span><br /></p>
<center>
<br /><span class="math display">$$\kernelScalar_f(\inputVector, \inputVector^\prime) = \kernelScalar_g(\inputVector, \inputVector^\prime) + \kernelScalar_h(\inputVector, \inputVector^\prime)$$</span><br />
</center>
<div class="figure">
<div id="add-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/add_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/add_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="add-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;add-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="add-covariance-plot-caption" class="caption-frame">
<p>Figure: An additive covariance function formed by combining a linear and an exponentiated quadratic covariance functions.</p>
</div>
</div>
<h2 id="product-covariance">Product Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/prod-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/prod-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>An product covariance function is derived from considering the result of multiplying two stochastic processes together. If the first stochastic process is <span class="math inline"><em>g</em>( ⋅ )</span>, governed by covariance <span class="math inline">$\kernelScalar_g(\cdot, \cdot)$</span> and the second process is <span class="math inline"><em>h</em>( ⋅ )</span>, governed by covariance <span class="math inline">$\kernelScalar_h(\cdot, \cdot)$</span> then the combined process <span class="math inline"><em>f</em>( ⋅ ) = <em>g</em>( ⋅ )<em>h</em>( ⋅ )</span> is governed by a covariance function, <br /><span class="math display">$$
\kernelScalar_f(\inputVector, \inputVector^\prime) = \kernelScalar_g(\inputVector, \inputVector^\prime) \kernelScalar_h(\inputVector, \inputVector^\prime)
$$</span><br /> Note that if <span class="math inline"><em>g</em>( ⋅ )</span> and <span class="math inline"><em>h</em>( ⋅ )</span> are Gaussian processes then <span class="math inline"><em>f</em>( ⋅ )</span> will not in general be a Gaussian process. So the base processes are (presumably) some (unspecified) non-Gaussian processes.</p>
<center>
<br /><span class="math display">$$\kernelScalar_f(\inputVector, \inputVector^\prime) = \kernelScalar_g(\inputVector, \inputVector^\prime) \kernelScalar_h(\inputVector, \inputVector^\prime)$$</span><br />
</center>
<div class="figure">
<div id="prod-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/prod_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/prod_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="prod-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;prod-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="prod-covariance-plot-caption" class="caption-frame">
<p>Figure: An product covariance function formed by combining a linear and an exponentiated quadratic covariance functions.</p>
</div>
</div>
<h2 id="mauna-loa-data">Mauna Loa Data</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/mauna-loa-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/mauna-loa-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The Mauna Loa data consists of monthly mean carbon dioxide measured at Mauna Loa Observatory, Hawaii. According to the website, <a href="https://www.esrl.noaa.gov/gmd/ccgg/trends/" class="uri">https://www.esrl.noaa.gov/gmd/ccgg/trends/</a>.</p>
<blockquote>
<p>The carbon dioxide data on Mauna Loa constitute the longest record of direct measurements of CO2 in the atmosphere. They were started by C. David Keeling of the Scripps Institution of Oceanography in March of 1958 at a facility of the National Oceanic and Atmospheric Administration <span class="citation" data-cites="Keeling-atmospheric76">(Keeling et al. 1976)</span>. NOAA started its own CO2 measurements in May of 1974, and they have run in parallel with those made by Scripps since then <span class="citation" data-cites="Thoning-atmospheric89">(Thoning, Tans, and Komhyr 1989)</span>.</p>
</blockquote>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a></span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>data <span class="op">=</span> pods.datasets.mauna_loa()</span>
<span id="cb19-2"><a href="#cb19-2"></a>x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb19-3"><a href="#cb19-3"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb19-4"><a href="#cb19-4"></a></span>
<span id="cb19-5"><a href="#cb19-5"></a>offset <span class="op">=</span> y.mean()</span>
<span id="cb19-6"><a href="#cb19-6"></a>scale <span class="op">=</span> np.sqrt(y.var())</span></code></pre></div>
<div class="figure">
<div id="mauna-loa-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/mauna-loa.svg" width="80%" style=" ">
</object>
</div>
<div id="mauna-loa-magnify" class="magnify" onclick="magnifyFigure(&#39;mauna-loa&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mauna-loa-caption" class="caption-frame">
<p>Figure: Mauna Loa data shows carbon dioxide monthly average measurements from the Mauna Loa Observatory in Hawaii.</p>
</div>
</div>
<h2 id="gaussian-process-fit">Gaussian Process Fit</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/mauna-loa-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/mauna-loa-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The data set was used as a demonstration of model selection for Gaussian processes in <span class="citation" data-cites="Rasmussen:book06">Rasmussen and Williams (2006)</span> (Chapter 5).</p>
<p>Here we reconstruct that analysis in GPy. Our first objective will be to perform a Gaussian process fit to the data, we’ll do this using the <a href="https://github.com/SheffieldML/GPy">GPy software</a>.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="im">import</span> GPy</span></code></pre></div>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>kernel1 <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, lengthscale<span class="op">=</span><span class="dv">40</span>, variance<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb21-2"><a href="#cb21-2"></a>kernel2 <span class="op">=</span> GPy.kern.PeriodicMatern52(<span class="dv">1</span>, variance<span class="op">=</span><span class="dv">4</span>, period<span class="op">=</span><span class="dv">1</span>, lengthscale<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb21-3"><a href="#cb21-3"></a>kernel3 <span class="op">=</span> GPy.kern.RatQuad(<span class="dv">1</span>, lengthscale<span class="op">=</span><span class="dv">5</span>, variance<span class="op">=</span><span class="dv">10</span>, power<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-4"><a href="#cb21-4"></a>kernel4 <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, lengthscale<span class="op">=</span><span class="fl">0.2</span>, variance<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-5"><a href="#cb21-5"></a>kernel5 <span class="op">=</span> GPy.kern.Bias(<span class="dv">1</span>, variance<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb21-6"><a href="#cb21-6"></a>kernel <span class="op">=</span> kernel1 <span class="op">+</span> kernel2 <span class="op">+</span> kernel3 <span class="op">+</span> kernel4 <span class="op">+</span> kernel5</span>
<span id="cb21-7"><a href="#cb21-7"></a>model <span class="op">=</span> GPy.models.GPRegression(x,yhat, kernel<span class="op">=</span>kernel)</span>
<span id="cb21-8"><a href="#cb21-8"></a>model.optimize(messages<span class="op">=</span><span class="va">True</span>) <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<p>The first command sets up the model, then <code>m_full.optimize()</code> optimizes the parameters of the covariance function and the noise level of the model. Once the fit is complete, we’ll try creating some test points, and computing the output of the GP model in terms of the mean and standard deviation of the posterior functions between 1870 and 2030. We plot the mean function and the standard deviation at 200 locations. We can obtain the predictions using <code>y_mean, y_var = m_full.predict(xt)</code></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>xt <span class="op">=</span> np.linspace(<span class="dv">1950</span>,<span class="dv">2020</span>,<span class="dv">300</span>)[:,np.newaxis]</span>
<span id="cb22-2"><a href="#cb22-2"></a>yt_mean, yt_var <span class="op">=</span> m_full.predict(xt)</span>
<span id="cb22-3"><a href="#cb22-3"></a>yt_sd<span class="op">=</span>np.sqrt(yt_var)</span></code></pre></div>
<p>Now we plot the results using the helper function in <code>teaching_plots</code>.</p>
<div class="figure">
<div id="mauna-loa-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/mauna-loa-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="mauna-loa-gp-magnify" class="magnify" onclick="magnifyFigure(&#39;mauna-loa-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mauna-loa-gp-caption" class="caption-frame">
<p>Figure: Gaussian process fit to the Mauna Loa Observatory data on CO2 concentrations.</p>
</div>
</div>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Bochner:book59">
<p>Bochner, Salomon. 1959. <em>Lectures on Fourier Integrals</em>. Princeton University Press. <a href="http://books.google.co.uk/books?id=-vU02QewWK8C">http://books.google.co.uk/books?id=-vU02QewWK8C</a>.</p>
</div>
<div id="ref-Keeling-atmospheric76">
<p>Keeling, Charles D., Robert B. Bacastow, Arnold E. Bainbridge, Carl A. Ekdahl Jr., Peter R. Guenther, Lee S. Waterman, and John F. S. Chin. 1976. “Atmospheric Carbon Dioxide Variations at Mauna Loa Observatory, Hawaii.” <em>Tellus</em> 28 (6): 538–51. <a href="https://doi.org/https://doi.org/10.1111/j.2153-3490.1976.tb00701.x">https://doi.org/https://doi.org/10.1111/j.2153-3490.1976.tb00701.x</a>.</p>
</div>
<div id="ref-Rasmussen:book06">
<p>Rasmussen, Carl Edward, and Christopher K. I. Williams. 2006. <em>Gaussian Processes for Machine Learning</em>. Cambridge, MA: mit.</p>
</div>
<div id="ref-Stein:interpolation99">
<p>Stein, Michael L. 1999. <em>Interpolation of Spatial Data: Some Theory for Kriging</em>. springer.</p>
</div>
<div id="ref-Thoning-atmospheric89">
<p>Thoning, Kirk W., Pieter P. Tans, and Walter D. Komhyr. 1989. “Atmospheric Carbon Dioxide at Mauna Loa Observatory: 2. Analysis of the NOAA GMCC Data, 1974–1985.” <em>Journal of Geophysical Research: Atmospheres</em> 94 (D6): 8549–65. <a href="https://doi.org/https://doi.org/10.1029/JD094iD06p08549">https://doi.org/https://doi.org/10.1029/JD094iD06p08549</a>.</p>
</div>
</div>

