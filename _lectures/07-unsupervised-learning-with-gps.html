---
title: "Unsupervised Learning wtih Gaussian Processes"
venue: "Gaussian Process Summer School"
abstract: "null"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: University of Cambridge
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orcid: 0000-0001-9258-1030
time: "null"
week: 0
session: 7
reveal: 07-unsupervised-learning-with-gps.slides.html
ipynb: 07-unsupervised-learning-with-gps.ipynb
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h1 id="probabilistic-pca">Probabilistic PCA</h1>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/probabilistic-pca.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/probabilistic-pca.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>In 1997 <a href="http://research.microsoft.com/pubs/67218/bishop-ppca-jrss.pdf">Tipping and Bishop</a> <span class="citation" data-cites="Tipping:pca97">(Tipping and Bishop 1999)</span> and <a href="https://www.cs.nyu.edu/~roweis/papers/empca.pdf">Roweis</a> <span class="citation" data-cites="Roweis:SPCA97">(Roweis, n.d.)</span> independently revisited Hotelling’s model and considered the case where the noise variance was finite, but <em>shared</em> across all output dimensons. Their model can be thought of as a factor analysis where <br /><span class="math display">$$
\boldsymbol{\Sigma} = \noiseStd^2 \eye.
$$</span><br /> This leads to a marginal likelihood of the form <br /><span class="math display">$$
p(\dataMatrix|\mappingMatrix, \noiseStd^2)
= \prod_{i=1}^\numData\gaussianDist{\dataVector_{i, :}}{\zerosVector}{\mappingMatrix\mappingMatrix^\top + \noiseStd^2 \eye}
$$</span><br /> where the limit of <span class="math inline">$\noiseStd^2\rightarrow 0$</span> is <em>not</em> taken. This defines a proper probabilistic model. Tippping and Bishop then went on to prove that the <em>maximum likelihood</em> solution of this model with respect to <span class="math inline">$\mappingMatrix$</span> is given by an eigenvalue problem. In the probabilistic PCA case the eigenvalues and eigenvectors are given as follows. <br /><span class="math display">$$
\mappingMatrix = \mathbf{U}\mathbf{L} \mathbf{R}^\top
$$</span><br /> where <span class="math inline"><strong>U</strong></span> is the eigenvectors of the empirical covariance matrix <br /><span class="math display">$$
\mathbf{S} = \sum_{i=1}^\numData (\dataVector_{i, :} - \meanVector)(\dataVector_{i,
:} - \meanVector)^\top,
$$</span><br /> which can be written <span class="math inline">$\mathbf{S} = \frac{1}{\numData} \dataMatrix^\top\dataMatrix$</span> if the data is zero mean. The matrix <span class="math inline"><strong>L</strong></span> is diagonal and is dependent on the <em>eigenvalues</em> of <span class="math inline"><strong>S</strong></span>, <span class="math inline"><strong>Λ</strong></span>. If the <span class="math inline"><em>i</em></span>th diagonal element of this matrix is given by <span class="math inline"><em>λ</em><sub><em>i</em></sub></span> then the corresponding element of <span class="math inline"><strong>L</strong></span> is <br /><span class="math display">$$
\ell_i = \sqrt{\lambda_i - \noiseStd^2}
$$</span><br /> where <span class="math inline">$\noiseStd^2$</span> is the noise variance. Note that if <span class="math inline">$\noiseStd^2$</span> is larger than any particular eigenvalue, then that eigenvalue (along with its corresponding eigenvector) is <em>discarded</em> from the solution.</p>
<h2 id="python-implementation-of-probabilistic-pca">Python Implementation of Probabilistic PCA</h2>
<p>We will now implement this algorithm in python.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># probabilistic PCA algorithm</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="kw">def</span> ppca(Y, q):</span>
<span id="cb2-3"><a href="#cb2-3"></a>    <span class="co"># remove mean</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>    Y_cent <span class="op">=</span> Y <span class="op">-</span> Y.mean(<span class="dv">0</span>)</span>
<span id="cb2-5"><a href="#cb2-5"></a></span>
<span id="cb2-6"><a href="#cb2-6"></a>    <span class="co"># Comute covariance</span></span>
<span id="cb2-7"><a href="#cb2-7"></a>    S <span class="op">=</span> np.dot(Y_cent.T, Y_cent)<span class="op">/</span>Y.shape[<span class="dv">0</span>]</span>
<span id="cb2-8"><a href="#cb2-8"></a>    lambd, U <span class="op">=</span> np.linalg.eig(S)</span>
<span id="cb2-9"><a href="#cb2-9"></a></span>
<span id="cb2-10"><a href="#cb2-10"></a>    <span class="co"># Choose number of eigenvectors</span></span>
<span id="cb2-11"><a href="#cb2-11"></a>    sigma2 <span class="op">=</span> np.<span class="bu">sum</span>(lambd[q:])<span class="op">/</span>(Y.shape[<span class="dv">1</span>]<span class="op">-</span>q)</span>
<span id="cb2-12"><a href="#cb2-12"></a>    l <span class="op">=</span> np.sqrt(lambd[:q]<span class="op">-</span>sigma2)</span>
<span id="cb2-13"><a href="#cb2-13"></a>    W <span class="op">=</span> U[:, :q]<span class="op">*</span>l[<span class="va">None</span>, :]</span>
<span id="cb2-14"><a href="#cb2-14"></a>    <span class="cf">return</span> W, sigma2</span></code></pre></div>
<p>In practice we may not wish to compute the eigenvectors of the covariance matrix directly. This is because it requires us to estimate the covariance, which involves a sum of squares term, before estimating the eigenvectors. We can estimate the eigenvectors directly either through <a href="http://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a> or <a href="http://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a>. We saw a similar issue arise when , where we also wished to avoid computation of <span class="math inline">$\latentMatrix^\top\latentMatrix$</span> (or in the case of  <span class="math inline"><strong>Φ</strong><sup>⊤</sup><strong>Φ</strong></span>).</p>
<h1 id="posterior-for-principal-component-analysis">Posterior for Principal Component Analysis</h1>
<p>Under the latent variable model justification for principal component analysis, we are normally interested in inferring something about the latent variables given the data. This is the distribution, <br /><span class="math display">$$
p(\latentVector_{i, :} | \dataVector_{i, :})
$$</span><br /> for any given data point. Determining this density turns out to be very similar to the approach for determining the Bayesian posterior of <span class="math inline">$\weightVector$</span> in Bayesian linear regression, only this time we place the prior density over <span class="math inline">$\latentVector_{i, :}$</span> instead of <span class="math inline">$\weightVector$</span>. The posterior is proportional to the joint density as follows, <br /><span class="math display">$$
p(\latentVector_{i, :} | \dataVector_{i, :}) \propto p(\dataVector_{i,
:}|\mappingMatrix, \latentVector_{i, :}, \noiseStd^2) p(\latentVector_{i, :})
$$</span><br /> And as in the Bayesian linear regression case we first consider the log posterior, <br /><span class="math display">$$
\log p(\latentVector_{i, :} | \dataVector_{i, :}) = \log p(\dataVector_{i, :}|\mappingMatrix,
\latentVector_{i, :}, \noiseStd^2) + \log p(\latentVector_{i, :}) + \text{const}
$$</span><br /> where the constant is not dependent on <span class="math inline">$\latentVector$</span>. As before we collect the quadratic terms in <span class="math inline">$\latentVector_{i, :}$</span> and we assemble them into a Gaussian density over <span class="math inline">$\latentVector$</span>. <br /><span class="math display">$$
\log p(\latentVector_{i, :} | \dataVector_{i, :}) =
-\frac{1}{2\noiseStd^2} (\dataVector_{i, :} - \mappingMatrix\latentVector_{i,
:})^\top(\dataVector_{i, :} - \mappingMatrix\latentVector_{i, :}) - \frac{1}{2}
\latentVector_{i, :}^\top \latentVector_{i, :} + \text{const}
$$</span><br /></p>
<h3 id="exercise-0">Exercise 0</h3>
<p>Multiply out the terms in the brackets. Then collect the quadratic term and the linear terms together. Show that the posterior has the form <br /><span class="math display">$$
\latentVector_{i, :} | \mappingMatrix \sim \gaussianSamp{\meanVector_x}{\covarianceMatrix_x}
$$</span><br /> where <br /><span class="math display">$$
\covarianceMatrix_x = \left(\noiseStd^{-2}
\mappingMatrix^\top\mappingMatrix + \eye\right)^{-1}
$$</span><br /> and <br /><span class="math display">$$
\meanVector_x
= \covarianceMatrix_x \noiseStd^{-2}\mappingMatrix^\top \dataVector_{i, :} 
$$</span><br /> Compare this to the posterior for the Bayesian linear regression from last week, do they have similar forms? What matches and what differs?</p>
<h2 id="python-implementation-of-the-posterior">Python Implementation of the Posterior</h2>
<p>Now let’s implement the system in code.</p>
<h3 id="exercise-1">Exercise 1</h3>
<p>Use the values for <span class="math inline">$\mappingMatrix$</span> and <span class="math inline">$\noiseStd^2$</span> you have computed, along with the data set <span class="math inline">$\dataMatrix$</span> to compute the posterior density over <span class="math inline">$\latentMatrix$</span>. Write a function of the form</p>
<p>python mu_x, C_x = posterior(Y, W, sigma2)} where <code>mu_x</code> and <code>C_x</code> are the posterior mean and posterior covariance for the given <span class="math inline">$\dataMatrix$</span>.</p>
<p>Don’t forget to subtract the mean of the data <code>Y</code> inside your function before computing the posterior: remember we assumed at the beginning of our analysis that the data had been centred (i.e. the mean was removed).}{20}</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># Question 4 Answer Code</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="co"># Write code for you answer to this question in this box</span></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="co"># Do not delete these comments, otherwise you will get zero for this answer.</span></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="co"># Make sure your code has run and the answer is correct *before* submitting your notebook for marking.</span></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="kw">def</span> posterior(Y, W, sigma2):</span>
<span id="cb3-8"><a href="#cb3-8"></a>    Y_cent <span class="op">=</span> Y <span class="op">-</span> Y.mean(<span class="dv">0</span>)</span>
<span id="cb3-9"><a href="#cb3-9"></a>    <span class="co"># Compute posterior over X</span></span>
<span id="cb3-10"><a href="#cb3-10"></a>    C_x <span class="op">=</span> </span>
<span id="cb3-11"><a href="#cb3-11"></a>    mu_x <span class="op">=</span> </span>
<span id="cb3-12"><a href="#cb3-12"></a>    <span class="cf">return</span> mu_x, C_x</span></code></pre></div>
<h1 id="numerically-stable-and-efficient-version">Numerically Stable and Efficient Version</h1>
<p>Just as we saw for  and  computation of a matrix such as <span class="math inline">$\dataMatrix^\top\dataMatrix$</span> (or its centred version) can be a bad idea in terms of loss of numerical accuracy. Fortunately, we can find the eigenvalues and eigenvectors of the matrix <span class="math inline">$\dataMatrix^\top\dataMatrix$</span> without direct computation of the matrix. This can be done with the <a href="http://en.wikipedia.org/wiki/Singular_value_decomposition"><em>singular value decomposition</em></a>. The singular value decompsition takes a matrix, <span class="math inline"><strong>Z</strong></span> and represents it in the form, <br /><span class="math display"><strong>Z</strong> = <strong>U</strong><strong>Λ</strong><strong>V</strong><sup>⊤</sup></span><br /> where <span class="math inline"><strong>U</strong></span> is a matrix of orthogonal vectors in the columns, meaning <span class="math inline">$\mathbf{U}^\top\mathbf{U} = \eye$</span>. It has the same number of rows and columns as <span class="math inline"><strong>Z</strong></span>. The matrices <span class="math inline"><strong>Λ</strong></span> and <span class="math inline"><strong>V</strong></span> are both square with dimensionality given by the number of columns of <span class="math inline"><strong>Z</strong></span>. The matrix <span class="math inline"><strong>Λ</strong></span> is <em>diagonal</em> and <span class="math inline"><strong>V</strong></span> is an orthogonal matrix so <span class="math inline">$\mathbf{V}^\top\mathbf{V} = \mathbf{V}\mathbf{V}^\top = \eye$</span>. The eigenvalues of the matrix <span class="math inline">$\dataMatrix^\top\dataMatrix$</span> are then given by the singular values of the matrix <span class="math inline">$\dataMatrix^\top$</span> squared and the eigenvectors are given by <span class="math inline"><strong>U</strong></span>.</p>
<h2 id="solution-for-mappingmatrix">Solution for <span class="math inline">$\mappingMatrix$</span></h2>
<p>Given the singular value decomposition of <span class="math inline">$\dataMatrix$</span> then we have <br /><span class="math display">$$
\mappingMatrix =
\mathbf{U}\mathbf{L}\mathbf{R}^\top
$$</span><br /> where <span class="math inline"><strong>R</strong></span> is an arbitrary rotation matrix. This implies that the posterior is given by <br /><span class="math display">$$
\covarianceMatrix_x =
\left[\noiseStd^{-2}\mathbf{R}\mathbf{L}^2\mathbf{R}^\top + \eye\right]^{-1}
$$</span><br /> because <span class="math inline">$\mathbf{U}^\top \mathbf{U} = \eye$</span>. Since, by convention, we normally take <span class="math inline">$\mathbf{R} = \eye$</span> to ensure that the principal components are orthonormal we can write <br /><span class="math display">$$
\covarianceMatrix_x = \left[\noiseStd^{-2}\mathbf{L}^2 +
\eye\right]^{-1}
$$</span><br /> which implies that <span class="math inline">$\covarianceMatrix_x$</span> is actually diagonal with elements given by <br /><span class="math display">$$
c_i = \frac{\noiseStd^2}{\noiseStd^2 + \ell^2_i}
$$</span><br /> and allows us to write <br /><span class="math display">$$
\meanVector_x = [\mathbf{L}^2 + \noiseStd^2
\eye]^{-1} \mathbf{L} \mathbf{U}^\top \dataVector_{i, :}
$$</span><br /> <br /><span class="math display">$$
\meanVector_x = \mathbf{D}\mathbf{U}^\top \dataVector_{i, :}
$$</span><br /> where <span class="math inline"><strong>D</strong></span> is a diagonal matrix with diagonal elements given by <span class="math inline">$d_{i} = \frac{\ell_i}{\noiseStd^2 + \ell_i^2}$</span>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># probabilistic PCA algorithm using SVD</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="kw">def</span> ppca(Y, q, center<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb5-3"><a href="#cb5-3"></a>    <span class="co">&quot;&quot;&quot;Probabilistic PCA through singular value decomposition&quot;&quot;&quot;</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>    <span class="co"># remove mean</span></span>
<span id="cb5-5"><a href="#cb5-5"></a>    <span class="cf">if</span> center:</span>
<span id="cb5-6"><a href="#cb5-6"></a>        Y_cent <span class="op">=</span> Y <span class="op">-</span> Y.mean(<span class="dv">0</span>)</span>
<span id="cb5-7"><a href="#cb5-7"></a>    <span class="cf">else</span>:</span>
<span id="cb5-8"><a href="#cb5-8"></a>        Y_cent <span class="op">=</span> Y</span>
<span id="cb5-9"><a href="#cb5-9"></a>        </span>
<span id="cb5-10"><a href="#cb5-10"></a>    <span class="co"># Comute singluar values, discard &#39;R&#39; as we will assume orthogonal</span></span>
<span id="cb5-11"><a href="#cb5-11"></a>    U, sqlambd, _ <span class="op">=</span> sp.linalg.svd(Y_cent.T,full_matrices<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-12"><a href="#cb5-12"></a>    lambd <span class="op">=</span> (sqlambd<span class="op">**</span><span class="dv">2</span>)<span class="op">/</span>Y.shape[<span class="dv">0</span>]</span>
<span id="cb5-13"><a href="#cb5-13"></a>    <span class="co"># Compute residual and extract eigenvectors</span></span>
<span id="cb5-14"><a href="#cb5-14"></a>    sigma2 <span class="op">=</span> np.<span class="bu">sum</span>(lambd[q:])<span class="op">/</span>(Y.shape[<span class="dv">1</span>]<span class="op">-</span>q)</span>
<span id="cb5-15"><a href="#cb5-15"></a>    ell <span class="op">=</span> np.sqrt(lambd[:q]<span class="op">-</span>sigma2)</span>
<span id="cb5-16"><a href="#cb5-16"></a>    <span class="cf">return</span> U[:, :q], ell, sigma2</span>
<span id="cb5-17"><a href="#cb5-17"></a></span>
<span id="cb5-18"><a href="#cb5-18"></a><span class="kw">def</span> posterior(Y, U, ell, sigma2, center<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb5-19"><a href="#cb5-19"></a>    <span class="co">&quot;&quot;&quot;Posterior computation for the latent variables given the eigendecomposition.&quot;&quot;&quot;</span></span>
<span id="cb5-20"><a href="#cb5-20"></a>    <span class="cf">if</span> center:</span>
<span id="cb5-21"><a href="#cb5-21"></a>        Y_cent <span class="op">=</span> Y <span class="op">-</span> Y.mean(<span class="dv">0</span>)</span>
<span id="cb5-22"><a href="#cb5-22"></a>    <span class="cf">else</span>:</span>
<span id="cb5-23"><a href="#cb5-23"></a>        Y_cent <span class="op">=</span> Y</span>
<span id="cb5-24"><a href="#cb5-24"></a>    C_x <span class="op">=</span> np.diag(sigma2<span class="op">/</span>(sigma2<span class="op">+</span>ell<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb5-25"><a href="#cb5-25"></a>    d <span class="op">=</span> ell<span class="op">/</span>(sigma2<span class="op">+</span>ell<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb5-26"><a href="#cb5-26"></a>    mu_x <span class="op">=</span> np.dot(Y_cent, U)<span class="op">*</span>d[<span class="va">None</span>, :]</span>
<span id="cb5-27"><a href="#cb5-27"></a>    <span class="cf">return</span> mu_x, C_x</span></code></pre></div>
<h2 id="difficulty-for-probabilistic-approaches">Difficulty for Probabilistic Approaches</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/non-linear-difficulty.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/non-linear-difficulty.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The challenge for composition of probabilistic models is that you need to propagate a probability densities through non linear mappings. This allows you to create broader classes of probability density. Unfortunately it renders the resulting densities <em>intractable</em>.</p>
<div class="figure">
<div id="nonlinear-mapping-3d-plot-figure" class="figure-frame">
<object class="svgplot 80%" data="../slides/diagrams/dimred/nonlinear-mapping-3d-plot.svg" width="center" style=" ">
</object>
</div>
<div id="nonlinear-mapping-3d-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;nonlinear-mapping-3d-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="nonlinear-mapping-3d-plot-caption" class="caption-frame">
<p>Figure: A two dimensional grid mapped into three dimensions to form a two dimensional manifold.</p>
</div>
</div>
<div class="figure">
<div id="non-linear-mapping-2d-plot-figure" class="figure-frame">
<object class="svgplot 80%" data="../slides/diagrams/dimred/nonlinear-mapping-2d-plot.svg" width="center" style=" ">
</object>
</div>
<div id="non-linear-mapping-2d-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;non-linear-mapping-2d-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="non-linear-mapping-2d-plot-caption" class="caption-frame">
<p>Figure: A one dimensional line mapped into two dimensions by two separate independent functions. Each point can be mapped exactly through the mappings.</p>
</div>
</div>
<div class="figure">
<div id="gaussian-through-nonlinear-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/gaussian-through-nonlinear.svg" width="80%" style=" ">
</object>
</div>
<div id="gaussian-through-nonlinear-magnify" class="magnify" onclick="magnifyFigure(&#39;gaussian-through-nonlinear&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gaussian-through-nonlinear-caption" class="caption-frame">
<p>Figure: A Gaussian density over the input of a non linear function leads to a very non Gaussian output. Here the output is multimodal.</p>
</div>
</div>
<p>This lab session will focus on three aspects of GPs: sampling, the design of Experiments and uncertainty propagation.</p>
<h2 id="getting-started-and-downloading-data">Getting Started and Downloading Data</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/gplvm-tutorial.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/gplvm-tutorial.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="im">import</span> GPy</span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="im">import</span> string</span></code></pre></div>
<p>The following code is for plotting and to prepare the bigger models for later useage. If you are interested, you can have a look, but this is not essential.</p>
<p>For this lab, we’ve created a dataset <code>digits.npy</code> containing all handwritten digits from <span class="math inline">0⋯9</span> handwritten, provided by@deCampos-character09. We will only use some of the digits for the demonstrations in this lab class, but you can edit the code below to select different subsets of the digit data as you wish.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a></span>
<span id="cb7-2"><a href="#cb7-2"></a>```[which <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">9</span>] <span class="co"># which digits to work on</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>data <span class="op">=</span> pods.datasets.decampos_digits(which_digits<span class="op">=</span>which)</span>
<span id="cb7-4"><a href="#cb7-4"></a>Y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb7-5"><a href="#cb7-5"></a>labels <span class="op">=</span> data[<span class="st">&#39;str_lbls&#39;</span>]}</span>
<span id="cb7-6"><a href="#cb7-6"></a></span>
<span id="cb7-7"><a href="#cb7-7"></a>You can <span class="cf">try</span> to plot some of the digits using `plt.matshow` (the digit images have size `16x16`).</span>
<span id="cb7-8"><a href="#cb7-8"></a></span>
<span id="cb7-9"><a href="#cb7-9"></a><span class="co">## Principal Component Analysis</span></span>
<span id="cb7-10"><a href="#cb7-10"></a></span>
<span id="cb7-11"><a href="#cb7-11"></a></span>
<span id="cb7-12"><a href="#cb7-12"></a></span>
<span id="cb7-13"><a href="#cb7-13"></a></span>
<span id="cb7-14"><a href="#cb7-14"></a></span>
<span id="cb7-15"><a href="#cb7-15"></a>Principal component analysis (PCA) finds a rotation of the observed outputs, such that the rotated principal component (PC) space maximizes the variance of the data observed, <span class="bu">sorted</span> <span class="im">from</span> most to least important (most to least variable <span class="kw">in</span> the corresponding PC).</span>
<span id="cb7-16"><a href="#cb7-16"></a></span>
<span id="cb7-17"><a href="#cb7-17"></a>In order to <span class="bu">apply</span> PCA <span class="kw">in</span> an easy way, we have included a PCA module <span class="kw">in</span> pca.py. You can <span class="im">import</span> the module by </span>
<span id="cb7-18"><a href="#cb7-18"></a><span class="im">import</span> <span class="op">&lt;</span>path.to.pca<span class="op">&gt;</span> (without the ending .py<span class="op">!</span>). </span>
<span id="cb7-19"><a href="#cb7-19"></a>To run PCA on the digits we have to reshape (Hint: np.reshape ) digits . </span>
<span id="cb7-20"><a href="#cb7-20"></a></span>
<span id="cb7-21"><a href="#cb7-21"></a><span class="op">*</span> What <span class="kw">is</span> the right shape $n <span class="op">\</span>times <span class="op">\</span>dataDim$ to use?</span>
<span id="cb7-22"><a href="#cb7-22"></a></span>
<span id="cb7-23"><a href="#cb7-23"></a>We will call the reshaped observed outputs $<span class="op">\</span>mathbf{Y}$ <span class="kw">in</span> the following.</span>
<span id="cb7-24"><a href="#cb7-24"></a></span>
<span id="cb7-25"><a href="#cb7-25"></a></span>
<span id="cb7-26"><a href="#cb7-26"></a>```{.python}</span>
<span id="cb7-27"><a href="#cb7-27"></a>Yn <span class="op">=</span> Y<span class="co">#Y-Y.mean()</span></span></code></pre></div>
<p>Now let’s run PCA on the reshaped dataset <span class="math inline"><strong>Y</strong></span>:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="im">from</span> GPy.util <span class="im">import</span> pca</span>
<span id="cb8-2"><a href="#cb8-2"></a>p <span class="op">=</span> pca.pca(Y) <span class="co"># create PCA class with digits dataset</span></span></code></pre></div>
<p>The resulting plot will show the lower dimensional representation of the digits in 2 dimensions.</p>
<h2 id="gaussian-process-latent-variable-model">Gaussian Process Latent Variable Model</h2>
<p>The Gaussian Process Latent Variable Model (GP-LVM) <span class="citation" data-cites="Lawrence:pnpca05">(Lawrence 2005)</span> embeds PCA into a Gaussian process framework, where the latent inputs <span class="math inline"><strong>X</strong></span> are learnt as hyperparameters and the mapping variables <span class="math inline"><strong>W</strong></span> are integrated out. The advantage of this interpretation is it allows PCA to be generalized in a non linear way by replacing the resulting <em>linear</em> covariance witha non linear covariance. But first, let’s see how GPLVM is equivalent to PCA using an automatic relevance determination (ARD, see e.g. <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span>) linear kernel:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>input_dim <span class="op">=</span> <span class="dv">4</span> <span class="co"># How many latent dimensions to use</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>kernel <span class="op">=</span> GPy.kern.Linear(input_dim, ARD<span class="op">=</span><span class="va">True</span>) <span class="co"># ARD kernel</span></span>
<span id="cb9-3"><a href="#cb9-3"></a>m <span class="op">=</span> GPy.models.GPLVM(Yn, input_dim<span class="op">=</span>input_dim, kernel<span class="op">=</span>kernel)</span>
<span id="cb9-4"><a href="#cb9-4"></a></span>
<span id="cb9-5"><a href="#cb9-5"></a>m.optimize(messages<span class="op">=</span><span class="dv">1</span>, max_iters<span class="op">=</span><span class="dv">1000</span>) <span class="co"># optimize for 1000 iterations</span></span></code></pre></div>
<p>As you can see the solution with a linear kernel is the same as the PCA solution with the exception of rotational changes and axis flips.</p>
<p>For the sake of time, the solution you see was only running for 1000 iterations, thus it might not be converged fully yet. The GP-LVM proceeds by iterative optimization of the <em>inputs</em> to the covariance. As we saw in the lecture earlier, for the linear covariance, these latent points can be optimized with an eigenvalue problem, but generally, for non-linear covariance functions, we are obliged to use gradient based optimization.</p>
<h2 id="bayesian-gplvm">Bayesian GPLVM</h2>
<p>In GP-LVM we use a point estimate of the distribution of the input <span class="math inline"><strong>X</strong></span>. This estimate is derived through maximum likelihood or through a maximum a posteriori (MAP) approach. Ideally, we would like to also estimate a distribution over the input <span class="math inline"><strong>X</strong></span>. In the Bayesian GPLVM we approximate the true distribution <span class="math inline"><em>p</em>(<strong>X</strong>|<strong>Y</strong>)</span> by a variational approximation <span class="math inline"><em>q</em>(<strong>X</strong>)</span> and integrate <span class="math inline"><strong>X</strong></span> out <span class="citation" data-cites="Titsias:bayesGPLVM10">(Titsias and Lawrence 2010)</span>.</p>
<p>Approximating the posterior in this way allows us to optimize a lower bound on the marginal likelihood. Handling the uncertainty in a principled way allows the model to make an assessment of whether a particular latent dimension is required, or the variation is better explained by noise. This allows the algorithm to switch off latent dimensions. The switching off can take some time though, so below in Section 6 we provide a pre-learnt module, but to complete section 6 you’ll need to be working in the IPython console instead of the notebook.</p>
<p>For the moment we’ll run a short experiment applying the Bayesian GP-LVM with an exponentiated quadratic covariance function.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="co"># Model optimization</span></span>
<span id="cb10-3"><a href="#cb10-3"></a>input_dim <span class="op">=</span> <span class="dv">5</span> <span class="co"># How many latent dimensions to use</span></span>
<span id="cb10-4"><a href="#cb10-4"></a>kern <span class="op">=</span> GPy.kern.RBF(input_dim,ARD<span class="op">=</span><span class="va">True</span>) <span class="co"># ARD kernel</span></span>
<span id="cb10-5"><a href="#cb10-5"></a>m <span class="op">=</span> GPy.models.BayesianGPLVM(Yn, input_dim<span class="op">=</span>input_dim, kernel<span class="op">=</span>kern, num_inducing<span class="op">=</span><span class="dv">25</span>)</span>
<span id="cb10-6"><a href="#cb10-6"></a></span>
<span id="cb10-7"><a href="#cb10-7"></a><span class="co"># initialize noise as 1% of variance in data</span></span>
<span id="cb10-8"><a href="#cb10-8"></a><span class="co">#m.likelihood.variance = m.Y.var()/100.</span></span>
<span id="cb10-9"><a href="#cb10-9"></a>m.optimize(messages<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
<p>Because we are now also considering the uncertainty in the model, this optimization can take some time. However, you are free to interrupt the optimization at any point selecting <code>Kernel-&gt;Interupt</code> from the notepad menu. This will leave you with the model, <code>m</code> in the current state and you can plot and look into the model parameters.</p>
<h2 id="preoptimized-model">Preoptimized Model</h2>
<p>A good way of working with latent variable models is to interact with the latent dimensions, generating data. This is a little bit tricky in the notebook, so below in section 6 we provide code for setting up an interactive demo in the standard IPython shell. If you are working on your own machine you can try this now. Otherwise continue with section 5.</p>
<h2 id="multiview-learning-manifold-relevance-determination">Multiview Learning: Manifold Relevance Determination</h2>
<p>In Manifold Relevance Determination we try to find one latent space, common for <span class="math inline"><em>K</em></span> observed output sets (modalities) <span class="math inline">{<strong>Y</strong><sub><em>k</em></sub>}<sub><em>k</em> = 1</sub><sup><em>K</em></sup></span>. Each modality is associated with a separate set of ARD parameters so that it switches off different parts of the whole latent space and, therefore, <span class="math inline"><strong>X</strong></span> is softly segmented into parts that are private to some, or shared for all modalities. Can you explain what happens in the following example?</p>
<p>Again, you can stop the optimizer at any point and explore the result obtained with the so far training:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>m <span class="op">=</span> GPy.examples.dimensionality_reduction.mrd_simulation(optimize <span class="op">=</span> <span class="va">False</span>, plot<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-2"><a href="#cb11-2"></a>m.optimize(messages <span class="op">=</span> <span class="va">True</span>, max_iters<span class="op">=</span><span class="fl">3e3</span>, optimizer <span class="op">=</span> <span class="st">&#39;bfgs&#39;</span>)</span></code></pre></div>
<h2 id="interactive-demo-for-use-outside-the-notepad">Interactive Demo: For Use Outside the Notepad</h2>
<p>The module below loads a pre-optimized Bayesian GPLVM model (like the one you just trained) and allows you to interact with the latent space. Three interactive figures pop up: the latent space, the ARD scales and a sample in the output space (corresponding to the current selected latent point of the other figure). You can sample with the mouse from the latent space and obtain samples in the output space. You can select different latent dimensions to vary by clicking on the corresponding scales with the left and right mouse buttons. This will also cause the latent space to be projected on the selected latent dimensions in the other figure.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="im">import</span> urllib2, os, sys</span></code></pre></div>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>model_path <span class="op">=</span>  <span class="st">&#39;digit_bgplvm_demo.pickle&#39;</span> <span class="co"># local name for model file</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>status <span class="op">=</span> <span class="st">&quot;&quot;</span></span>
<span id="cb13-3"><a href="#cb13-3"></a></span>
<span id="cb13-4"><a href="#cb13-4"></a>re <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="cf">if</span> <span class="bu">len</span>(sys.argv) <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb13-6"><a href="#cb13-6"></a>    re <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb13-7"><a href="#cb13-7"></a></span>
<span id="cb13-8"><a href="#cb13-8"></a><span class="cf">if</span> re <span class="kw">or</span> <span class="kw">not</span> os.path.exists(model_path): <span class="co"># only download the model new, if it was not already</span></span>
<span id="cb13-9"><a href="#cb13-9"></a>    url <span class="op">=</span> <span class="st">&#39;http://staffwww.dcs.sheffield.ac.uk/people/M.Zwiessele/gpss/lab3/digit_bgplvm_demo.pickle&#39;</span></span>
<span id="cb13-10"><a href="#cb13-10"></a>    <span class="cf">with</span> <span class="bu">open</span>(model_path, <span class="st">&#39;wb&#39;</span>) <span class="im">as</span> f:</span>
<span id="cb13-11"><a href="#cb13-11"></a>        u <span class="op">=</span> urllib2.urlopen(url)</span>
<span id="cb13-12"><a href="#cb13-12"></a>        meta <span class="op">=</span> u.info()</span>
<span id="cb13-13"><a href="#cb13-13"></a>        file_size <span class="op">=</span> <span class="bu">int</span>(meta.getheaders(<span class="st">&quot;Content-Length&quot;</span>)[<span class="dv">0</span>])</span>
<span id="cb13-14"><a href="#cb13-14"></a>        <span class="bu">print</span> <span class="st">&quot;Downloading: </span><span class="sc">%s</span><span class="st">&quot;</span> <span class="op">%</span> (model_path)</span>
<span id="cb13-15"><a href="#cb13-15"></a></span>
<span id="cb13-16"><a href="#cb13-16"></a>        file_size_dl <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-17"><a href="#cb13-17"></a>        block_sz <span class="op">=</span> <span class="dv">8192</span></span>
<span id="cb13-18"><a href="#cb13-18"></a>        <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb13-19"><a href="#cb13-19"></a>            buff <span class="op">=</span> u.read(block_sz)</span>
<span id="cb13-20"><a href="#cb13-20"></a>            <span class="cf">if</span> <span class="kw">not</span> buff:</span>
<span id="cb13-21"><a href="#cb13-21"></a>                <span class="cf">break</span></span>
<span id="cb13-22"><a href="#cb13-22"></a>            file_size_dl <span class="op">+=</span> <span class="bu">len</span>(buff)</span>
<span id="cb13-23"><a href="#cb13-23"></a>            f.write(buff)</span>
<span id="cb13-24"><a href="#cb13-24"></a>            sys.stdout.write(<span class="st">&quot; &quot;</span><span class="op">*</span>(<span class="bu">len</span>(status)) <span class="op">+</span> <span class="st">&quot;</span><span class="ch">\r</span><span class="st">&quot;</span>)</span>
<span id="cb13-25"><a href="#cb13-25"></a>            status <span class="op">=</span> <span class="vs">r&quot;</span><span class="sc">{:7.3f}</span><span class="vs">/</span><span class="sc">{:.3f}</span><span class="vs">MB [</span><span class="sc">{: &gt;7.2%}</span><span class="vs">]&quot;</span>.<span class="bu">format</span>(file_size_dl<span class="op">/</span>(<span class="fl">1.</span><span class="op">*</span><span class="fl">1e6</span>), file_size<span class="op">/</span>(<span class="fl">1.</span><span class="op">*</span><span class="fl">1e6</span>), <span class="bu">float</span>(file_size_dl)<span class="op">/</span>file_size)</span>
<span id="cb13-26"><a href="#cb13-26"></a>            sys.stdout.write(status)</span>
<span id="cb13-27"><a href="#cb13-27"></a>            sys.stdout.flush()</span>
<span id="cb13-28"><a href="#cb13-28"></a>        sys.stdout.write(<span class="st">&quot; &quot;</span><span class="op">*</span>(<span class="bu">len</span>(status)) <span class="op">+</span> <span class="st">&quot;</span><span class="ch">\r</span><span class="st">&quot;</span>)</span>
<span id="cb13-29"><a href="#cb13-29"></a>        <span class="bu">print</span> status</span>
<span id="cb13-30"><a href="#cb13-30"></a><span class="cf">else</span>:</span>
<span id="cb13-31"><a href="#cb13-31"></a>    <span class="bu">print</span>(<span class="st">&quot;Already cached, to reload run with &#39;reload&#39; as the only argument&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="im">import</span> cPickle <span class="im">as</span> pickle</span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">&#39;./digit_bgplvm_demo.pickle&#39;</span>, <span class="st">&#39;rb&#39;</span>) <span class="im">as</span> f:</span>
<span id="cb14-3"><a href="#cb14-3"></a>    m <span class="op">=</span> pickle.load(f)</span></code></pre></div>
<p>Prepare for plotting of this model. If you run on a webserver the interactive plotting will not work. Thus, you can skip to the next codeblock and run it on your own machine, later.</p>
<h3 id="observations">Observations</h3>
<p>Confirm the following observations by interacting with the demo:</p>
<ul>
<li>We tend to obtain more “strange” outputs when sampling from latent space areas away from the training inputs.</li>
<li>When sampling from the two dominant latent dimensions (the ones corresponding to large scales) we differentiate between all digits. Also note that projecting the latent space into the two dominant dimensions better separates the classes.</li>
<li>When sampling from less dominant latent dimensions the outputs vary in a more subtle way.</li>
</ul>
<p>You can also run the dimensionality reduction example</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>GPy.examples.dimensionality_reduction.bgplvm_simulation()</span></code></pre></div>
<h3 id="questions">Questions</h3>
<ul>
<li>Can you see a difference in the ARD parameters to the non Bayesian GPLVM?</li>
<li>How does the Bayesian GPLVM allow the ARD parameters of the RBF kernel magnify the two first dimensions?</li>
<li>Is Bayesian GPLVM better in differentiating between different kinds of digits?</li>
<li>Why does the starting noise variance have to be lower then the variance of the observed values?</li>
<li>How come we use the lowest variance when using a linear kernel, but the highest lengtscale when using an RBF kernel?</li>
</ul>
<h2 id="example-latent-doodle-space">Example: Latent Doodle Space</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/latent-doodle-space.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/latent-doodle-space.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="figure">
<div id="latent-doodle-space-figure" class="figure-frame">
<iframe width height src="https://player.vimeo.com/video/3235882#t=" frameborder="0" allow="autoplay; fullscreen" allowfullscreen>
</iframe>
</div>
<div id="latent-doodle-space-magnify" class="magnify" onclick="magnifyFigure(&#39;latent-doodle-space&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="latent-doodle-space-caption" class="caption-frame">
<p>Figure: The latent doodle space idea of <span class="citation" data-cites="Baxter:doodle06">Baxter and Anjyo (2006)</span> manages to build a smooth mapping across very sparse data.</p>
</div>
</div>
<p><strong>Generalization with much less Data than Dimensions</strong></p>
<ul>
<li><p>Powerful uncertainly handling of GPs leads to surprising properties.</p></li>
<li><p>Non-linear models can be used where there are fewer data points than dimensions <em>without overfitting</em>.</p></li>
</ul>
<p><span style="text-align:right"><span class="citation" data-cites="Baxter:doodle06">(Baxter and Anjyo 2006)</span></span></p>
<h2 id="continuous-character-control">Continuous Character Control</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/character-control.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/character-control.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p><span style="text-align:right"><span class="citation" data-cites="Levine:control12">Levine et al. (2012)</span></span> - Graph diffusion prior for enforcing connectivity between motions. <br /><span class="math display">$$\log p(\inputMatrix) = w_c \sum_{i,j} \log K_{ij}^d$$</span><br /> with the graph diffusion kernel <span class="math inline">$\kernelMatrix^d$</span> obtain from <br /><span class="math display"><em>K</em><sub><em>i</em><em>j</em></sub><sup><em>d</em></sup> = exp (<em>β</em><strong>H</strong>)   with   <strong>H</strong> =  − <strong>T</strong><sup> − 1/2</sup><strong>L</strong><strong>T</strong><sup> − 1/2</sup></span><br /> the graph Laplacian, and <span class="math inline"><strong>T</strong></span> is a diagonal matrix with <span class="math inline">$T_{ii} = \sum_j w(\inputVector_i, \inputVector_j)$</span>, <br /><span class="math display">$$L_{ij} = \begin{cases} \sum_k w(\inputVector_i,\inputVector_k) &amp; \text{if $i=j$}
    \\
    -w(\inputVector_i,\inputVector_j) &amp;\text{otherwise.}
    \end{cases}$$</span><br /> and <span class="math inline">$w(\inputVector_i,\inputVector_j) = || \inputVector_i - \inputVector_j||^{-p}$</span> measures similarity.</p>
<h2 id="character-control-results">Character Control: Results</h2>
<div class="figure">
<div id="charcter-control-gplvm-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/hr3pdDl5IAg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="charcter-control-gplvm-magnify" class="magnify" onclick="magnifyFigure(&#39;charcter-control-gplvm&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="charcter-control-gplvm-caption" class="caption-frame">
<p>Figure: Character control in the latent space described the the GP-LVM <span class="citation" data-cites="Levine:control12">Levine et al. (2012)</span></p>
</div>
</div>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Baxter:doodle06">
<p>Baxter, William V., and Ken-Ichi Anjyo. 2006. “Latent Doodle Space.” In <em>EUROGRAPHICS</em>, 25:477–85. 3. Vienna, Austria. <a href="https://doi.org/10.1111/j.1467-8659.2006.00967.x">https://doi.org/10.1111/j.1467-8659.2006.00967.x</a>.</p>
</div>
<div id="ref-Bishop:book06">
<p>Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. springer.</p>
</div>
<div id="ref-Lawrence:pnpca05">
<p>Lawrence, Neil D. 2005. “Probabilistic Non-Linear Principal Component Analysis with Gaussian Process Latent Variable Models.” <em>Journal of Machine Learning Research</em> 6 (November): 1783–1816.</p>
</div>
<div id="ref-Levine:control12">
<p>Levine, Segey, Jack M. Wang, Alexis Haraux, Zoran Popović, and Vladlen Koltun. 2012. “Continuous Character Control with Low-Dimensional Embeddings.” <em>ACM Transactions on Graphics (SIGGRAPH 2012)</em> 31 (4).</p>
</div>
<div id="ref-Roweis:SPCA97">
<p>Roweis, Sam T. n.d. “EM Algorithms for PCA and SPCA.” In, 626–32.</p>
</div>
<div id="ref-Tipping:pca97">
<p>Tipping, Michael E., and Christopher M. Bishop. 1999. “Mixtures of Probabilistic Principal Component Analysers.” <em>Neural Computation</em> 11 (2): 443–82.</p>
</div>
<div id="ref-Titsias:bayesGPLVM10">
<p>Titsias, Michalis K., and Neil D. Lawrence. 2010. “Bayesian Gaussian Process Latent Variable Model.” In, 9:844–51.</p>
</div>
</div>

