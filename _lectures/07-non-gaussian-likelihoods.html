---
title: "Non Gaussian Likelihoods"
venue: "Gaussian Process Summer School"
abstract: "null"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: University of Cambridge
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orcid: 0000-0001-9258-1030
time: "null"
week: 0
session: 7
reveal: 07-non-gaussian-likelihoods.slides.html
ipynb: 07-non-gaussian-likelihoods.ipynb
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h1 id="non-gaussian-likelihoods">Non Gaussian Likelihoods</h1>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/non-gaussian-gps.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/non-gaussian-gps.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>[Gaussian processes model functions. If our observation is a corrupted version of this function and the corruption process is <em>also</em> Gaussian, it is trivial to account for this. However, there are many circumstances where our observation may be non Gaussian. In these cases we need to turn to approximate inference techniques. As a simple illustration, we’ll use a dataset of binary observations of the language that is spoken in different regions of East-Timor. First we will load the data and a couple of libraries to visualize it.}</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">from</span> matplotlib.patches <span class="im">import</span> Polygon</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">from</span> matplotlib.collections <span class="im">import</span> PatchCollection</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> cPickle <span class="im">as</span> pickle</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> urllib</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>urllib.urlretrieve(<span class="st">&#39;http://staffwww.dcs.sheffield.ac.uk/people/M.Zwiessele/gpss/lab2/EastTimor.pickle&#39;</span>, <span class="st">&#39;EastTimor2.pickle&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># Load the data</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;./EastTimor2.pickle&quot;</span>,<span class="st">&quot;rb&quot;</span>) <span class="im">as</span> f:</span>
<span id="cb3-3"><a href="#cb3-3"></a>    X,y,polygons <span class="op">=</span> pickle.load(f)</span></code></pre></div>
<p>Now we will create a map of East Timor and, using GPy, plot the data on top of it. A classification model can be defined in a similar way to the regression model, but now using <code>GPy.models.GPClassification</code>. However, once we’ve define the model, we also need to update the approximation to the likelihood. This runs the Expectation propagation updates.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">import</span> GPy</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="co">#Define the model</span></span>
<span id="cb5-2"><a href="#cb5-2"></a>kern <span class="op">=</span> GPy.kern.RBF(<span class="dv">2</span>)</span>
<span id="cb5-3"><a href="#cb5-3"></a>m <span class="op">=</span> GPy.models.GPClassification(X,y, kernel<span class="op">=</span>kern)</span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="bu">print</span>(m)</span></code></pre></div>
<p>The decision boundary should be quite poor! However we haven’t optimized the model. Try the following:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>m.optimize()</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="bu">print</span>(m)</span></code></pre></div>
<p>The optimization is based on the likelihood approximation that was made after we constructed the model. However, because we’ve now changed the model parameters the quality of that approximation has now probably deteriorated. To improve the model we should iterate between updating the Expectation propagation approximation and optimizing the model parameters.</p>
<h2 id="robust-regression-a-running-example">Robust Regression: A Running Example</h2>
<h2 id="olympic-marathon-data">Olympic Marathon Data</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-marathon-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-marathon-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardised distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
<p>The first thing we will do is load a standard data set for regression modelling. The data consists of the pace of Olympic Gold Medal Marathon winners for the Olympics from 1896 to present. First we load in the data and plot.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="op">%</span>pip install <span class="op">--</span>upgrade git<span class="op">+</span>https:<span class="op">//</span>github.com<span class="op">/</span>sods<span class="op">/</span>ods</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>data <span class="op">=</span> pods.datasets.olympic_marathon_men()</span>
<span id="cb9-2"><a href="#cb9-2"></a>x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb9-3"><a href="#cb9-3"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb9-4"><a href="#cb9-4"></a></span>
<span id="cb9-5"><a href="#cb9-5"></a>offset <span class="op">=</span> y.mean()</span>
<span id="cb9-6"><a href="#cb9-6"></a>scale <span class="op">=</span> np.sqrt(y.var())</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-magnify" class="magnify" onclick="magnifyFigure(&#39;olympic-marathon&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-caption" class="caption-frame">
<p>Figure: Olympic marathon pace times since 1892.</p>
</div>
</div>
<p>Things to notice about the data include the outlier in 1904, in this year, the olympics was in St Louis, USA. Organizational problems and challenges with dust kicked up by the cars following the race meant that participants got lost, and only very few participants completed.</p>
<p>More recent years see more consistently quick marathons.</p>
<p>We already considered the olympic marathon data. In 1904 we noted there was an outlier example. Today we’ll see if we can deal with that outlier by considering a non-Gaussian likelihood. Noise sampled from a Student-<span class="math inline"><em>t</em></span> density is heavier tailed than that sampled from a Gaussian. However, it cannot be trivially assimilated into the Gaussian process. Below we use the <em>Laplace approximation</em> to incorporate this noise model.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>GPy.likelihoods.noise_model_constructors.student_t?</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>GPy.models.GPRegression?</span></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># # make a student t likelihood with standard parameters</span></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="co"># t_distribution = GPy.likelihoods.noise_model_constructors.student_t(deg_free=5, sigma2=2)</span></span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="co"># stu_t_likelihood = GPy.likelihoods.Laplace(Y.copy(), t_distribution)</span></span>
<span id="cb12-4"><a href="#cb12-4"></a></span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="co"># kern = GPy.kern.RBF(1, lengthscale=10) + GPy.kern.Bias(1)</span></span>
<span id="cb12-6"><a href="#cb12-6"></a><span class="co"># model = GPy.models.GPRegression(X, Y, kernel=kern, likelihood=stu_t_likelihood)</span></span>
<span id="cb12-7"><a href="#cb12-7"></a></span>
<span id="cb12-8"><a href="#cb12-8"></a>t_distribution <span class="op">=</span> GPy.likelihoods.StudentT(deg_free<span class="op">=</span><span class="dv">5</span>, sigma2<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-9"><a href="#cb12-9"></a>laplace <span class="op">=</span> GPy.inference.latent_function_inference.Laplace()</span>
<span id="cb12-10"><a href="#cb12-10"></a></span>
<span id="cb12-11"><a href="#cb12-11"></a>kern <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, lengthscale<span class="op">=</span><span class="dv">5</span>) <span class="op">+</span> GPy.kern.Bias(<span class="dv">1</span>)</span>
<span id="cb12-12"><a href="#cb12-12"></a>model <span class="op">=</span> GPy.core.GP(X, Y, kernel<span class="op">=</span>kern, inference_method<span class="op">=</span>laplace, likelihood<span class="op">=</span>t_distribution)</span>
<span id="cb12-13"><a href="#cb12-13"></a>model.constrain_positive(<span class="st">&#39;t_noise&#39;</span>)</span>
<span id="cb12-14"><a href="#cb12-14"></a></span>
<span id="cb12-15"><a href="#cb12-15"></a>model.optimize()</span>
<span id="cb12-16"><a href="#cb12-16"></a><span class="bu">print</span>(model)</span></code></pre></div>
<h1 id="sparse-gp-classification">Sparse GP Classification</h1>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/non-gaussian-gps.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/non-gaussian-gps.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>In this section we’ll combine expectation propagation with the low rank approximation to build a simple image classification application. For this toy example we’ll classify whether or not the subject of the image is wearing glasses.</p>
<p>Correspond to whether the subject of the image is wearing glasses. Set up the ipython environment and download the data:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="im">from</span> scipy <span class="im">import</span> io</span></code></pre></div>
<p>First let’s retrieve some data. We will use the ORL faces data set, our objective will be to classify whether or not the subject in an image is wearing glasess.</p>
<p>Here’s a simple way to visualise the data. Each pixel in the image will become an input to the GP.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>data <span class="op">=</span> pods.datasets.olivetti_glasses()</span>
<span id="cb15-2"><a href="#cb15-2"></a>X <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb15-3"><a href="#cb15-3"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb15-4"><a href="#cb15-4"></a>Xtest <span class="op">=</span> data[<span class="st">&#39;Xtest&#39;</span>]</span>
<span id="cb15-5"><a href="#cb15-5"></a>ytest <span class="op">=</span> data[<span class="st">&#39;Ytest&#39;</span>]</span>
<span id="cb15-6"><a href="#cb15-6"></a><span class="bu">print</span>(data[<span class="st">&#39;info&#39;</span>], data[<span class="st">&#39;details&#39;</span>], data[<span class="st">&#39;citation&#39;</span>])</span></code></pre></div>
<div class="figure">
<div id="olivetti-glasses-image-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="'../slides/diagrams/datasets/olivetti-glasses-image.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="olivetti-glasses-image-magnify" class="magnify" onclick="magnifyFigure(&#39;olivetti-glasses-image&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olivetti-glasses-image-caption" class="caption-frame">
<p>Figure: Image from the Oivetti glasses data sets.</p>
</div>
</div>
<p>Next we choose some inducing inputs. Here we’ve chosen inducing inputs by applying k-means clustering to the training data. Think about whether this is a good scheme for choosing the inputs? Can you devise a better one?</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>X.shape</span>
<span id="cb16-2"><a href="#cb16-2"></a>y.shape</span>
<span id="cb16-3"><a href="#cb16-3"></a><span class="bu">print</span>(X)</span></code></pre></div>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="im">from</span> scipy <span class="im">import</span> cluster</span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>M <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb18-2"><a href="#cb18-2"></a>X <span class="op">=</span> (X <span class="op">-</span> X.mean(<span class="dv">0</span>)[<span class="va">None</span>,:])<span class="op">/</span>X.std(<span class="dv">0</span>)[<span class="va">None</span>,:]</span>
<span id="cb18-3"><a href="#cb18-3"></a>Z <span class="op">=</span> np.random.permutation(X)[:M]</span></code></pre></div>
<p>Finally, we’re ready to build the classifier object.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>kern <span class="op">=</span> GPy.kern.RBF(X.shape[<span class="dv">1</span>],lengthscale<span class="op">=</span><span class="dv">20</span>) <span class="op">+</span> GPy.kern.White(X.shape[<span class="dv">1</span>],<span class="fl">0.001</span>)</span>
<span id="cb19-2"><a href="#cb19-2"></a>m <span class="op">=</span> GPy.models.SparseGPClassification(X, y, kernel<span class="op">=</span>kern, Z<span class="op">=</span>Z)</span>
<span id="cb19-3"><a href="#cb19-3"></a>m.optimize()</span>
<span id="cb19-4"><a href="#cb19-4"></a><span class="bu">print</span>(m)</span></code></pre></div>
<div class="figure">
<div id="olivetti-inducing-variable-gradients-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="'../slides/diagrams/datasets/olivetti-inducing-variable-gradients.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="olivetti-inducing-variable-gradients-magnify" class="magnify" onclick="magnifyFigure(&#39;olivetti-inducing-variable-gradients&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olivetti-inducing-variable-gradients-caption" class="caption-frame">
<p>Figure: The gradients of the inducing variable.</p>
</div>
</div>
<h2 id="thanks">Thanks!</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/non-gaussian-likelihoods.mdtmp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/non-gaussian-likelihoods.mdtmp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>For more information on these subjects and more you might want to check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 id="references">References</h1>

