---
title: "Approximate Gaussian Processes"
edit_url: https://github.com/lawrennd/talks/edit/gh-pages/_gpss/approximate-gps.md
week: 0
session: 6
reveal: 06-approximate-gps.slides.html
youtube: "b635kuSqLww"
edit_url: https://github.com/lawrennd/talks/edit/gh-pages/_gpss/approximate-gps.md
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install gpy</span></code></pre></div>
<h2 id="gpy-a-gaussian-process-framework-in-python">GPy: A Gaussian
Process Framework in Python</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/gpy-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/gpy-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Gaussian processes are a flexible tool for non-parametric analysis
with uncertainty. The GPy software was started in Sheffield to provide a
easy to use interface to GPs. One which allowed the user to focus on the
modelling rather than the mathematics.</p>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/gpss/./slides/diagrams//gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gpy-software-magnify" class="magnify"
onclick="magnifyFigure(&#39;gpy-software&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-software-caption" class="caption-frame">
<p>Figure: GPy is a BSD licensed software code base for implementing
Gaussian process models in Python. It is designed for teaching and
modelling. We welcome contributions which can be made through the GitHub
repository <a href="https://github.com/SheffieldML/GPy"
class="uri">https://github.com/SheffieldML/GPy</a></p>
</div>
</div>
<p>GPy is a BSD licensed software code base for implementing Gaussian
process models in python. This allows GPs to be combined with a wide
variety of software libraries.</p>
<p>The software itself is available on <a
href="https://github.com/SheffieldML/GPy">GitHub</a> and the team
welcomes contributions.</p>
<p>The aim for GPy is to be a probabilistic-style programming language,
i.e., you specify the model rather than the algorithm. As well as a
large range of covariance functions the software allows for non-Gaussian
likelihoods, multivariate outputs, dimensionality reduction and
approximations for larger data sets.</p>
<p>The documentation for GPy can be found <a
href="https://gpy.readthedocs.io/en/latest/">here</a>.</p>
<div class="figure">
<div id="sparse-gp-comic-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="https://mlatcl.github.io/gpss/./slides/diagrams//sparse-gps.jpg" width="45%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="sparse-gp-comic-magnify" class="magnify"
onclick="magnifyFigure(&#39;sparse-gp-comic&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sparse-gp-comic-caption" class="caption-frame">
<p>Figure: In recent years, approximations for Gaussian process models
haven’t been the most fashionable approach to machine learning. Image
credit: Kai Arulkumaran</p>
</div>
</div>
<p>Inference in a Gaussian process has computational complexity of <span
class="math inline">\(\mathcal{O}(n^3)\)</span> and storage demands of
<span class="math inline">\(\mathcal{O}(n^2)\)</span>. This is too large
for many modern data sets.</p>
<p>Low rank approximations allow us to work with Gaussian processes with
computational complexity of <span
class="math inline">\(\mathcal{O}(nm^2)\)</span> and storage demands of
<span class="math inline">\(\mathcal{O}(nm)\)</span>, where <span
class="math inline">\(m\)</span> is a user chosen parameter.</p>
<p>In machine learning, low rank approximations date back to <span
class="citation" data-cites="Smola:sparsegp00">Smola and Bartlett
(n.d.)</span>, <span class="citation"
data-cites="Williams:nystrom00">Williams and Seeger (n.d.)</span>, who
considered the Nyström approximation and <span class="citation"
data-cites="Csato:sparse02">Csató and Opper (2002)</span>;<span
class="citation" data-cites="Csato:thesis02">Csató (2002)</span> who
considered low rank approximations in the context of on-line learning.
Selection of active points for the approximation was considered by <span
class="citation" data-cites="Seeger:fast03">Seeger et al. (n.d.)</span>
and <span class="citation" data-cites="Snelson:pseudo05">Snelson and
Ghahramani (n.d.)</span> first proposed that the active set could be
optimized directly. Those approaches were reviewed by <span
class="citation" data-cites="Quinonero:unifying05">Quiñonero Candela and
Rasmussen (2005)</span> under a unifying likelihood approximation
perspective. General rules for deriving the maximum likelihood for these
sparse approximations were given in <span class="citation"
data-cites="Lawrence:larger07">Lawrence (n.d.)</span>.</p>
<p>Modern variational interpretations of these low rank approaches were
first explored in <span class="citation"
data-cites="Titsias:variational09">Titsias (n.d.)</span>. A more modern
summary which considers each of these approximations as an <span
class="math inline">\(\alpha\)</span>-divergence is given by <span
class="citation" data-cites="Thang:unifying17">Bui et al.
(2017)</span>.</p>
<h2 id="variational-compression">Variational Compression</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-variational-complexity.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-variational-complexity.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Inducing variables are a compression of the real observations. The
basic idea is can I create a new data set that summarizes all the
information in the original data set. If this data set is smaller, I’ve
compressed the information in the original data set.</p>
<p>Inducing variables can be thought of as pseudo-data, indeed in <span
class="citation" data-cites="Snelson:pseudo05">Snelson and Ghahramani
(n.d.)</span> they were referred to as <em>pseudo-points</em>.</p>
<p>The only requirement for inducing variables is that they are jointly
distributed as a Gaussian process with the original data. This means
that they can be from the space <span class="math inline">\(\mathbf{
f}\)</span> or a space that is related through a linear operator (see
e.g. <span class="citation" data-cites="Alvarez:efficient10">Álvarez et
al. (2010)</span>). For example we could choose to store the gradient of
the function at particular points or a value from the frequency spectrum
of the function <span class="citation"
data-cites="Lazaro:spectrum10">(Lázaro-Gredilla et al.,
2010)</span>.</p>
<h2 id="variational-compression-ii">Variational Compression II</h2>
<p>Inducing variables don’t only allow for the compression of the
non-parameteric information into a reduced data aset but they also allow
for computational scaling of the algorithms through, for example
stochastic variational approaches <span class="citation"
data-cites="Hensman:bigdata13">Hensman et al. (n.d.)</span> or
parallelization <span class="citation"
data-cites="Gal:distributed14">Gal et al. (n.d.)</span>,<span
class="citation" data-cites="Dai:gpu14">Dai et al. (2014)</span>, <span
class="citation" data-cites="Seeger:auto17">Seeger et al.
(2017)</span>.</p>
<p>We’ve seen how we go from parametric to non-parametric. The limit
implies infinite dimensional <span class="math inline">\(\mathbf{
w}\)</span>. Gaussian processes are generally non-parametric: combine
data with covariance function to get model. This representation
<em>cannot</em> be summarized by a parameter vector of a fixed size.</p>
<p>Parametric models have a representation that does not respond to
increasing training set size. Bayesian posterior distributions over
parameters contain the information about the training data, for example
if we use use Bayes’ rule from training data, <span
class="math display">\[
p\left(\mathbf{ w}|\mathbf{ y}, \mathbf{X}\right),
\]</span> to make predictions on test data <span class="math display">\[
p\left(y_*|\mathbf{X}_*, \mathbf{ y}, \mathbf{X}\right) = \int
              p\left(y_*|\mathbf{ w},\mathbf{X}_*\right)p\left(\mathbf{
w}|\mathbf{ y},
                \mathbf{X})\text{d}\mathbf{ w}\right)
\]</span> then <span class="math inline">\(\mathbf{ w}\)</span> becomes
a bottleneck for information about the training set to pass to the test
set. The solution is to increase <span class="math inline">\(m\)</span>
so that the bottleneck is so large that it no longer presents a problem.
How big is big enough for <span class="math inline">\(m\)</span>?
Non-parametrics says <span class="math inline">\(m\rightarrow
\infty\)</span>.</p>
<p>Now no longer possible to manipulate the model through the standard
parametric form. However, it <em>is</em> possible to express
<em>parametric</em> as GPs: <span class="math display">\[
k\left(\mathbf{ x}_i,\mathbf{ x}_j\right)=\phi_:\left(\mathbf{
x}_i\right)^\top\phi_:\left(\mathbf{ x}_j\right).
\]</span> These are known as degenerate covariance matrices. Their rank
is at most <span class="math inline">\(m\)</span>, non-parametric models
have full rank covariance matrices. Most well known is the “linear
kernel”, <span class="math display">\[
k(\mathbf{ x}_i, \mathbf{ x}_j) = \mathbf{ x}_i^\top\mathbf{ x}_j.
\]</span> For non-parametrics prediction at a new point, <span
class="math inline">\(\mathbf{ f}_*\)</span>, is made by conditioning on
<span class="math inline">\(\mathbf{ f}\)</span> in the joint
distribution. In GPs this involves combining the training data with the
covariance function and the mean function. Parametric is a special case
when conditional prediction can be summarized in a <em>fixed</em> number
of parameters. Complexity of parametric model remains fixed regardless
of the size of our training data set. For a non-parametric model the
required number of parameters grows with the size of the training
data.</p>
<ul>
<li>Everything we want to do with a GP involves marginalising <span
class="math inline">\(\mathbf{ f}\)</span>
<ul>
<li>Predictions</li>
<li>Marginal likelihood</li>
<li>Estimating covariance parameters</li>
</ul></li>
<li>The posterior of <span class="math inline">\(\mathbf{ f}\)</span> is
the central object. This means inverting <span
class="math inline">\(\mathbf{K}_{\mathbf{ f}\mathbf{ f}}\)</span>.</li>
</ul>
<img class="negate" src="https://mlatcl.github.io/gpss/./slides/diagrams//cov_approx.png" width="90%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
<div class="caption" style="">
Figure: Figure originally from presentation by Ed Snelson at NIPS
</div>
<p>The Nystr"om approximation takes the form, <span
class="math display">\[
\mathbf{K}_{\mathbf{ f}\mathbf{ f}}\approx \mathbf{Q}_{\mathbf{
f}\mathbf{ f}}= \mathbf{K}_{\mathbf{ f}\mathbf{ u}}\mathbf{K}_{\mathbf{
u}\mathbf{ u}}^{-1}\mathbf{K}_{\mathbf{ u}\mathbf{ f}}
\]</span> The idea is that instead of inverting <span
class="math inline">\(\mathbf{K}_{\mathbf{ f}\mathbf{ f}}\)</span>, we
make a low rank (or Nyström) approximation, and invert <span
class="math inline">\(\mathbf{K}_{\mathbf{ u}\mathbf{ u}}\)</span>
instead.</p>
<p>In the original Nystr"om method the columns to incorporate are
sampled from the complete set of columns (without replacement). In a
kernel matrix each of these columns corresponds to a data point. In the
Nystr"om method these points are sometimes called <em>landmark</em>
points.</p>
<table>
<tr>
<td width="30%">
<span class="math display">\[\mathbf{X},\,\mathbf{ y}\]</span>
</td>
<td width="70%">
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/gpss/./slides/diagrams//nomenclature1.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
<table>
<tr>
<td width="30%">
<span class="math display">\[\mathbf{X},\,\mathbf{ y}\]</span> <span
class="math display">\[{\color{blue} f(\mathbf{ x})} \sim {\mathcal
GP}\]</span>
</td>
<td width="70%">
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/gpss/./slides/diagrams//nomenclature2.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
<table>
<tr>
<td width="30%">
<span class="math display">\[\mathbf{X},\,\mathbf{ y}\]</span> <span
class="math display">\[f(\mathbf{ x}) \sim {\mathcal GP}\]</span><span
class="math display">\[p({\color{blue} \mathbf{ f}}) =
\mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{ f}\mathbf{
f}}\right)\]</span>
</td>
<td width="70%">
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/gpss/./slides/diagrams//nomenclature3.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
<table>
<tr>
<td width="30%">
<span class="math display">\[
\mathbf{X},\,\mathbf{ y}\]</span> <span
class="math display">\[f(\mathbf{ x}) \sim {\mathcal GP}
\]</span> <span class="math display">\[
p(\mathbf{ f}) = \mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{
f}\mathbf{ f}}\right)
\]</span> <span class="math display">\[p( \mathbf{ f}|\mathbf{
y},\mathbf{X})
\]</span>
</td>
<td width="70%">
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/gpss/./slides/diagrams//nomenclature3a.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
<p>Take an extra <span class="math inline">\(m\)</span> points on the
function, <span class="math inline">\(\mathbf{ u}=
f(\mathbf{Z})\)</span>. <span class="math display">\[p(\mathbf{
y},\mathbf{ f},\mathbf{ u}) = p(\mathbf{ y}|\mathbf{ f}) p(\mathbf{
f}|\mathbf{ u}) p(\mathbf{ u})\]</span></p>
<p><img class="negate" src="https://mlatcl.github.io/gpss/./slides/diagrams//cov_inducing_withX.png" width="60%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></p>
<p>Take and extra <span class="math inline">\(M\)</span> points on the
function, <span class="math inline">\(\mathbf{ u}=
f(\mathbf{Z})\)</span>. <span class="math display">\[p(\mathbf{
y},\mathbf{ f},\mathbf{ u}) = p(\mathbf{ y}|\mathbf{ f}) p(\mathbf{
f}|\mathbf{ u}) p(\mathbf{ u})\]</span> <span
class="math display">\[\begin{aligned}
    p(\mathbf{ y}|\mathbf{ f}) &amp;= \mathcal{N}\left(\mathbf{
y}|\mathbf{ f},\sigma^2 \mathbf{I}\right)\\
    p(\mathbf{ f}|\mathbf{ u}) &amp;= \mathcal{N}\left(\mathbf{ f}|
\mathbf{K}_{\mathbf{ f}\mathbf{ u}}\mathbf{K}_{\mathbf{ u}\mathbf{
u}}^{-1}\mathbf{ u}, \tilde{\mathbf{K}}\right)\\
    p(\mathbf{ u}) &amp;= \mathcal{N}\left(\mathbf{ u}|
\mathbf{0},\mathbf{K}_{\mathbf{ u}\mathbf{ u}}\right)
  \end{aligned}\]</span></p>
<table>
<tr>
<td width="30%">
<span class="math display">\[\mathbf{X},\,\mathbf{ y}\]</span> <span
class="math display">\[f(\mathbf{ x}) \sim {\mathcal GP}\]</span> <span
class="math display">\[p(\mathbf{ f}) =
\mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{ f}\mathbf{
f}}\right)\]</span> <span class="math display">\[p(\mathbf{ f}|\mathbf{
y},\mathbf{X})\]</span>
</td>
<td width="70%">
<img class="negate" src="https://mlatcl.github.io/gpss/./slides/diagrams//nomenclature4" width="90%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</td>
</tr>
</table>
<p><span class="math display">\[
\begin{align}
                           &amp;\qquad\mathbf{Z}, \mathbf{
u}\\                      &amp;p({\color{red} \mathbf{ u}})  =
\mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{ u}\mathbf{
u}}\right)\end{align}
\]</span></p>
<table>
<tr>
<td width="30%">
<span class="math display">\[\mathbf{X},\,\mathbf{ y}\]</span> <span
class="math display">\[f(\mathbf{ x}) \sim {\mathcal GP}\]</span> <span
class="math display">\[p(\mathbf{ f}) =
\mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{ f}\mathbf{
f}}\right)\]</span> <span class="math display">\[p(\mathbf{ f}|\mathbf{
y},\mathbf{X})\]</span> <span class="math display">\[p(\mathbf{ u})  =
\mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{ u}\mathbf{
u}}\right)\]</span> <span class="math display">\[\widetilde
p({\color{red}\mathbf{ u}}|\mathbf{ y},\mathbf{X})\]</span>
</td>
<td width="70%">
<img class="negate" src="https://mlatcl.github.io/gpss/./slides/diagrams//nomenclature5.png" width="90%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</td>
</tr>
</table>
<p><span>Instead of doing</span> <span class="math display">\[
p(\mathbf{ f}|\mathbf{ y},\mathbf{X}) = \frac{p(\mathbf{ y}|\mathbf{
f})p(\mathbf{ f}|\mathbf{X})}{\int p(\mathbf{ y}|\mathbf{ f})p(\mathbf{
f}|\mathbf{X}){\text{d}\mathbf{ f}}}
\]</span> <span>We’ll do</span> <span class="math display">\[
p(\mathbf{ u}|\mathbf{ y},\mathbf{Z}) = \frac{p(\mathbf{ y}|\mathbf{
u})p(\mathbf{ u}|\mathbf{Z})}{\int p(\mathbf{ y}|\mathbf{ u})p(\mathbf{
u}|\mathbf{Z}){\text{d}\mathbf{ u}}}
\]</span> </p>
<!--Flexible Parametric Approximation-->
<ul>
<li>Date back to {<span class="citation"
data-cites="Williams:nystrom00">Williams and Seeger (n.d.)</span>; <span
class="citation" data-cites="Smola:sparsegp00">Smola and Bartlett
(n.d.)</span>; <span class="citation" data-cites="Csato:sparse02">Csató
and Opper (2002)</span>; <span class="citation"
data-cites="Seeger:fast03">Seeger et al. (n.d.)</span>; <span
class="citation" data-cites="Snelson:pseudo05">Snelson and Ghahramani
(n.d.)</span>}. See {<span class="citation"
data-cites="Quinonero:unifying05">Quiñonero Candela and Rasmussen
(2005)</span>; <span class="citation" data-cites="Thang:unifying17">Bui
et al. (2017)</span>} for reviews.</li>
<li>We follow variational perspective of {<span class="citation"
data-cites="Titsias:variational09">Titsias (n.d.)</span>}.</li>
<li>This is an augmented variable method, followed by a collapsed
variational approximation {<span class="citation"
data-cites="King:klcorrection06">King and Lawrence (n.d.)</span>; <span
class="citation" data-cites="Hensman:fast12">Hensman et al.
(2012)</span>}.</li>
</ul>
<table>
<tr>
<td width="60%">
</td>
<td width="40%">
</td>
</tr>
</table>
<h2 id="variational-bound-on-pmathbf-ymathbf-u">Variational Bound on
<span class="math inline">\(p(\mathbf{ y}|\mathbf{ u})\)</span></h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/larger-variational.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/larger-variational.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The conditional density of the data given the inducing points can be
<em>lower</em> bounded variationally <span class="math display">\[
\begin{aligned}
    \log p(\mathbf{ y}|\mathbf{ u}) &amp; = \log \int p(\mathbf{
y}|\mathbf{ f}) p(\mathbf{ f}|\mathbf{ u}) \text{d}\mathbf{ f}\\ &amp; =
\int q(\mathbf{ f}) \log \frac{p(\mathbf{ y}|\mathbf{ f}) p(\mathbf{
f}|\mathbf{ u})}{q(\mathbf{ f})}\text{d}\mathbf{ f}+ \text{KL}\left(
q(\mathbf{ f})\,\|\,p(\mathbf{ f}|\mathbf{ y}, \mathbf{ u}) \right).
\end{aligned}
\]</span></p>
<p>The key innovation from <span class="citation"
data-cites="Titsias:variational09">Titsias (n.d.)</span> was to then
make a particular choice for <span class="math inline">\(q(\mathbf{
f})\)</span>. If we set <span class="math inline">\(q(\mathbf{
f})=p(\mathbf{ f}|\mathbf{ u})\)</span>, <span class="math display">\[
  \log p(\mathbf{ y}|\mathbf{ u}) \geq \int p(\mathbf{ f}|\mathbf{ u})
\log p(\mathbf{ y}|\mathbf{ f})\text{d}\mathbf{ f}.
  \]</span> <span class="math display">\[
  p(\mathbf{ y}|\mathbf{ u}) \geq \exp \int p(\mathbf{ f}|\mathbf{ u})
\log p(\mathbf{ y}|\mathbf{ f})\text{d}\mathbf{ f}.
  \]</span></p>
<h2 id="optimal-compression-in-inducing-variables">Optimal Compression
in Inducing Variables</h2>
<p>Maximizing the lower bound minimizes the Kullback-Leibler divergence
(or <em>information gain</em>) between our approximating density, <span
class="math inline">\(p(\mathbf{ f}|\mathbf{ u})\)</span> and the true
posterior density, <span class="math inline">\(p(\mathbf{ f}|\mathbf{
y}, \mathbf{ u})\)</span>.</p>
<p><span class="math display">\[
  \text{KL}\left( p(\mathbf{ f}|\mathbf{ u})\,\|\,p(\mathbf{ f}|\mathbf{
y}, \mathbf{ u}) \right) = \int p(\mathbf{ f}|\mathbf{ u}) \log
\frac{p(\mathbf{ f}|\mathbf{ u})}{p(\mathbf{ f}|\mathbf{ y}, \mathbf{
u})}\text{d}\mathbf{ u}
  \]</span></p>
<p>This bound is minimized when the information stored about <span
class="math inline">\(\mathbf{ y}\)</span> is already stored in <span
class="math inline">\(\mathbf{ u}\)</span>. In other words, maximizing
the bound seeks an <em>optimal compression</em> from the <em>information
gain</em> perspective.</p>
<p>For the case where <span class="math inline">\(\mathbf{ u}= \mathbf{
f}\)</span> the bound is exact (<span class="math inline">\(\mathbf{
f}\)</span> <span class="math inline">\(d\)</span>-separates <span
class="math inline">\(\mathbf{ y}\)</span> from <span
class="math inline">\(\mathbf{ u}\)</span>).</p>
<h2 id="choice-of-inducing-variables">Choice of Inducing Variables</h2>
<p>The quality of the resulting bound is determined by the choice of the
inducing variables. You are free to choose whichever heuristics you like
for the inducing variables, as long as they are drawn jointly from a
valid Gaussian process, i.e. such that <span class="math display">\[
\begin{bmatrix}
\mathbf{ f}\\
\mathbf{ u}
\end{bmatrix} \sim \mathcal{N}\left(\mathbf{0},\mathbf{K}\right)
\]</span> where the kernel matrix itself can be decomposed into <span
class="math display">\[
\mathbf{K}=
\begin{bmatrix}
\mathbf{K}_{\mathbf{ f}\mathbf{ f}}&amp; \mathbf{K}_{\mathbf{ f}\mathbf{
u}}\\
\mathbf{K}_{\mathbf{ u}\mathbf{ f}}&amp; \mathbf{K}_{\mathbf{ u}\mathbf{
u}}
\end{bmatrix}
\]</span> Choosing the inducing variables amounts to specifying <span
class="math inline">\(\mathbf{K}_{\mathbf{ f}\mathbf{ u}}\)</span> and
<span class="math inline">\(\mathbf{K}_{\mathbf{ u}\mathbf{ u}}\)</span>
such that <span class="math inline">\(\mathbf{K}\)</span> remains
positive definite. The typical choice is to choose <span
class="math inline">\(\mathbf{ u}\)</span> in the same domain as <span
class="math inline">\(\mathbf{ f}\)</span>, associating each inducing
output, <span class="math inline">\(u_i\)</span> with a corresponding
input location <span class="math inline">\(\mathbf{ z}\)</span>.
However, more imaginative choices are absolutely possible. In
particular, if <span class="math inline">\(\mathbf{ u}\)</span> is
related to <span class="math inline">\(\mathbf{ f}\)</span> through a
linear operator (see e.g. <span class="citation"
data-cites="Alvarez:efficient10">Álvarez et al. (2010)</span>), then
valid <span class="math inline">\(\mathbf{K}_{\mathbf{ u}\mathbf{
u}}\)</span> and <span class="math inline">\(\mathbf{K}_{\mathbf{
u}\mathbf{ f}}\)</span> can be constructed. For example we could choose
to store the gradient of the function at particular points or a value
from the frequency spectrum of the function <span class="citation"
data-cites="Lazaro:spectrum10">(Lázaro-Gredilla et al.,
2010)</span>.</p>
<h2 id="variational-compression-ii-1">Variational Compression II</h2>
<p>Inducing variables don’t only allow for the compression of the
non-parameteric information into a reduced data set but they also allow
for computational scaling of the algorithms through, for example
stochastic variational approaches<span class="citation"
data-cites="Hoffman:stochastic12 Hensman:bigdata13">(Hensman et al.,
n.d.; Hoffman et al., 2012)</span> or parallelization <span
class="citation"
data-cites="Gal:distributed14 Dai:gpu14 Seeger:auto17">(Dai et al.,
2014; Gal et al., n.d.; Seeger et al., 2017)</span>.</p>
<ul>
<li><p>If the likelihood, <span class="math inline">\(p(\mathbf{
y}|\mathbf{ f})\)</span>, factorizes </p></li>
<li><p>&lt;8-&gt; Then the bound factorizes.</p></li>
<li><p>&lt;10-&gt; Now need a choice of distributions for <span
class="math inline">\(\mathbf{ f}\)</span> and <span
class="math inline">\(\mathbf{ y}|\mathbf{ f}\)</span> …</p></li>
<li><p>Choose to go a different way.</p></li>
<li><p>Introduce a set of auxiliary variables, <span
class="math inline">\(\mathbf{ u}\)</span>, which are <span
class="math inline">\(m\)</span> in length.</p></li>
<li><p>They are like “artificial data”.</p></li>
<li><p>Used to <em>induce</em> a distribution: <span
class="math inline">\(q(\mathbf{ u}|\mathbf{ y})\)</span></p></li>
<li><p>Introduce variable set which is <em>finite</em> dimensional.
<span class="math display">\[
p(\mathbf{ y}^*|\mathbf{ y}) \approx \int p(\mathbf{ y}^*|\mathbf{ u})
q(\mathbf{ u}|\mathbf{ y}) \text{d}\mathbf{ u}
\]</span></p></li>
<li><p>But dimensionality of <span class="math inline">\(\mathbf{
u}\)</span> can be changed to improve approximation.</p></li>
<li><p>Model for our data, <span class="math inline">\(\mathbf{
y}\)</span></p>
<table>
<tr>
<td width>
<p><span class="math display">\[p(\mathbf{ y})\]</span></p>
</td>
<td width>
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/py.svg" width style=" ">
</object>
</td>
</tr>
</table></li>
<li><p>Prior density over <span class="math inline">\(\mathbf{
f}\)</span>. Likelihood relates data, <span
class="math inline">\(\mathbf{ y}\)</span>, to <span
class="math inline">\(\mathbf{ f}\)</span>.</p>
<table>
<tr>
<td width>
<p><span class="math display">\[p(\mathbf{ y})=\int p(\mathbf{
y}|\mathbf{ f})p(\mathbf{ f})\text{d}\mathbf{ f}\]</span></p>
</td>
<td width>
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/pygfpf.svg" width style=" ">
</object>
</td>
</tr>
</table></li>
<li><p>Prior density over <span class="math inline">\(\mathbf{
f}\)</span>. Likelihood relates data, <span
class="math inline">\(\mathbf{ y}\)</span>, to <span
class="math inline">\(\mathbf{ f}\)</span>.</p>
<table>
<tr>
<td width>
<p><span class="math display">\[p(\mathbf{ y})=\int p(\mathbf{
y}|\mathbf{ f})p(\mathbf{ u}|\mathbf{ f})p(\mathbf{ f})\text{d}\mathbf{
f}\text{d}\mathbf{ u}\]</span></p>
</td>
<td width>
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/pygfpugfpf.svg" width style=" ">
</object>
</td>
</tr>
</table></li>
</ul>
<table>
<tr>
<td width>
<span class="math display">\[p(\mathbf{ y})=\int \int p(\mathbf{
y}|\mathbf{ f})p(\mathbf{ f}|\mathbf{ u})\text{d}\mathbf{ f}p(\mathbf{
u})\text{d}\mathbf{ u}\]</span>
</td>
<td width>
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/pygfpfgupu.svg" width style=" ">
</object>
</td>
</tr>
</table>
<table>
<tr>
<td width>
<span class="math display">\[p(\mathbf{ y})=\int \int p(\mathbf{
y}|\mathbf{ f})p(\mathbf{ f}|\mathbf{ u})\text{d}\mathbf{ f}p(\mathbf{
u})\text{d}\mathbf{ u}\]</span>
</td>
<td width>
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/pygfpfgupu2.svg" width style=" ">
</object>
</td>
</tr>
</table>
<table>
<tr>
<td width>
<span class="math display">\[p(\mathbf{ y}|\mathbf{ u})=\int p(\mathbf{
y}|\mathbf{ f})p(\mathbf{ f}|\mathbf{ u})\text{d}\mathbf{ f}\]</span>
</td>
<td width>
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/pygfpfgu.svg" width style=" ">
</object>
</td>
</tr>
</table>
<table>
<tr>
<td width>
<span class="math display">\[p(\mathbf{ y}|\mathbf{ u})\]</span>
</td>
<td width>
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/pygu.svg" width style=" ">
</object>
</td>
</tr>
</table>
<table>
<tr>
<td width>
<span class="math display">\[p(\mathbf{ y}|\boldsymbol{
\theta})\]</span>
</td>
<td width>
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/pygtheta.svg" width style=" ">
</object>
</td>
</tr>
</table>
<ul>
<li>Replace true <span class="math inline">\(p(\mathbf{ u}|\mathbf{
y})\)</span> with approximation <span class="math inline">\(q(\mathbf{
u}|\mathbf{ y})\)</span>.</li>
<li>Minimize KL divergence between approximation and truth.</li>
<li>This is similar to the Bayesian posterior distribution.</li>
<li>But it’s placed over a set of ‘pseudo-observations’.</li>
</ul>
<p><span class="math display">\[\mathbf{ f}, \mathbf{ u}\sim
\mathcal{N}\left(\mathbf{0},\begin{bmatrix}\mathbf{K}_{\mathbf{
f}\mathbf{ f}}&amp; \mathbf{K}_{\mathbf{ f}\mathbf{
u}}\\\mathbf{K}_{\mathbf{ u}\mathbf{ f}}&amp; \mathbf{K}_{\mathbf{
u}\mathbf{ u}}\end{bmatrix}\right)\]</span> <span
class="math display">\[\mathbf{ y}|\mathbf{ f}= \prod_{i}
\mathcal{N}\left(f,\sigma^2\right)\]</span></p>
<!--Variational Compression-->
<p>For Gaussian likelihoods: </p>
<p>Define: <span class="math display">\[q_{i, i} =
\text{var}_{p(f_i|\mathbf{ u})}\left( f_i \right) = \left\langle
f_i^2\right\rangle_{p(f_i|\mathbf{ u})} - \left\langle
f_i\right\rangle_{p(f_i|\mathbf{ u})}^2\]</span> We can write: <span
class="math display">\[c_i =
\exp\left(-{\frac{q_{i,i}}{2\sigma^2}}\right)\]</span> If joint
distribution of <span class="math inline">\(p(\mathbf{ f}, \mathbf{
u})\)</span> is Gaussian then: <span class="math display">\[q_{i, i} =
k_{i, i} - \mathbf{ k}_{i, \mathbf{ u}}^\top \mathbf{K}_{\mathbf{ u},
\mathbf{ u}}^{-1} \mathbf{ k}_{i, \mathbf{ u}}\]</span></p>
<p><span class="math inline">\(c_i\)</span> is not a function of <span
class="math inline">\(\mathbf{ u}\)</span> but <em>is</em> a function of
<span class="math inline">\(\mathbf{X}_\mathbf{ u}\)</span>.</p>
<!--frame end-->
<!--frame start-->
<ul>
<li><p>The sum of <span class="math inline">\(q_{i,i}\)</span> is the
<em>total conditional variance</em>.</p></li>
<li><p>If conditional density <span class="math inline">\(p(\mathbf{
f}|\mathbf{ u})\)</span> is Gaussian then it has covariance <span
class="math display">\[\mathbf{Q} = \mathbf{K}_{\mathbf{ f}\mathbf{ f}}
- \mathbf{K}_{\mathbf{ f}\mathbf{ u}}\mathbf{K}_{\mathbf{ u}\mathbf{
u}}^{-1} \mathbf{K}_{\mathbf{ u}\mathbf{ f}}\]</span></p></li>
<li><p><span class="math inline">\(\text{tr}\left(\mathbf{Q}\right) =
\sum_{i}q_{i,i}\)</span> is known as total variance.</p></li>
<li><p>Because it is on conditional distribution we call it <em>total
conditional variance</em>.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
<ul>
<li><p>Measure the ’capacity of a density’.</p></li>
<li><p>Determinant of covariance represents ’volume’ of
density.</p></li>
<li><p>log determinant is entropy: sum of <em>log</em> eigenvalues of
covariance.</p></li>
<li><p>trace of covariance is total variance: sum of eigenvalues of
covariance.</p></li>
<li><p><span class="math inline">\(\lambda &gt; \log \lambda\)</span>
then total conditional variance upper bounds entropy.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
<p>Exponentiated total variance bounds determinant. <span
class="math display">\[\det{\mathbf{Q}} &lt; \exp
\text{tr}\left(\mathbf{Q}\right)\]</span> Because <span
class="math display">\[\prod_{i=1}^k \lambda_i &lt; \prod_{i=1}^k
\exp(\lambda_i)\]</span> where <span
class="math inline">\(\{\lambda_i\}_{i=1}^k\)</span> are the
<em>positive</em> eigenvalues of <span
class="math inline">\(\mathbf{Q}\)</span> This in turn implies <span
class="math display">\[\det{\mathbf{Q}} &lt; \prod_{i=1}^k
\exp\left(q_{i,i}\right)\]</span></p>
<!--frame end-->
<!--frame start-->
<ul>
<li><p>Conditional density <span class="math inline">\(p(\mathbf{
f}|\mathbf{ u})\)</span> can be seen as a <em>communication
channel</em>.</p></li>
<li><p>Normally we have: <span> <span
class="math display">\[\text{Transmitter} \stackrel{\mathbf{
u}}{\rightarrow} \begin{smallmatrix}p(\mathbf{ f}|\mathbf{ u}) \\
\text{Channel}\end{smallmatrix} \stackrel{\mathbf{ f}}{\rightarrow}
\text{Receiver}\]</span></span> and we control <span
class="math inline">\(p(\mathbf{ u})\)</span> (the source
density).</p></li>
<li><p><em>Here</em> we can also control the transmission channel <span
class="math inline">\(p(\mathbf{ f}|\mathbf{ u})\)</span>.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
<p>Substitute variational bound into marginal likelihood: <span
class="math display">\[p(\mathbf{ y})\geq \prod_{i=1}^nc_i \int
\mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle,\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{
u}\]</span> Note that: <span class="math display">\[\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u})} = \mathbf{K}_{\mathbf{ f},
\mathbf{ u}} \mathbf{K}_{\mathbf{ u}, \mathbf{ u}}^{-1}\mathbf{
u}\]</span> is <em>linearly</em> dependent on <span
class="math inline">\(\mathbf{ u}\)</span>.</p>
<!--frame end-->
<!--frame start-->
<p>Making the marginalization of <span class="math inline">\(\mathbf{
u}\)</span> straightforward. In the Gaussian case: <span
class="math display">\[p(\mathbf{ u}) = \mathcal{N}\left(\mathbf{
u}|\mathbf{0},\mathbf{K}_{\mathbf{ u},\mathbf{ u}}\right)\]</span> </p>
<!--frame end-->
<p><span class="math display">\[\log p(\mathbf{ y}|\mathbf{ u}) =
\log\int p(\mathbf{ y}|\mathbf{ f})p(\mathbf{ f}|\mathbf{
u},\mathbf{X})\text{d}\mathbf{ f}\]</span></p>
<p><span class="math display">\[\log p(\mathbf{ y}|\mathbf{ u}) = \log
\mathbb{E}_{p(\mathbf{ f}|\mathbf{ u},\mathbf{X})}\left[p(\mathbf{
y}|\mathbf{ f})\right]\]</span> <span class="math display">\[\log
p(\mathbf{ y}|\mathbf{ u}) \geq  \mathbb{E}_{p(\mathbf{ f}|\mathbf{
u},\mathbf{X})}\left[\log p(\mathbf{ y}|\mathbf{ f})\right]\triangleq
\log\widetilde p(\mathbf{ y}|\mathbf{ u})\]</span> </p>
<p><span> No inversion of <span
class="math inline">\(\mathbf{K}_{\mathbf{ f}\mathbf{ f}}\)</span>
required</span></p>
<div style="text-align:right">
<span class="citation" data-cites="Titsias:variational09">Titsias
(n.d.)</span>
</div>
<p><span class="math display">\[p(\mathbf{ y}|\mathbf{ u}) =
\frac{p(\mathbf{ y}|\mathbf{ f})p(\mathbf{ f}|\mathbf{ u})}{p(\mathbf{
f}|\mathbf{ y}, \mathbf{ u})}\]</span> <span class="math display">\[\log
p(\mathbf{ y}|\mathbf{ u}) = \log p(\mathbf{ y}|\mathbf{ f}) + \log
\frac{p(\mathbf{ f}|\mathbf{ u})}{p(\mathbf{ f}|\mathbf{ y}, \mathbf{
u})}\]</span> <span class="math display">\[\log p(\mathbf{ y}|\mathbf{
u}) = \bbE_{p(\mathbf{ f}|\mathbf{ u})}\big[\log p(\mathbf{ y}|\mathbf{
f})\big] + \bbE_{p(\mathbf{ f}|\mathbf{ u})}\big[\log \frac{p(\mathbf{
f}|\mathbf{ u})}{p(\mathbf{ f}|\mathbf{ y}, \mathbf{ u})}\big]\]</span>
<span class="math display">\[\log p(\mathbf{ y}|\mathbf{ u}) =
\widetilde p(\mathbf{ y}|\mathbf{ u}) + \textsc{KL}[p(\mathbf{
f}|\mathbf{ u})||p(\mathbf{ f}|\mathbf{ y}, \mathbf{ u})]\]</span></p>
<p><span> No inversion of <span
class="math inline">\(\mathbf{K}_{\mathbf{ f}\mathbf{ f}}\)</span>
required</span></p>
<p><span class="math display">\[\widetilde p(\mathbf{ y}|\mathbf{ u})  =
\prod_{i=1}^n\widetilde p(y_i|\mathbf{ u})\]</span> <span
class="math display">\[\widetilde p(y|\mathbf{ u}) =
\mathcal{N}\left(y|\mathbf{ k}_{fu}\mathbf{K}_{\mathbf{ u}\mathbf{
u}}^{-1}\mathbf{ u},\sigma^2\right)
\,{\color{red}\exp\left\{-\tfrac{1}{2\sigma^2}\left(k_{ff}- \mathbf{
k}_{fu}\mathbf{K}_{\mathbf{ u}\mathbf{ u}}^{-1}\mathbf{
k}_{uf}\right)\right\}}\]</span></p>
<p><span>A straightforward likelihood approximation, and a penalty
term</span></p>
<p><span class="math display">\[\widetilde p(\mathbf{ u}|\mathbf{
y},\mathbf{Z}) = \frac{\widetilde p(\mathbf{ y}|\mathbf{ u})p(\mathbf{
u}|\mathbf{Z})}{\int \widetilde p(\mathbf{ y}|\mathbf{ u})p(\mathbf{
u}|\mathbf{Z})\text{d}{\mathbf{ u}}}\]</span></p>
<ul>
<li><p>Computing the posterior costs <span
class="math inline">\(\mathcal{O}(nm^2)\)</span></p></li>
<li><p>We also get a lower bound of the marginal likelihood</p></li>
</ul>
<p><span
class="math display">\[{\color{red}\sum_{i=1}^n-\tfrac{1}{2\sigma^2}\left(k_{ff}-
\mathbf{ k}_{fu}\mathbf{K}_{\mathbf{ u}\mathbf{ u}}^{-1}\mathbf{
k}_{uf}\right)}\]</span></p>
<p><span
class="math display">\[{\color{red}\sum_{i=1}^n-\tfrac{1}{2\sigma^2}\left(k_{ff}-
\mathbf{ k}_{fu}\mathbf{K}_{\mathbf{ u}\mathbf{ u}}^{-1}\mathbf{
k}_{uf}\right)}\]</span></p>
<!--![image](../../../gp/tex/diagrams/cov_approx){width="60.00000%"}-->
<!--![image](../../../gp/tex/diagrams/cov_approx_opt){width="60.00000%"}-->
<p><span>It’s easy to show that as <span
class="math inline">\(\mathbf{Z}\to \mathbf{X}\)</span>:</span></p>
<ul>
<li><p><span class="math inline">\(\mathbf{ u}\to \mathbf{ f}\)</span>
(and the posterior is exact)</p></li>
<li><p>The penalty term is zero.</p></li>
<li><p>The cost returns to <span
class="math inline">\(\mathcal{O}(n^3)\)</span></p></li>
</ul>
<ul>
<li><p><br />
</p></li>
<li><p></p></li>
</ul>
<p><span>So far we:</span></p>
<ul>
<li><p>introduced <span class="math inline">\(\mathbf{Z}, \mathbf{
u}\)</span></p></li>
<li><p>approximated the intergral over <span
class="math inline">\(\mathbf{ f}\)</span> variationally</p></li>
<li><p>captured the information in <span
class="math inline">\(\widetilde p(\mathbf{ u}|\mathbf{
y})\)</span></p></li>
<li><p>obtained a lower bound on the marginal likeihood</p></li>
<li><p>saw the effect of the penalty term</p></li>
<li><p>prediction for new points</p></li>
</ul>
<p><span>Omitted details:</span></p>
<ul>
<li><p>optimization of the covariance parameters using the
bound</p></li>
<li><p>optimization of Z (simultaneously)</p></li>
<li><p>the form of <span class="math inline">\(\widetilde p(\mathbf{
u}|\mathbf{ y})\)</span></p></li>
<li><p>historical approximations</p></li>
</ul>
<span>Subset selection</span>
<div style="text-align:right">
<span class="citation" data-cites="Lawrence:ivm02">Lawrence et al.
(n.d.)</span>
</div>
<ul>
<li><p>Random or systematic</p></li>
<li><p>Set <span class="math inline">\(\mathbf{Z}\)</span> to subset of
<span class="math inline">\(\mathbf{X}\)</span></p></li>
<li><p>Set <span class="math inline">\(\mathbf{ u}\)</span> to subset of
<span class="math inline">\(\mathbf{ f}\)</span></p></li>
<li><p>Approximation to <span class="math inline">\(p(\mathbf{
y}|\mathbf{ u})\)</span>:</p>
<ul>
<li><p>$ p(_i) = p(_i_i) i$</p></li>
<li><p>$ p(_i) = 1 i$</p></li>
</ul></li>
</ul>
<div style="text-align:right">
<span class="citation" data-cites="Quinonero:unifying05">Quiñonero
Candela and Rasmussen (2005)</span>
</div>
<p>{Deterministic Training Conditional (DTC)}</p>
<ul>
<li><p>Approximation to <span class="math inline">\(p(\mathbf{
y}|\mathbf{ u})\)</span>:</p>
<ul>
<li>$ p(_i) = (_i, [_i])$</li>
</ul></li>
<li><p>As our variational formulation, but without penalty</p></li>
</ul>
<p>Optimization of <span class="math inline">\(\mathbf{Z}\)</span> is
difficult</p>
<span>Fully Independent Training Conditional</span>
<div style="text-align:right">
<span class="citation" data-cites="Snelson:pseudo05">Snelson and
Ghahramani (n.d.)</span>
</div>
<ul>
<li><p>Approximation to <span class="math inline">\(p(\mathbf{
y}|\mathbf{ u})\)</span>:</p></li>
<li><p>$ p() = _i p(_i) $</p></li>
</ul>
<p>Optimization of <span class="math inline">\(\mathbf{Z}\)</span> is
still difficult, and there are some weird heteroscedatic effects</p>
<ul>
<li>GP-LVM Provides probabilistic non-linear dimensionality
reduction.</li>
<li>How to select the dimensionality?</li>
<li>Need to estimate marginal likelihood.</li>
<li>In standard GP-LVM it increases with increasing <span
class="math inline">\(q\)</span>.</li>
</ul>
<table>
<tr>
<td width="40%">
<p><strong>Bayesian GP-LVM</strong></p>
<ul>
<li>Start with a standard GP-LVM.</li>
<li>Apply standard latent variable approach:
<ul>
<li>Define Gaussian prior over , <span
class="math inline">\(\mathbf{Z}\)</span>.</li>
<li>Integrate out .</li>
<li>Unfortunately integration is intractable.</li>
</ul></li>
</ul>
</td>
<td width="60%">
<center>
{ }
</center>
</td>
</tr>
</table>
<h2 id="standard-variational-approach-fails">Standard Variational
Approach Fails</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/variational-bayes-gplvm-long.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/variational-bayes-gplvm-long.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<ul>
<li>Standard variational bound has the form: <span
class="math display">\[
\mathcal{L}= \left\langle\log p(\mathbf{
y}|\mathbf{Z})\right\rangle_{q(\mathbf{Z})} + \text{KL}\left(
q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)
\]</span></li>
</ul>
<p>The standard variational approach would require the expectation of
<span class="math inline">\(\log p(\mathbf{ y}|\mathbf{Z})\)</span>
under <span class="math inline">\(q(\mathbf{Z})\)</span>. <span
class="math display">\[
  \begin{align}
  \log p(\mathbf{ y}|\mathbf{Z}) = &amp; -\frac{1}{2}\mathbf{
y}^\top\left(\mathbf{K}_{\mathbf{ f}, \mathbf{
f}}+\sigma^2\mathbf{I}\right)^{-1}\mathbf{ y}\\ &amp; -\frac{1}{2}\log
\det{\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}+\sigma^2 \mathbf{I}}
-\frac{n}{2}\log 2\pi
  \end{align}
  \]</span> But this is extremely difficult to compute because <span
class="math inline">\(\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}\)</span> is
dependent on <span class="math inline">\(\mathbf{Z}\)</span> and it
appears in the inverse.</p>
<h2 id="variational-bayesian-gp-lvm">Variational Bayesian GP-LVM</h2>
<p>The alternative approach is to consider the collapsed variational
bound (used for low rank (sparse is a misnomer) Gaussian process
approximations. <span class="math display">\[
    p(\mathbf{ y})\geq \prod_{i=1}^nc_i \int \mathcal{N}\left(\mathbf{
y}|\left\langle\mathbf{
f}\right\rangle,\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{
u}
  \]</span> <span class="math display">\[
    p(\mathbf{ y}|\mathbf{Z})\geq \prod_{i=1}^nc_i \int
\mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{ u}
  \]</span> <span class="math display">\[
      \int p(\mathbf{ y}|\mathbf{Z})p(\mathbf{Z}) \text{d}\mathbf{Z}\geq
\int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)
p(\mathbf{Z})\text{d}\mathbf{Z}p(\mathbf{ u}) \text{d}\mathbf{ u}
  \]</span></p>
<p>To integrate across <span class="math inline">\(\mathbf{Z}\)</span>
we apply the lower bound to the inner integral. <span
class="math display">\[
    \begin{align}
    \int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{
y}|\left\langle\mathbf{ f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)
p(\mathbf{Z})\text{d}\mathbf{Z}\geq &amp;
\left\langle\sum_{i=1}^n\log  c_i\right\rangle_{q(\mathbf{Z})}\\ &amp;
+\left\langle\log\mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)\right\rangle_{q(\mathbf{Z})}\\&amp;
+ \text{KL}\left( q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)    
    \end{align}
  \]</span> * Which is analytically tractable for Gaussian <span
class="math inline">\(q(\mathbf{Z})\)</span> and some covariance
functions.</p>
<ul>
<li><p>Need expectations under <span
class="math inline">\(q(\mathbf{Z})\)</span> of: <span
class="math display">\[
\log c_i = \frac{1}{2\sigma^2} \left[k_{i, i} - \mathbf{ k}_{i, \mathbf{
u}}^\top \mathbf{K}_{\mathbf{ u}, \mathbf{ u}}^{-1} \mathbf{ k}_{i,
\mathbf{ u}}\right]
\]</span> and <span class="math display">\[
\log \mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{
u},\mathbf{Y})},\sigma^2\mathbf{I}\right) = -\frac{1}{2}\log
2\pi\sigma^2 - \frac{1}{2\sigma^2}\left(y_i - \mathbf{K}_{\mathbf{ f},
\mathbf{ u}}\mathbf{K}_{\mathbf{ u},\mathbf{ u}}^{-1}\mathbf{
u}\right)^2
\]</span></p></li>
<li><p>This requires the expectations <span class="math display">\[
\left\langle\mathbf{K}_{\mathbf{ f},\mathbf{
u}}\right\rangle_{q(\mathbf{Z})}
\]</span> and <span class="math display">\[
\left\langle\mathbf{K}_{\mathbf{ f},\mathbf{ u}}\mathbf{K}_{\mathbf{
u},\mathbf{ u}}^{-1}\mathbf{K}_{\mathbf{ u},\mathbf{
f}}\right\rangle_{q(\mathbf{Z})}
\]</span> which can be computed analytically for some covariance
functions <span class="citation"
data-cites="Damianou:variational15">(Damianou et al., 2016)</span> or
through sampling <span class="citation"
data-cites="Damianou:thesis2015 Salimbeni:doubly2017">(Damianou, 2015;
Salimbeni and Deisenroth, 2017)</span>.</p></li>
</ul>
<div style="text-align:right">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Andreas Damianou
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/gpss/./slides/diagrams//people/andreas-damianou.png" clip-path="url(#clip0)"/>
</svg>
</div>
<br />

<div style="text-align:right">
<span class="citation" data-cites="Damianou:deepgp13">Damianou and
Lawrence (2013)</span>
</div>
<p><br />
</p>
<ul>
<li><p>Augment each layer with inducing variables <span
class="math inline">\(\mathbf{ u}_i\)</span>.</p></li>
<li><p>Apply variational compression, <span
class="math display">\[\begin{align}
      p(\mathbf{ y}, \{\mathbf{ h}_i\}_{i=1}^{\ell-1}|\{\mathbf{
u}_i\}_{i=1}^{\ell}, \mathbf{X}) \geq &amp;
      \tilde p(\mathbf{ y}|\mathbf{ u}_{\ell}, \mathbf{
h}_{\ell-1})\prod_{i=2}^{\ell-1} \tilde p(\mathbf{ h}_i|\mathbf{
u}_i,\mathbf{ h}_{i-1}) \tilde p(\mathbf{ h}_1|\mathbf{ u}_i,\mathbf{X})
\nonumber \\
      &amp; \times
      \exp\left(\sum_{i=1}^\ell-\frac{1}{2\sigma^2_i}\text{tr}\left(\boldsymbol{
\Sigma}_{i}\right)\right)
      \label{eq:deep_structure}
    \end{align}\]</span> where <span class="math display">\[\tilde
p(\mathbf{ h}_i|\mathbf{ u}_i,\mathbf{ h}_{i-1})
    = \mathcal{N}\left(\mathbf{ h}_i|\mathbf{K}_{\mathbf{ h}_{i}\mathbf{
u}_{i}}\mathbf{K}_{\mathbf{ u}_i\mathbf{ u}_i}^{-1}\mathbf{
u}_i,\sigma^2_i\mathbf{I}\right).\]</span></p></li>
</ul>
<div style="text-align:right">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip1">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
James Hensman
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/gpss/./slides/diagrams//people/james-hensman.png" clip-path="url(#clip1)"/>
</svg>
</div>
<br />

<div style="text-align:right">
<span class="citation" data-cites="Hensman:nested14">Hensman and
Lawrence (2014)</span>
</div>
<ul>
<li><p>By sustaining explicity distributions over inducing variables
James Hensman has developed a nested variant of variational
compression.</p></li>
<li><p>Exciting thing: it mathematically looks like a deep neural
network, but with inducing variables in the place of basis
functions.</p></li>
<li><p>Additional complexity control term in the objective
function.</p></li>
</ul>
<p><span class="math display">\[\begin{align}
    \log p(\mathbf{ y}|\mathbf{X})  \geq &amp;
    %
    -\frac{1}{\sigma_1^2} \text{tr}\left(\boldsymbol{ \Sigma}_1\right)
    %
    -\sum_{i=2}^\ell\frac{1}{2\sigma_i^2} \left(\psi_{i}
    %
    - \text{tr}\left({\boldsymbol \Phi}_{i}\mathbf{K}_{\mathbf{ u}_{i}
\mathbf{ u}_{i}}^{-1}\right)\right) \nonumber \\
    %
    &amp; - \sum_{i=1}^{\ell}\text{KL}\left( q(\mathbf{
u}_i)\,\|\,p(\mathbf{ u}_i) \right) \nonumber \\
    %
    &amp; -
\sum_{i=2}^{\ell}\frac{1}{2\sigma^2_{i}}\text{tr}\left(({\boldsymbol
        \Phi}_i - {\boldsymbol \Psi}_i^\top{\boldsymbol \Psi}_i)
      \mathbf{K}_{\mathbf{ u}_{i}
        \mathbf{ u}_{i}}^{-1}
      \left\langle\mathbf{ u}_{i}\mathbf{
u}_{i}^\top\right\rangle_{q(\mathbf{ u}_{i})}\mathbf{K}_{\mathbf{
u}_{i}\mathbf{ u}_{i}}^{-1}\right) \nonumber \\
    %
    &amp; + {\only&lt;2&gt;{\color{red}}\log \mathcal{N}\left(\mathbf{
y}|{\boldsymbol
        \Psi}_{\ell}\mathbf{K}_{\mathbf{ u}_{\ell}
        \mathbf{ u}_{\ell}}^{-1}{\mathbf
        m}_\ell,\sigma^2_\ell\mathbf{I}\right)}
    \label{eq:deep_bound}
  \end{align}\]</span></p>
<p><span class="math display">\[{\only&lt;1&gt;{\color{red}}\log
\mathcal{N}\left(\mathbf{ y}|{\only&lt;2-&gt;{\color{blue}}{\boldsymbol
          \Psi}_{\ell}}\mathbf{K}_{\mathbf{ u}_{\ell}
          \mathbf{ u}_{\ell}}^{-1}{\mathbf
          m}_\ell,\sigma^2_\ell\mathbf{I}\right)}\]</span> where </p>
<p>For Gaussian likelihoods: </p>
<p>Define: <span class="math display">\[q_{i, i} =
\text{var}_{p(f_i|\mathbf{ u})}\left( f_i \right) = \left\langle
f_i^2\right\rangle_{p(f_i|\mathbf{ u})} - \left\langle
f_i\right\rangle_{p(f_i|\mathbf{ u})}^2\]</span> We can write: <span
class="math display">\[c_i =
\exp\left(-{\frac{q_{i,i}}{2\sigma^2}}\right)\]</span> If joint
distribution of <span class="math inline">\(p(\mathbf{ f}, \mathbf{
u})\)</span> is Gaussian then: <span class="math display">\[q_{i, i} =
k_{i, i} - \mathbf{ k}_{i, \mathbf{ u}}^\top \mathbf{K}_{\mathbf{ u},
\mathbf{ u}}^{-1} \mathbf{ k}_{i, \mathbf{ u}}\]</span></p>
<p><span class="math inline">\(c_i\)</span> is not a function of <span
class="math inline">\(\mathbf{ u}\)</span> but <em>is</em> a function of
<span class="math inline">\(\mathbf{X}_\mathbf{ u}\)</span>.</p>
<p>Substitute variational bound into marginal likelihood: <span
class="math display">\[p(\mathbf{ y})\geq \prod_{i=1}^nc_i \int
\mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle,\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{
u}\]</span> Note that: <span class="math display">\[\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u})} = \mathbf{K}_{\mathbf{ f},
\mathbf{ u}} \mathbf{K}_{\mathbf{ u}, \mathbf{ u}}^{-1}\mathbf{
u}\]</span> is <em>linearly</em> dependent on <span
class="math inline">\(\mathbf{ u}\)</span>.</p>
<p>Making the marginalization of <span class="math inline">\(\mathbf{
u}\)</span> straightforward. In the Gaussian case: <span
class="math display">\[p(\mathbf{ u}) = \mathcal{N}\left(\mathbf{
u}|\mathbf{0},\mathbf{K}_{\mathbf{ u},\mathbf{ u}}\right)\]</span> </p>
<ul>
<li><p>Thang and Turner paper</p></li>
<li><p>Joint Gaussianity is analytic, but not flexible.</p></li>
</ul>
<h2 id="a-simple-regression-problem">A Simple Regression Problem</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/inducing-variables-demo.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/inducing-variables-demo.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Here we set up a simple one dimensional regression problem. The input
locations, <span class="math inline">\(\mathbf{X}\)</span>, are in two
separate clusters. The response variable, <span
class="math inline">\(\mathbf{ y}\)</span>, is sampled from a Gaussian
process with an exponentiated quadratic covariance.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> GPy</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">101</span>)</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>noise_var <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.zeros((<span class="dv">50</span>, <span class="dv">1</span>))</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>X[:<span class="dv">25</span>, :] <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">25</span>)[:,<span class="va">None</span>] <span class="co"># First cluster of inputs/covariates</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>X[<span class="dv">25</span>:, :] <span class="op">=</span> np.linspace(<span class="dv">7</span>,<span class="dv">10</span>,<span class="dv">25</span>)[:,<span class="va">None</span>] <span class="co"># Second cluster of inputs/covariates</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample response variables from a Gaussian process with exponentiated quadratic covariance.</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.multivariate_normal(np.zeros(N),k.K(X)<span class="op">+</span>np.eye(N)<span class="op">*</span>np.sqrt(noise_var)).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span></code></pre></div>
<p>First we perform a full Gaussian process regression on the data. We
create a GP model, <code>m_full</code>, and fit it to the data, plotting
the resulting fit.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>m_full <span class="op">=</span> GPy.models.GPRegression(X,y)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_full.optimize(messages<span class="op">=</span><span class="va">True</span>) <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<div class="figure">
<div id="sparse-demo-full-gp-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/sparse-demo-full-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="sparse-demo-full-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;sparse-demo-full-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sparse-demo-full-gp-caption" class="caption-frame">
<p>Figure: Full Gaussian process fitted to the data set.</p>
</div>
</div>
<p>Now we set up the inducing variables, <span
class="math inline">\(\mathbf{u}\)</span>. Each inducing variable has
its own associated input index, <span
class="math inline">\(\mathbf{Z}\)</span>, which lives in the same space
as <span class="math inline">\(\mathbf{X}\)</span>. Here we are using
the true covariance function parameters to generate the fit.</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.hstack(</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        (np.linspace(<span class="fl">2.5</span>,<span class="fl">4.</span>,<span class="dv">3</span>),</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        np.linspace(<span class="dv">7</span>,<span class="fl">8.5</span>,<span class="dv">3</span>)))[:,<span class="va">None</span>]</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> GPy.models.SparseGPRegression(X,y,kernel<span class="op">=</span>kern,Z<span class="op">=</span>Z)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>m.noise_var <span class="op">=</span> noise_var</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>m.inducing_inputs.constrain_fixed()</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>display(m)</span></code></pre></div>
<div class="figure">
<div id="sparse-demo-constrained-inducing-6-unlearned-gp-figure"
class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/sparse-demo-constrained-inducing-6-unlearned-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="sparse-demo-constrained-inducing-6-unlearned-gp-magnify"
class="magnify"
onclick="magnifyFigure(&#39;sparse-demo-constrained-inducing-6-unlearned-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sparse-demo-constrained-inducing-6-unlearned-gp-caption"
class="caption-frame">
<p>Figure: Sparse Gaussian process fitted with six inducing variables,
no optimization of parameters or inducing variables.</p>
</div>
</div>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m.optimize(messages<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>display(m)</span></code></pre></div>
<div class="figure">
<div id="sparse-demo-constrained-inducing-6-learned-gp-figure"
class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/sparse-demo-constrained-inducing-6-learned-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="sparse-demo-constrained-inducing-6-learned-gp-magnify"
class="magnify"
onclick="magnifyFigure(&#39;sparse-demo-constrained-inducing-6-learned-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sparse-demo-constrained-inducing-6-learned-gp-caption"
class="caption-frame">
<p>Figure: Gaussian process fitted with inducing variables fixed and
parameters optimized</p>
</div>
</div>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>m.randomize()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>m.inducing_inputs.unconstrain()</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m.optimize(messages<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="figure">
<div id="sparse-demo-unconstrained-inducing-6-gp-figure"
class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/sparse-demo-unconstrained-inducing-6-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="sparse-demo-unconstrained-inducing-6-gp-magnify"
class="magnify"
onclick="magnifyFigure(&#39;sparse-demo-unconstrained-inducing-6-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sparse-demo-unconstrained-inducing-6-gp-caption"
class="caption-frame">
<p>Figure: Gaussian process fitted with location of inducing variables
and parameters both optimized</p>
</div>
</div>
<p>Now we will vary the number of inducing points used to form the
approximation.</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>m.num_inducing<span class="op">=</span><span class="dv">8</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>m.randomize()</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>m.set_Z(np.random.rand(M,<span class="dv">1</span>)<span class="op">*</span><span class="dv">12</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m.optimize(messages<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="figure">
<div id="sparse-demo-sparse-inducing-8-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/sparse-demo-sparse-inducing-8-gp.svg" width="80%" style=" ">
</object>
<object class="svgplot " data="https://mlatcl.github.io/gpss/./slides/diagrams//gp/sparse-demo-full-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="sparse-demo-sparse-inducing-8-magnify" class="magnify"
onclick="magnifyFigure(&#39;sparse-demo-sparse-inducing-8&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sparse-demo-sparse-inducing-8-caption" class="caption-frame">
<p>Figure: Comparison of the full Gaussian process fit with a sparse
Gaussian process using eight inducing varibles. Both inducing variables
and parameters are optimized.</p>
</div>
</div>
<p>And we can compare the probability of the result to the full
model.</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(m.log_likelihood(), m_full.log_likelihood())</span></code></pre></div>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to
check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></li>
<li>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></li>
<li>blog: <a
href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography">
<div id="ref-Alvarez:efficient10" class="csl-entry"
role="doc-biblioentry">
Álvarez, M.A., Luengo, D., Titsias, M.K., Lawrence, N.D., 2010.
Efficient multioutput <span>G</span>aussian processes through
variational inducing kernels. pp. 25–32.
</div>
<div id="ref-Thang:unifying17" class="csl-entry" role="doc-biblioentry">
Bui, T.D., Yan, J., Turner, R.E., 2017. <a
href="http://jmlr.org/papers/v18/16-603.html">A unifying framework for
<span>G</span>aussian process pseudo-point approximations using power
expectation propagation</a>. Journal of Machine Learning Research 18,
1–72.
</div>
<div id="ref-Csato:thesis02" class="csl-entry" role="doc-biblioentry">
Csató, L., 2002. Gaussian processes — iterative sparse approximations
(PhD thesis). Aston University.
</div>
<div id="ref-Csato:sparse02" class="csl-entry" role="doc-biblioentry">
Csató, L., Opper, M., 2002. Sparse on-line <span>G</span>aussian
processes. Neural Computation 14, 641–668.
</div>
<div id="ref-Dai:gpu14" class="csl-entry" role="doc-biblioentry">
Dai, Z., Damianou, A., Hensman, J., Lawrence, N.D., 2014. Gaussian
process models with parallelization and <span>GPU</span> acceleration.
</div>
<div id="ref-Damianou:thesis2015" class="csl-entry"
role="doc-biblioentry">
Damianou, A., 2015. Deep <span>G</span>aussian processes and variational
propagation of uncertainty (PhD thesis). University of Sheffield.
</div>
<div id="ref-Damianou:deepgp13" class="csl-entry"
role="doc-biblioentry">
Damianou, A., Lawrence, N.D., 2013. Deep <span>G</span>aussian
processes. pp. 207–215.
</div>
<div id="ref-Damianou:variational15" class="csl-entry"
role="doc-biblioentry">
Damianou, A., Titsias, M.K., Lawrence, N.D., 2016. Variational inference
for latent variables and uncertain inputs in <span>G</span>aussian
processes. Journal of Machine Learning Research 17.
</div>
<div id="ref-Gal:distributed14" class="csl-entry"
role="doc-biblioentry">
Gal, Y., Wilk, M. van der, Rasmussen, C.E., n.d. Distributed variational
inference in sparse <span>G</span>aussian process regression and latent
variable models.
</div>
<div id="ref-Hensman:bigdata13" class="csl-entry"
role="doc-biblioentry">
Hensman, J., Fusi, N., Lawrence, N.D., n.d. <span>G</span>aussian
processes for big data.
</div>
<div id="ref-Hensman:nested14" class="csl-entry" role="doc-biblioentry">
Hensman, J., Lawrence, N.D., 2014. Nested variational compression in
deep <span>G</span>aussian processes. University of Sheffield.
</div>
<div id="ref-Hensman:fast12" class="csl-entry" role="doc-biblioentry">
Hensman, J., Rattray, M., Lawrence, N.D., 2012. Fast variational
inference in the conjugate exponential family.
</div>
<div id="ref-Hoffman:stochastic12" class="csl-entry"
role="doc-biblioentry">
Hoffman, M., Blei, D.M., Wang, C., Paisley, J., 2012. Stochastic
variational inference, arXiv preprint arXiv:1206.7051.
</div>
<div id="ref-King:klcorrection06" class="csl-entry"
role="doc-biblioentry">
King, N.J., Lawrence, N.D., n.d. Fast variational inference for
<span>G</span>aussian <span>P</span>rocess models through
<span>KL</span>-correction. pp. 270–281.
</div>
<div id="ref-Lawrence:larger07" class="csl-entry"
role="doc-biblioentry">
Lawrence, N.D., n.d. Learning for larger datasets with the
<span>G</span>aussian process latent variable model. pp. 243–250.
</div>
<div id="ref-Lawrence:ivm02" class="csl-entry" role="doc-biblioentry">
Lawrence, N.D., Seeger, M., Herbrich, R., n.d. Fast sparse
<span>G</span>aussian process methods: The informative vector machine.
pp. 625–632.
</div>
<div id="ref-Lazaro:spectrum10" class="csl-entry"
role="doc-biblioentry">
Lázaro-Gredilla, M., Quiñonero-Candela, J., Rasmussen, C.E., 2010.
Sparse spectrum gaussian processes. Journal of Machine Learning Research
11, 1865–1881.
</div>
<div id="ref-Quinonero:unifying05" class="csl-entry"
role="doc-biblioentry">
Quiñonero Candela, J., Rasmussen, C.E., 2005. A unifying view of sparse
approximate <span>G</span>aussian process regression. Journal of Machine
Learning Research 6, 1939–1959.
</div>
<div id="ref-Salimbeni:doubly2017" class="csl-entry"
role="doc-biblioentry">
Salimbeni, H., Deisenroth, M., 2017. <a
href="http://papers.nips.cc/paper/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.pdf">Doubly
stochastic variational inference for deep <span>G</span>aussian
processes</a>, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H.,
Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural
Information Processing Systems 30. Curran Associates, Inc., pp.
4591–4602.
</div>
<div id="ref-Seeger:auto17" class="csl-entry" role="doc-biblioentry">
Seeger, M.W., Hetzel, A., Dai, Z., Lawrence, N.D., 2017. <a
href="http://arxiv.org/abs/1710.08717">Auto-differentiating linear
algebra</a>. CoRR abs/1710.08717.
</div>
<div id="ref-Seeger:fast03" class="csl-entry" role="doc-biblioentry">
Seeger, M., Williams, C.K.I., Lawrence, N.D., n.d. Fast forward
selection to speed up sparse <span>G</span>aussian process regression.
</div>
<div id="ref-Smola:sparsegp00" class="csl-entry" role="doc-biblioentry">
Smola, A.J., Bartlett, P.L., n.d. Sparse greedy <span>G</span>aussian
process regression. pp. 619–625.
</div>
<div id="ref-Snelson:pseudo05" class="csl-entry" role="doc-biblioentry">
Snelson, E., Ghahramani, Z., n.d. Sparse <span>G</span>aussian processes
using pseudo-inputs.
</div>
<div id="ref-Titsias:variational09" class="csl-entry"
role="doc-biblioentry">
Titsias, M.K., n.d. Variational learning of inducing variables in sparse
<span>G</span>aussian processes. pp. 567–574.
</div>
<div id="ref-Williams:nystrom00" class="csl-entry"
role="doc-biblioentry">
Williams, C.K.I., Seeger, M., n.d. Using the <span>N</span>yström method
to speed up kernel machines. pp. 682–688.
</div>
</div>

