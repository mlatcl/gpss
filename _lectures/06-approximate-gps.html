---
title: "Approximate Gaussian Processes"
venue: "Gaussian Process Summer School"
abstract: "null"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: University of Cambridge
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orcid: 0000-0001-9258-1030
time: "null"
week: 0
session: 6
reveal: 06-approximate-gps.slides.html
ipynb: 06-approximate-gps.ipynb
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<div class="figure">
<div id="sparse-gp-comic-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/sparse-gps.jpg" width="45%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="sparse-gp-comic-magnify" class="magnify" onclick="magnifyFigure(&#39;sparse-gp-comic&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sparse-gp-comic-caption" class="caption-frame">
<p>Figure: In recent years, approximations for Gaussian process models haven’t been the most fashionable approach to machine learning. Image credit: Kai Arulkumaran</p>
</div>
</div>
<p>Inference in a Gaussian process has computational complexity of <span class="math inline">$\bigO(\numData^3)$</span> and storage demands of <span class="math inline">$\bigO(\numData^2)$</span>. This is too large for many modern data sets.</p>
<p>Low rank approximations allow us to work with Gaussian processes with computational complexity of <span class="math inline">$\bigO(\numData\numInducing^2)$</span> and storage demands of <span class="math inline">$\bigO(\numData\numInducing)$</span>, where <span class="math inline">$\numInducing$</span> is a user chosen parameter.</p>
<p>In machine learning, low rank approximations date back to <span class="citation" data-cites="Smola:sparsegp00">Smola and Bartlett (n.d.)</span>, <span class="citation" data-cites="Williams:nystrom00">Williams and Seeger (n.d.)</span>, who considered the Nyström approximation and <span class="citation" data-cites="Csato:sparse02">Csató and Opper (2002)</span>;<span class="citation" data-cites="Csato:thesis02">Csató (2002)</span> who considered low rank approximations in the context of on-line learning. Selection of active points for the approximation was considered by <span class="citation" data-cites="Seeger:fast03">Seeger, Williams, and Lawrence (n.d.)</span> and <span class="citation" data-cites="Snelson:pseudo05">Snelson and Ghahramani (n.d.)</span> first proposed that the active set could be optimized directly. Those approaches were reviewed by <span class="citation" data-cites="Quinonero:unifying05">Quiñonero Candela and Rasmussen (2005)</span> under a unifying likelihood approximation perspective. General rules for deriving the maximum likelihood for these sparse approximations were given in <span class="citation" data-cites="Lawrence:larger07">Lawrence (n.d.)</span>.</p>
<p>Modern variational interpretations of these low rank approaches were first explored in <span class="citation" data-cites="Titsias:variational09">Titsias (n.d.)</span>. A more modern summary which considers each of these approximations as an <span class="math inline"><em>α</em></span>-divergence is given by <span class="citation" data-cites="Thang:unifying17">Bui, Yan, and Turner (2017)</span>.</p>
<h2 id="variational-compression">Variational Compression</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-variational-complexity.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-variational-complexity.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Inducing variables are a compression of the real observations. The basic idea is can I create a new data set that summarizes all the information in the original data set. If this data set is smaller, I’ve compressed the information in the original data set.</p>
<p>Inducing variables can be thought of as pseudo-data, indeed in <span class="citation" data-cites="Snelson:pseudo05">Snelson and Ghahramani (n.d.)</span> they were referred to as <em>pseudo-points</em>.</p>
<p>The only requirement for inducing variables is that they are jointly distributed as a Gaussian process with the original data. This means that they can be from the space <span class="math inline">$\mappingFunctionVector$</span> or a space that is related through a linear operator (see e.g. <span class="citation" data-cites="Alvarez:efficient10">Álvarez et al. (2010)</span>). For example we could choose to store the gradient of the function at particular points or a value from the frequency spectrum of the function <span class="citation" data-cites="Lazaro:spectrum10">(Lázaro-Gredilla, Quiñonero-Candela, and Rasmussen 2010)</span>.</p>
<h2 id="variational-compression-ii">Variational Compression II</h2>
<p>Inducing variables don’t only allow for the compression of the non-parameteric information into a reduced data aset but they also allow for computational scaling of the algorithms through, for example stochastic variational approaches <span class="citation" data-cites="Hensman:bigdata13">Hensman, Fusi, and Lawrence (n.d.)</span> or parallelization <span class="citation" data-cites="Gal:distributed14">Gal, Wilk, and Rasmussen (n.d.)</span>,<span class="citation" data-cites="Dai:gpu14">Dai et al. (2014)</span>, <span class="citation" data-cites="Seeger:auto17">Seeger et al. (2017)</span>.</p>
<p>We’ve seen how we go from parametric to non-parametric. The limit implies infinite dimensional <span class="math inline">$\mappingVector$</span>. Gaussian processes are generally non-parametric: combine data with covariance function to get model. This representation <em>cannot</em> be summarized by a parameter vector of a fixed size.</p>
<p>Parametric models have a representation that does not respond to increasing training set size. Bayesian posterior distributions over parameters contain the information about the training data, for example if we use use Bayes’ rule from training data, <br /><span class="math display">$$
p\left(\mappingVector|\dataVector, \inputMatrix\right),
$$</span><br /> to make predictions on test data <br /><span class="math display">$$
p\left(\dataScalar_*|\inputMatrix_*, \dataVector, \inputMatrix\right) = \int
              p\left(\dataScalar_*|\mappingVector,\inputMatrix_*\right)p\left(\mappingVector|\dataVector,
                \inputMatrix)\text{d}\mappingVector\right)
$$</span><br /> then <span class="math inline">$\mappingVector$</span> becomes a bottleneck for information about the training set to pass to the test set. The solution is to increase <span class="math inline">$\numBasisFunc$</span> so that the bottleneck is so large that it no longer presents a problem. How big is big enough for <span class="math inline">$\numBasisFunc$</span>? Non-parametrics says <span class="math inline">$\numBasisFunc \rightarrow \infty$</span>.</p>
<p>Now no longer possible to manipulate the model through the standard parametric form. However, it <em>is</em> possible to express <em>parametric</em> as GPs: <br /><span class="math display">$$
\kernelScalar\left(\inputVector_i,\inputVector_j\right)=\basisFunction_:\left(\inputVector_i\right)^\top\basisFunction_:\left(\inputVector_j\right).
$$</span><br /> These are known as degenerate covariance matrices. Their rank is at most <span class="math inline">$\numBasisFunc$</span>, non-parametric models have full rank covariance matrices. Most well known is the “linear kernel”, <br /><span class="math display">$$
\kernelScalar(\inputVector_i, \inputVector_j) = \inputVector_i^\top\inputVector_j.
$$</span><br /> For non-parametrics prediction at a new point, <span class="math inline">$\mappingFunctionVector_*$</span>, is made by conditioning on <span class="math inline">$\mappingFunctionVector$</span> in the joint distribution. In GPs this involves combining the training data with the covariance function and the mean function. Parametric is a special case when conditional prediction can be summarized in a <em>fixed</em> number of parameters. Complexity of parametric model remains fixed regardless of the size of our training data set. For a non-parametric model the required number of parameters grows with the size of the training data.</p>
<ul>
<li>Everything we want to do with a GP involves marginalising <span class="math inline">$\mappingFunctionVector$</span>
<ul>
<li>Predictions</li>
<li>Marginal likelihood</li>
<li>Estimating covariance parameters</li>
</ul></li>
<li>The posterior of <span class="math inline">$\mappingFunctionVector$</span> is the central object. This means inverting <span class="math inline">$\Kff$</span>.</li>
</ul>
<img class="negate" src="../slides/diagrams/cov_approx.png" width="90%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
<div class="caption" style="">
Figure: Figure originally from presentation by Ed Snelson at NIPS
</div>
<p>The Nystr"om approximation takes the form, <br /><span class="math display">$$
\Kff \approx \Qff = \Kfu \Kuu^{-1}\Kuf
$$</span><br /> The idea is that instead of inverting <span class="math inline">$\Kff$</span>, we make a low rank (or Nyström) approximation, and invert <span class="math inline">$\Kuu$</span> instead.</p>
<p>In the original Nystr"om method the columns to incorporate are sampled from the complete set of columns (without replacement). In a kernel matrix each of these columns corresponds to a data point. In the Nystr"om method these points are sometimes called <em>landmark</em> points.</p>
<table>
<tr>
<td width="30%">
<br /><span class="math display">$$\inputMatrix,\,\dataVector$$</span><br />
</td>
<td width="70%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/nomenclature1.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
<table>
<tr>
<td width="30%">
<br /><span class="math display">$$\inputMatrix,\,\dataVector$$</span><br /> <br /><span class="math display">$${\color{blue} \mappingFunction(\inputVector)} \sim {\mathcal GP}$$</span><br />
</td>
<td width="70%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/nomenclature2.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
<table>
<tr>
<td width="30%">
<br /><span class="math display">$$\inputMatrix,\,\dataVector$$</span><br /> <br /><span class="math display">$$\mappingFunction(\inputVector) \sim {\mathcal GP}$$</span><br /><br /><span class="math display">$$p({\color{blue} \mappingFunctionVector}) = \gaussianSamp{\zerosVector}{\Kff}$$</span><br />
</td>
<td width="70%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/nomenclature3.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
<table>
<tr>
<td width="30%">
<br /><span class="math display">$$
\inputMatrix,\,\dataVector$$</span><br /> <br /><span class="math display">$$\mappingFunction(\inputVector) \sim {\mathcal GP}
$$</span><br /> <br /><span class="math display">$$
p(\mappingFunctionVector) = \gaussianSamp{\zerosVector}{\Kff}
$$</span><br /> <br /><span class="math display">$$p( \mappingFunctionVector \given \dataVector,\inputMatrix)
$$</span><br />
</td>
<td width="70%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/nomenclature3a.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
<p>Take an extra <span class="math inline">$\numInducing$</span> points on the function, <span class="math inline">$\inducingVector = \mappingFunction(\inducingInputMatrix)$</span>. <br /><span class="math display">$$p(\dataVector,\mappingFunctionVector,\inducingVector) = p(\dataVector\given \mappingFunctionVector) p(\mappingFunctionVector\given \inducingVector) p(\inducingVector)$$</span><br /></p>
<p><img class="negate" src="../slides/diagrams/cov_inducing_withX.png" width="60%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></p>
<p>Take and extra <span class="math inline"><em>M</em></span> points on the function, <span class="math inline">$\inducingVector = \mappingFunction(\inducingInputMatrix)$</span>. <br /><span class="math display">$$p(\dataVector,\mappingFunctionVector,\inducingVector) = p(\dataVector\given \mappingFunctionVector) p(\mappingFunctionVector\given \inducingVector) p(\inducingVector)$$</span><br /> <br /><span class="math display">$$\begin{aligned}
    p(\dataVector\given\mappingFunctionVector) &amp;= \gaussianDist{\dataVector}{\mappingFunctionVector}{\dataStd^2 \eye}\\
    p(\mappingFunctionVector\given\inducingVector) &amp;= \gaussianDist{\mappingFunctionVector}{ \Kfu\Kuu^{-1}\inducingVector}{ \tilde{\kernelMatrix}}\\
    p(\inducingVector) &amp;= \gaussianDist{\inducingVector}{ \zerosVector}{\Kuu}
  \end{aligned}$$</span><br /></p>
<table>
<tr>
<td width="30%">
<br /><span class="math display">$$\inputMatrix,\,\dataVector$$</span><br /> <br /><span class="math display">$$\mappingFunction(\inputVector) \sim {\mathcal GP}$$</span><br /> <br /><span class="math display">$$p(\mappingFunctionVector) = \gaussianSamp{\zerosVector}{\Kff}$$</span><br /> <br /><span class="math display">$$p(\mappingFunctionVector\given \dataVector,\inputMatrix)$$</span><br />
</td>
<td width="70%">
<img class="negate" src="../slides/diagrams/nomenclature4" width="90%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</td>
</tr>
</table>
<p><br /><span class="math display">$$
\begin{align}
                           &amp;\qquad\inducingInputMatrix, \inducingVector\\                      &amp;p({\color{red} \inducingVector})  = \gaussianSamp{\zerosVector}{\Kuu}\end{align}
$$</span><br /></p>
<table>
<tr>
<td width="30%">
<br /><span class="math display">$$\inputMatrix,\,\dataVector$$</span><br /> <br /><span class="math display">$$\mappingFunction(\inputVector) \sim {\mathcal GP}$$</span><br /> <br /><span class="math display">$$p(\mappingFunctionVector) = \gaussianSamp{\zerosVector}{\Kff}$$</span><br /> <br /><span class="math display">$$p(\mappingFunctionVector\given \dataVector,\inputMatrix)$$</span><br /> <br /><span class="math display">$$p(\inducingVector)  = \gaussianSamp{\zerosVector}{\Kuu}$$</span><br /> <br /><span class="math display">$$\widetilde p({\color{red}\inducingVector}\given \dataVector,\inputMatrix)$$</span><br />
</td>
<td width="70%">
<img class="negate" src="../slides/diagrams/nomenclature5.png" width="90%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</td>
</tr>
</table>
<p><span>Instead of doing</span> <br /><span class="math display">$$
p(\mappingFunctionVector\given\dataVector,\inputMatrix) = \frac{p(\dataVector\given\mappingFunctionVector)p(\mappingFunctionVector\given\inputMatrix)}{\int p(\dataVector\given\mappingFunctionVector)p(\mappingFunctionVector\given\inputMatrix){\text{d}\mappingFunctionVector}}
$$</span><br /> <span>We’ll do</span> <br /><span class="math display">$$
p(\inducingVector\given\dataVector,\inducingInputMatrix) = \frac{p(\dataVector\given\inducingVector)p(\inducingVector\given\inducingInputMatrix)}{\int p(\dataVector\given\inducingVector)p(\inducingVector\given\inducingInputMatrix){\text{d}\inducingVector}}
$$</span><br /> </p>
<!--Flexible Parametric Approximation-->
<ul>
<li>Date back to {<span class="citation" data-cites="Williams:nystrom00">Williams and Seeger (n.d.)</span>; <span class="citation" data-cites="Smola:sparsegp00">Smola and Bartlett (n.d.)</span>; <span class="citation" data-cites="Csato:sparse02">Csató and Opper (2002)</span>; <span class="citation" data-cites="Seeger:fast03">Seeger, Williams, and Lawrence (n.d.)</span>; <span class="citation" data-cites="Snelson:pseudo05">Snelson and Ghahramani (n.d.)</span>}. See {<span class="citation" data-cites="Quinonero:unifying05">Quiñonero Candela and Rasmussen (2005)</span>; <span class="citation" data-cites="Thang:unifying17">Bui, Yan, and Turner (2017)</span>} for reviews.</li>
<li>We follow variational perspective of {<span class="citation" data-cites="Titsias:variational09">Titsias (n.d.)</span>}.</li>
<li>This is an augmented variable method, followed by a collapsed variational approximation {<span class="citation" data-cites="King:klcorrection06">King and Lawrence (n.d.)</span>; <span class="citation" data-cites="Hensman:fast12">Hensman, Rattray, and Lawrence (2012)</span>}.</li>
</ul>
<table>
<tr>
<td width="60%">
</td>
<td width="40%">
</td>
</tr>
</table>
<h2 id="variational-bound-on-pdatavector-inducingvector">Variational Bound on <span class="math inline">$p(\dataVector |\inducingVector)$</span></h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/larger-variational.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/larger-variational.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The conditional density of the data given the inducing points can be <em>lower</em> bounded variationally <br /><span class="math display">$$
\begin{aligned}
    \log p(\dataVector|\inducingVector) &amp; = \log \int p(\dataVector|\mappingFunctionVector) p(\mappingFunctionVector|\inducingVector) \text{d}\mappingFunctionVector\\ &amp; = \int q(\mappingFunctionVector) \log \frac{p(\dataVector|\mappingFunctionVector) p(\mappingFunctionVector|\inducingVector)}{q(\mappingFunctionVector)}\text{d}\mappingFunctionVector + \KL{q(\mappingFunctionVector)}{p(\mappingFunctionVector|\dataVector, \inducingVector)}.
\end{aligned}
$$</span><br /></p>
<p>The key innovation from <span class="citation" data-cites="Titsias:variational09">Titsias (n.d.)</span> was to then make a particular choice for <span class="math inline">$q(\mappingFunctionVector)$</span>. If we set <span class="math inline">$q(\mappingFunctionVector)=p(\mappingFunctionVector|\inducingVector)$</span>, <br /><span class="math display">$$
  \log p(\dataVector|\inducingVector) \geq \int p(\mappingFunctionVector|\inducingVector) \log p(\dataVector|\mappingFunctionVector)\text{d}\mappingFunctionVector.
  $$</span><br /> <br /><span class="math display">$$
  p(\dataVector|\inducingVector) \geq \exp \int p(\mappingFunctionVector|\inducingVector) \log p(\dataVector|\mappingFunctionVector)\text{d}\mappingFunctionVector.
  $$</span><br /></p>
<h2 id="optimal-compression-in-inducing-variables">Optimal Compression in Inducing Variables</h2>
<p>Maximizing the lower bound minimizes the Kullback-Leibler divergence (or <em>information gain</em>) between our approximating density, <span class="math inline">$p(\mappingFunctionVector|\inducingVector)$</span> and the true posterior density, <span class="math inline">$p(\mappingFunctionVector|\dataVector, \inducingVector)$</span>.</p>
<p><br /><span class="math display">$$
  \KL{p(\mappingFunctionVector|\inducingVector)}{p(\mappingFunctionVector|\dataVector, \inducingVector)} = \int p(\mappingFunctionVector|\inducingVector) \log \frac{p(\mappingFunctionVector|\inducingVector)}{p(\mappingFunctionVector|\dataVector, \inducingVector)}\text{d}\inducingVector
  $$</span><br /></p>
<p>This bound is minimized when the information stored about <span class="math inline">$\dataVector$</span> is already stored in <span class="math inline">$\inducingVector$</span>. In other words, maximizing the bound seeks an <em>optimal compression</em> from the <em>information gain</em> perspective.</p>
<p>For the case where <span class="math inline">$\inducingVector = \mappingFunctionVector$</span> the bound is exact (<span class="math inline">$\mappingFunctionVector$</span> <span class="math inline"><em>d</em></span>-separates <span class="math inline">$\dataVector$</span> from <span class="math inline">$\inducingVector$</span>).</p>
<h2 id="choice-of-inducing-variables">Choice of Inducing Variables</h2>
<p>The quality of the resulting bound is determined by the choice of the inducing variables. You are free to choose whichever heuristics you like for the inducing variables, as long as they are drawn jointly from a valid Gaussian process, i.e. such that <br /><span class="math display">$$
\begin{bmatrix}
\mappingFunctionVector\\
\inducingVector
\end{bmatrix} \sim \gaussianSamp{\zerosVector}{\kernelMatrix}
$$</span><br /> where the kernel matrix itself can be decomposed into <br /><span class="math display">$$
\kernelMatrix =
\begin{bmatrix}
\Kff &amp; \Kfu \\
\Kuf &amp; \Kuu
\end{bmatrix}
$$</span><br /> Choosing the inducing variables amounts to specifying <span class="math inline">$\Kfu$</span> and <span class="math inline">$\Kuu$</span> such that <span class="math inline">$\kernelMatrix$</span> remains positive definite. The typical choice is to choose <span class="math inline">$\inducingVector$</span> in the same domain as <span class="math inline">$\mappingFunctionVector$</span>, associating each inducing output, <span class="math inline">$\inducingScalar_i$</span> with a corresponding input location <span class="math inline">$\inducingInputVector$</span>. However, more imaginative choices are absolutely possible. In particular, if <span class="math inline">$\inducingVector$</span> is related to <span class="math inline">$\mappingFunctionVector$</span> through a linear operator (see e.g. <span class="citation" data-cites="Alvarez:efficient10">Álvarez et al. (2010)</span>), then valid <span class="math inline">$\Kuu$</span> and <span class="math inline">$\Kuf$</span> can be constructed. For example we could choose to store the gradient of the function at particular points or a value from the frequency spectrum of the function <span class="citation" data-cites="Lazaro:spectrum10">(Lázaro-Gredilla, Quiñonero-Candela, and Rasmussen 2010)</span>.</p>
<h2 id="variational-compression-ii-1">Variational Compression II</h2>
<p>Inducing variables don’t only allow for the compression of the non-parameteric information into a reduced data set but they also allow for computational scaling of the algorithms through, for example stochastic variational approaches<span class="citation" data-cites="Hoffman:stochastic12 Hensman:bigdata13">(Hoffman et al. 2012; Hensman, Fusi, and Lawrence, n.d.)</span> or parallelization <span class="citation" data-cites="Gal:distributed14 Dai:gpu14 Seeger:auto17">(Gal, Wilk, and Rasmussen, n.d.; Dai et al. 2014; Seeger et al. 2017)</span>.</p>
<ul>
<li><p>If the likelihood, <span class="math inline">$p(\dataVector|\mappingFunctionVector)$</span>, factorizes     </p></li>
<li><p>&lt;8-&gt; Then the bound factorizes.</p></li>
<li><p>&lt;10-&gt; Now need a choice of distributions for <span class="math inline">$\mappingFunctionVector$</span> and <span class="math inline">$\dataVector|\mappingFunctionVector$</span> …</p></li>
<li><p>Choose to go a different way.</p></li>
<li><p>Introduce a set of auxiliary variables, <span class="math inline">$\inducingVector$</span>, which are <span class="math inline"><em>m</em></span> in length.</p></li>
<li><p>They are like “artificial data”.</p></li>
<li><p>Used to <em>induce</em> a distribution: <span class="math inline">$q(\inducingVector|\dataVector)$</span></p></li>
<li><p>Introduce variable set which is <em>finite</em> dimensional. <br /><span class="math display">$$
p(\dataVector^*|\dataVector) \approx \int p(\dataVector^*|\inducingVector) q(\inducingVector|\dataVector) \text{d}\inducingVector
$$</span><br /></p></li>
<li><p>But dimensionality of <span class="math inline">$\inducingVector$</span> can be changed to improve approximation.</p></li>
<li><p>Model for our data, <span class="math inline">$\dataVector$</span></p>
<table>
<tr>
<td width>
<p><br /><span class="math display">$$p(\dataVector)$$</span><br /></p>
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/py.svg" width style=" ">
</object>
</td>
</tr>
</table></li>
<li><p>Prior density over <span class="math inline">$\mappingFunctionVector$</span>. Likelihood relates data, <span class="math inline">$\dataVector$</span>, to <span class="math inline">$\mappingFunctionVector$</span>.</p>
<table>
<tr>
<td width>
<p><br /><span class="math display">$$p(\dataVector)=\int p(\dataVector|\mappingFunctionVector)p(\mappingFunctionVector)\text{d}\mappingFunctionVector$$</span><br /></p>
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/pygfpf.svg" width style=" ">
</object>
</td>
</tr>
</table></li>
<li><p>Prior density over <span class="math inline">$\mappingFunctionVector$</span>. Likelihood relates data, <span class="math inline">$\dataVector$</span>, to <span class="math inline">$\mappingFunctionVector$</span>.</p>
<table>
<tr>
<td width>
<p><br /><span class="math display">$$p(\dataVector)=\int p(\dataVector|\mappingFunctionVector)p(\inducingVector|\mappingFunctionVector)p(\mappingFunctionVector)\text{d}\mappingFunctionVector\text{d}\inducingVector$$</span><br /></p>
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/pygfpugfpf.svg" width style=" ">
</object>
</td>
</tr>
</table></li>
</ul>
<table>
<tr>
<td width>
<br /><span class="math display">$$p(\dataVector)=\int \int p(\dataVector|\mappingFunctionVector)p(\mappingFunctionVector|\inducingVector)\text{d}\mappingFunctionVector p(\inducingVector)\text{d}\inducingVector$$</span><br />
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/pygfpfgupu.svg" width style=" ">
</object>
</td>
</tr>
</table>
<table>
<tr>
<td width>
<br /><span class="math display">$$p(\dataVector)=\int \int p(\dataVector|\mappingFunctionVector)p(\mappingFunctionVector|\inducingVector)\text{d}\mappingFunctionVector p(\inducingVector)\text{d}\inducingVector$$</span><br />
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/pygfpfgupu2.svg" width style=" ">
</object>
</td>
</tr>
</table>
<table>
<tr>
<td width>
<br /><span class="math display">$$p(\dataVector|\inducingVector)=\int p(\dataVector|\mappingFunctionVector)p(\mappingFunctionVector|\inducingVector)\text{d}\mappingFunctionVector$$</span><br />
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/pygfpfgu.svg" width style=" ">
</object>
</td>
</tr>
</table>
<table>
<tr>
<td width>
<br /><span class="math display">$$p(\dataVector|\inducingVector)$$</span><br />
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/pygu.svg" width style=" ">
</object>
</td>
</tr>
</table>
<table>
<tr>
<td width>
<br /><span class="math display">$$p(\dataVector|\paramVector)$$</span><br />
</td>
<td width>
<object class="svgplot " data="../slides/diagrams/gp/pygtheta.svg" width style=" ">
</object>
</td>
</tr>
</table>
<ul>
<li>Replace true <span class="math inline">$p(\inducingVector|\dataVector)$</span> with approximation <span class="math inline">$q(\inducingVector|\dataVector)$</span>.</li>
<li>Minimize KL divergence between approximation and truth.</li>
<li>This is similar to the Bayesian posterior distribution.</li>
<li>But it’s placed over a set of ‘pseudo-observations’.</li>
</ul>
<p><br /><span class="math display">$$\mappingFunctionVector, \inducingVector \sim \gaussianSamp{\mathbf{0}}{\begin{bmatrix}\Kff &amp; \Kfu\\\Kuf &amp; \Kuu\end{bmatrix}}$$</span><br /> <br /><span class="math display">$$\dataVector|\mappingFunctionVector = \prod_{i} \gaussianSamp{\mappingFunction}{\dataStd^2}$$</span><br /></p>
<!--Variational Compression-->
<p>For Gaussian likelihoods:  </p>
<p>Define: <br /><span class="math display">$$q_{i, i} = \varianceDist{\mappingFunction_i}{p(\mappingFunction_i|\inducingVector)} = \expDist{\mappingFunction_i^2}{p(\mappingFunction_i|\inducingVector)} - \expDist{\mappingFunction_i}{p(\mappingFunction_i|\inducingVector)}^2$$</span><br /> We can write: <br /><span class="math display">$$c_i = \exp\left(-{\frac{q_{i,i}}{2\dataStd^2}}\right)$$</span><br /> If joint distribution of <span class="math inline">$p(\mappingFunctionVector, \inducingVector)$</span> is Gaussian then: <br /><span class="math display">$$q_{i, i} = \kernelScalar_{i, i} - \kernelVector_{i, \inducingVector}^\top \kernelMatrix_{\inducingVector, \inducingVector}^{-1} \kernelVector_{i, \inducingVector}$$</span><br /></p>
<p><span class="math inline"><em>c</em><sub><em>i</em></sub></span> is not a function of <span class="math inline">$\inducingVector$</span> but <em>is</em> a function of <span class="math inline">$\inputMatrix_\inducingVector$</span>.</p>
<!--frame end-->
<!--frame start-->
<ul>
<li><p>The sum of <span class="math inline"><em>q</em><sub><em>i</em>, <em>i</em></sub></span> is the <em>total conditional variance</em>.</p></li>
<li><p>If conditional density <span class="math inline">$p(\mappingFunctionVector|\inducingVector)$</span> is Gaussian then it has covariance <br /><span class="math display">$$\mathbf{Q} = \kernelMatrix_{\mappingFunctionVector\mappingFunctionVector} - \kernelMatrix_{\mappingFunctionVector \inducingVector}\kernelMatrix_{\inducingVector\inducingVector}^{-1} \kernelMatrix_{\inducingVector\mappingFunctionVector}$$</span><br /></p></li>
<li><p><span class="math inline">$\trace{\mathbf{Q}} = \sum_{i}q_{i,i}$</span> is known as total variance.</p></li>
<li><p>Because it is on conditional distribution we call it <em>total conditional variance</em>.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
<ul>
<li><p>Measure the ’capacity of a density’.</p></li>
<li><p>Determinant of covariance represents ’volume’ of density.</p></li>
<li><p>log determinant is entropy: sum of <em>log</em> eigenvalues of covariance.</p></li>
<li><p>trace of covariance is total variance: sum of eigenvalues of covariance.</p></li>
<li><p><span class="math inline"><em>λ</em> &gt; log <em>λ</em></span> then total conditional variance upper bounds entropy.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
<p>Exponentiated total variance bounds determinant. <br /><span class="math display">$$\det{\mathbf{Q}} &lt; \exp \trace{\mathbf{Q}}$$</span><br /> Because <br /><span class="math display">$$\prod_{i=1}^k \lambda_i &lt; \prod_{i=1}^k \exp(\lambda_i)$$</span><br /> where <span class="math inline">{<em>λ</em><sub><em>i</em></sub>}<sub><em>i</em> = 1</sub><sup><em>k</em></sup></span> are the <em>positive</em> eigenvalues of <span class="math inline"><strong>Q</strong></span> This in turn implies <br /><span class="math display">$$\det{\mathbf{Q}} &lt; \prod_{i=1}^k \exp\left(q_{i,i}\right)$$</span><br /></p>
<!--frame end-->
<!--frame start-->
<ul>
<li><p>Conditional density <span class="math inline">$p(\mappingFunctionVector|\inducingVector)$</span> can be seen as a <em>communication channel</em>.</p></li>
<li><p>Normally we have: <span> <br /><span class="math display">$$\text{Transmitter} \stackrel{\inducingVector}{\rightarrow} \begin{smallmatrix}p(\mappingFunctionVector|\inducingVector) \\ \text{Channel}\end{smallmatrix} \stackrel{\mappingFunctionVector}{\rightarrow} \text{Receiver}$$</span><br /></span> and we control <span class="math inline">$p(\inducingVector)$</span> (the source density).</p></li>
<li><p><em>Here</em> we can also control the transmission channel <span class="math inline">$p(\mappingFunctionVector|\inducingVector)$</span>.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
<p>Substitute variational bound into marginal likelihood: <br /><span class="math display">$$p(\dataVector)\geq \prod_{i=1}^\numData c_i \int \gaussianDist{\dataVector}{\expSamp{\mappingFunctionVector}}{\dataStd^2\eye}p(\inducingVector) \text{d}\inducingVector$$</span><br /> Note that: <br /><span class="math display">$$\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector)} = \kernelMatrix_{\mappingFunctionVector, \inducingVector} \kernelMatrix_{\inducingVector, \inducingVector}^{-1}\inducingVector$$</span><br /> is <em>linearly</em> dependent on <span class="math inline">$\inducingVector$</span>.</p>
<!--frame end-->
<!--frame start-->
<p>Making the marginalization of <span class="math inline">$\inducingVector$</span> straightforward. In the Gaussian case: <br /><span class="math display">$$p(\inducingVector) = \gaussianDist{\inducingVector}{\zerosVector}{\kernelMatrix_{\inducingVector,\inducingVector}}$$</span><br />      </p>
<!--frame end-->
<p><br /><span class="math display">$$\log p(\dataVector\given\inducingVector) = \log\int p(\dataVector \given\mappingFunctionVector)p(\mappingFunctionVector\given\inducingVector,\inputMatrix)\text{d}\mappingFunctionVector$$</span><br /></p>
<p><br /><span class="math display">$$\log p(\dataVector\given\inducingVector) = \log \mathbb{E}_{p(\mappingFunctionVector\given \inducingVector,\inputMatrix)}\left[p(\dataVector \given\mappingFunctionVector)\right]$$</span><br />  <br /><span class="math display">$$\log p(\dataVector\given\inducingVector) \geq  \mathbb{E}_{p(\mappingFunctionVector\given \inducingVector,\inputMatrix)}\left[\log p(\dataVector \given\mappingFunctionVector)\right]\triangleq \log\widetilde p(\dataVector\given \inducingVector)$$</span><br /> </p>
<p><span> No inversion of <span class="math inline">$\Kff$</span> required</span></p>
<p><span style="text-align:right"><span class="citation" data-cites="Titsias:variational09">Titsias (n.d.)</span></span> <br /><span class="math display">$$p(\dataVector\given\inducingVector) = \frac{p(\dataVector \given\mappingFunctionVector)p(\mappingFunctionVector\given\inducingVector)}{p(\mappingFunctionVector\given\dataVector, \inducingVector)}$$</span><br />  <br /><span class="math display">$$\log p(\dataVector\given\inducingVector) = \log p(\dataVector \given\mappingFunctionVector) + \log \frac{p(\mappingFunctionVector\given\inducingVector)}{p(\mappingFunctionVector\given\dataVector, \inducingVector)}$$</span><br />  <br /><span class="math display">$$\log p(\dataVector\given\inducingVector) = \bbE_{p(\mappingFunctionVector\given\inducingVector)}\big[\log p(\dataVector \given\mappingFunctionVector)\big] + \bbE_{p(\mappingFunctionVector\given\inducingVector)}\big[\log \frac{p(\mappingFunctionVector\given\inducingVector)}{p(\mappingFunctionVector\given\dataVector, \inducingVector)}\big]$$</span><br />  <br /><span class="math display">$$\log p(\dataVector\given\inducingVector) = \widetilde p(\dataVector\given\inducingVector) + \textsc{KL}[p(\mappingFunctionVector|\inducingVector)||p(\mappingFunctionVector\given\dataVector, \inducingVector)]$$</span><br /></p>
<p><span> No inversion of <span class="math inline">$\Kff$</span> required</span></p>
<p><br /><span class="math display">$$\widetilde p(\dataVector\given\inducingVector)  = \prod_{i=1}^\numData \widetilde p(\dataScalar_i\given\inducingVector)$$</span><br /> <br /><span class="math display">$$\widetilde p(\dataScalar\given\inducingVector) = \gaussianDist{\dataScalar}{\kfu\Kuu^{-1}\inducingVector}{\dataStd^2} \,{\color{red}\exp\left\{-\tfrac{1}{2\dataStd^2}\left(\kff- \kfu\Kuu^{-1}\kuf\right)\right\}}$$</span><br /></p>
<p><span>A straightforward likelihood approximation, and a penalty term</span></p>
<p><br /><span class="math display">$$\widetilde p(\inducingVector\given\dataVector,\inducingInputMatrix) = \frac{\widetilde p(\dataVector\given\inducingVector)p(\inducingVector\given\inducingInputMatrix)}{\int \widetilde p(\dataVector\given\inducingVector)p(\inducingVector\given\inducingInputMatrix)\text{d}{\inducingVector}}$$</span><br /></p>
<ul>
<li><p>Computing the posterior costs <span class="math inline">$\bigO(\numData\numInducing^2)$</span></p></li>
<li><p>We also get a lower bound of the marginal likelihood</p></li>
</ul>
<p><br /><span class="math display">$${\color{red}\sum_{i=1}^\numData-\tfrac{1}{2\dataStd^2}\left(\kff- \kfu\Kuu^{-1}\kuf\right)}$$</span><br /></p>
<p><br /><span class="math display">$${\color{red}\sum_{i=1}^\numData-\tfrac{1}{2\dataStd^2}\left(\kff - \kfu\Kuu^{-1}\kuf\right)}$$</span><br /></p>
<!--![image](../../../gp/tex/diagrams/cov_approx){width="60.00000%"}-->
<!--![image](../../../gp/tex/diagrams/cov_approx_opt){width="60.00000%"}-->
<p><span>It’s easy to show that as <span class="math inline">$\inducingInputMatrix \to \inputMatrix$</span>:</span></p>
<ul>
<li><p><span class="math inline">$\inducingVector \to \mappingFunctionVector$</span> (and the posterior is exact)</p></li>
<li><p>The penalty term is zero.</p></li>
<li><p>The cost returns to <span class="math inline">$\bigO(\numData^3)$</span></p></li>
</ul>
<ul>
<li><p><br />
</p></li>
<li><p></p></li>
</ul>
<p><span>So far we:</span></p>
<ul>
<li><p>introduced <span class="math inline">$\inducingInputMatrix, \inducingVector$</span></p></li>
<li><p>approximated the intergral over <span class="math inline">$\mappingFunctionVector$</span> variationally</p></li>
<li><p>captured the information in <span class="math inline">$\widetilde p(\inducingVector\given \dataVector)$</span></p></li>
<li><p>obtained a lower bound on the marginal likeihood</p></li>
<li><p>saw the effect of the penalty term</p></li>
<li><p>prediction for new points</p></li>
</ul>
<p><span>Omitted details:</span></p>
<ul>
<li><p>optimization of the covariance parameters using the bound</p></li>
<li><p>optimization of Z (simultaneously)</p></li>
<li><p>the form of <span class="math inline">$\widetilde p(\inducingVector\given \dataVector)$</span></p></li>
<li><p>historical approximations</p></li>
</ul>
<p><span>Subset selection</span> <span style="text-align:right"><span class="citation" data-cites="Lawrence:ivm02">Lawrence, Seeger, and Herbrich (n.d.)</span></span></p>
<ul>
<li><p>Random or systematic</p></li>
<li><p>Set <span class="math inline">$\inducingInputMatrix$</span> to subset of <span class="math inline">$\inputMatrix$</span></p></li>
<li><p>Set <span class="math inline">$\inducingVector$</span> to subset of <span class="math inline">$\mappingFunctionVector$</span></p></li>
<li><p>Approximation to <span class="math inline">$p(\dataVector\given \inducingVector)$</span>:</p>
<ul>
<li><p>$ p(_i) = p(_i_i) i$</p></li>
<li><p>$ p(_i) = 1  i$</p></li>
</ul></li>
</ul>
<p><span style="text-align:right"><span class="citation" data-cites="Quinonero:unifying05">Quiñonero Candela and Rasmussen (2005)</span></span> {Deterministic Training Conditional (DTC)}</p>
<ul>
<li><p>Approximation to <span class="math inline">$p(\dataVector\given \inducingVector)$</span>:</p>
<ul>
<li>$ p(_i) = (_i, [_i])$</li>
</ul></li>
<li><p>As our variational formulation, but without penalty</p></li>
</ul>
<p>Optimization of <span class="math inline">$\inducingInputMatrix$</span> is difficult</p>
<p><span>Fully Independent Training Conditional</span> <span style="text-align:right"><span class="citation" data-cites="Snelson:pseudo05">Snelson and Ghahramani (n.d.)</span></span></p>
<ul>
<li><p>Approximation to <span class="math inline">$p(\dataVector\given \inducingVector)$</span>:</p></li>
<li><p>$ p() = _i p(_i) $</p></li>
</ul>
<p>Optimization of <span class="math inline">$\inducingInputMatrix$</span> is still difficult, and there are some weird heteroscedatic effects</p>
<ul>
<li>GP-LVM Provides probabilistic non-linear dimensionality reduction.</li>
<li>How to select the dimensionality?</li>
<li>Need to estimate marginal likelihood.</li>
<li>In standard GP-LVM it increases with increasing <span class="math inline">$\latentDim$</span>.</li>
</ul>
<table>
<tr>
<td width="40%">
<p><strong>Bayesian GP-LVM</strong></p>
<ul>
<li>Start with a standard GP-LVM.</li>
<li>Apply standard latent variable approach:
<ul>
<li>Define Gaussian prior over , <span class="math inline">$\latentMatrix$</span>.</li>
<li>Integrate out .</li>
<li>Unfortunately integration is intractable.</li>
</ul></li>
</ul>
</td>
<td width="60%">
<center>
{ }
</center>
</td>
</tr>
</table>
<h2 id="standard-variational-approach-fails">Standard Variational Approach Fails</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/variational-bayes-gplvm-long.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/variational-bayes-gplvm-long.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<ul>
<li>Standard variational bound has the form: <br /><span class="math display">$$
\likelihoodBound = \expDist{\log p(\dataVector|\latentMatrix)}{q(\latentMatrix)} + \KL{q(\latentMatrix)}{p(\latentMatrix)}
$$</span><br /></li>
</ul>
<p>The standard variational approach would require the expectation of <span class="math inline">$\log p(\dataVector|\latentMatrix)$</span> under <span class="math inline">$q(\latentMatrix)$</span>. <br /><span class="math display">$$
  \begin{align}
  \log p(\dataVector|\latentMatrix) = &amp; -\frac{1}{2}\dataVector^\top\left(\kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector}+\dataStd^2\eye\right)^{-1}\dataVector \\ &amp; -\frac{1}{2}\log \det{\kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector}+\dataStd^2 \eye} -\frac{\numData}{2}\log 2\pi
  \end{align}
  $$</span><br /> But this is extremely difficult to compute because <span class="math inline">$\kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector}$</span> is dependent on <span class="math inline">$\latentMatrix$</span> and it appears in the inverse.</p>
<h2 id="variational-bayesian-gp-lvm">Variational Bayesian GP-LVM</h2>
<p>The alternative approach is to consider the collapsed variational bound (used for low rank (sparse is a misnomer) Gaussian process approximations. <br /><span class="math display">$$
    p(\dataVector)\geq \prod_{i=1}^\numData c_i \int \gaussianDist{\dataVector}{\expSamp{\mappingFunctionVector}}{\dataStd^2\eye}p(\inducingVector) \text{d}\inducingVector
  $$</span><br /> <br /><span class="math display">$$
    p(\dataVector|\latentMatrix )\geq \prod_{i=1}^\numData c_i \int \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye}p(\inducingVector) \text{d}\inducingVector
  $$</span><br /> <br /><span class="math display">$$
      \int p(\dataVector|\latentMatrix)p(\latentMatrix) \text{d}\latentMatrix \geq \int \prod_{i=1}^\numData c_i \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye} p(\latentMatrix)\text{d}\latentMatrix p(\inducingVector) \text{d}\inducingVector
  $$</span><br /></p>
<p>To integrate across <span class="math inline">$\latentMatrix$</span> we apply the lower bound to the inner integral. <br /><span class="math display">$$
    \begin{align}
    \int \prod_{i=1}^\numData c_i \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye} p(\latentMatrix)\text{d}\latentMatrix \geq &amp; \expDist{\sum_{i=1}^\numData\log  c_i}{q(\latentMatrix)}\\ &amp; +\expDist{\log\gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye}}{q(\latentMatrix)}\\&amp; + \KL{q(\latentMatrix)}{p(\latentMatrix)}    
    \end{align}
  $$</span><br /> * Which is analytically tractable for Gaussian <span class="math inline">$q(\latentMatrix)$</span> and some covariance functions.</p>
<ul>
<li><p>Need expectations under <span class="math inline">$q(\latentMatrix)$</span> of: <br /><span class="math display">$$
\log c_i = \frac{1}{2\dataStd^2} \left[\kernelScalar_{i, i} - \kernelVector_{i, \inducingVector}^\top \kernelMatrix_{\inducingVector, \inducingVector}^{-1} \kernelVector_{i, \inducingVector}\right]
$$</span><br /> and <br /><span class="math display">$$
\log \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector,\dataMatrix)}}{\dataStd^2\eye} = -\frac{1}{2}\log 2\pi\dataStd^2 - \frac{1}{2\dataStd^2}\left(\dataScalar_i - \kernelMatrix_{\mappingFunctionVector, \inducingVector}\kernelMatrix_{\inducingVector,\inducingVector}^{-1}\inducingVector\right)^2
$$</span><br /></p></li>
<li><p>This requires the expectations <br /><span class="math display">$$
\expDist{\kernelMatrix_{\mappingFunctionVector,\inducingVector}}{q(\latentMatrix)}
$$</span><br /> and <br /><span class="math display">$$
\expDist{\kernelMatrix_{\mappingFunctionVector,\inducingVector}\kernelMatrix_{\inducingVector,\inducingVector}^{-1}\kernelMatrix_{\inducingVector,\mappingFunctionVector}}{q(\latentMatrix)}
$$</span><br /> which can be computed analytically for some covariance functions <span class="citation" data-cites="Damianou:variational15">(Damianou, Titsias, and Lawrence 2016)</span> or through sampling <span class="citation" data-cites="Damianou:thesis2015 Salimbeni:doubly2017">(Damianou 2015; Salimbeni and Deisenroth 2017)</span>.</p></li>
</ul>
<p><span style="text-align:right"></p>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Andreas Damianou
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/andreas-damianou.png" clip-path="url(#clip0)"/>
</svg>
<p></span><br />
<span style="text-align:right"><span class="citation" data-cites="Damianou:deepgp13">Damianou and Lawrence (2013)</span></span><br />
</p>
<ul>
<li><p>Augment each layer with inducing variables <span class="math inline">$\inducingVector_i$</span>.</p></li>
<li><p>Apply variational compression,  where <br /><span class="math display">$$\tilde p(\hiddenVector_i|\inducingVector_i,\hiddenVector_{i-1})
    = \gaussianDist{\hiddenVector_i}{\kernelMatrix_{\hiddenVector_{i}\inducingVector_{i}}\kernelMatrix_{\inducingVector_i\inducingVector_i}^{-1}\inducingVector_i}{\sigma^2_i\eye}.$$</span><br /></p></li>
</ul>
<p><span style="text-align:right"></p>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip1">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
James Hensman
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/james-hensman.png" clip-path="url(#clip1)"/>
</svg>
<p></span><br />
<span style="text-align:right"><span class="citation" data-cites="Hensman:nested14">Hensman and Lawrence (2014)</span></span></p>
<ul>
<li><p>By sustaining explicity distributions over inducing variables James Hensman has developed a nested variant of variational compression.</p></li>
<li><p>Exciting thing: it mathematically looks like a deep neural network, but with inducing variables in the place of basis functions.</p></li>
<li><p>Additional complexity control term in the objective function.</p></li>
</ul>
<p></p>
<p><br /><span class="math display">$${\only&lt;1&gt;{\color{red}}\log \gaussianDist{\dataVector}{{\only&lt;2-&gt;{\color{blue}}{\boldsymbol
          \Psi}_{\numLayers}}\kernelMatrix_{\inducingVector_{\numLayers}
          \inducingVector_{\numLayers}}^{-1}{\mathbf
          m}_\numLayers}{\sigma^2_\numLayers\eye}}$$</span><br /> where   </p>
<p>For Gaussian likelihoods:  </p>
<p>Define: <br /><span class="math display">$$q_{i, i} = \varianceDist{\mappingFunction_i}{p(\mappingFunction_i|\inducingVector)} = \expDist{\mappingFunction_i^2}{p(\mappingFunction_i|\inducingVector)} - \expDist{\mappingFunction_i}{p(\mappingFunction_i|\inducingVector)}^2$$</span><br /> We can write: <br /><span class="math display">$$c_i = \exp\left(-{\frac{q_{i,i}}{2\sigma^2}}\right)$$</span><br /> If joint distribution of <span class="math inline">$p(\mappingFunctionVector, \inducingVector)$</span> is Gaussian then: <br /><span class="math display">$$q_{i, i} = \kernelScalar_{i, i} - \kernelVector_{i, \inducingVector}^\top \kernelMatrix_{\inducingVector, \inducingVector}^{-1} \kernelVector_{i, \inducingVector}$$</span><br /></p>
<p><span class="math inline"><em>c</em><sub><em>i</em></sub></span> is not a function of <span class="math inline">$\inducingVector$</span> but <em>is</em> a function of <span class="math inline">$\inputMatrix_\inducingVector$</span>.</p>
<p>Substitute variational bound into marginal likelihood: <br /><span class="math display">$$p(\dataVector)\geq \prod_{i=1}^\numData c_i \int \gaussianDist{\dataVector}{\expSamp{\mappingFunctionVector}}{\dataStd^2\eye}p(\inducingVector) \text{d}\inducingVector$$</span><br /> Note that: <br /><span class="math display">$$\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector)} = \kernelMatrix_{\mappingFunctionVector, \inducingVector} \kernelMatrix_{\inducingVector, \inducingVector}^{-1}\inducingVector$$</span><br /> is <em>linearly</em> dependent on <span class="math inline">$\inducingVector$</span>.</p>
<p>Making the marginalization of <span class="math inline">$\inducingVector$</span> straightforward. In the Gaussian case: <br /><span class="math display">$$p(\inducingVector) = \gaussianDist{\inducingVector}{\zerosVector}{\kernelMatrix_{\inducingVector,\inducingVector}}$$</span><br />      </p>
<ul>
<li><p>Thang and Turner paper</p></li>
<li><p>Joint Gaussianity is analytic, but not flexible.</p></li>
</ul>
<h2 id="a-simple-regression-problem">A Simple Regression Problem</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/inducing-variables-demo.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/inducing-variables-demo.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Here we set up a simple one dimensional regression problem. The input locations, <span class="math inline">$\inputMatrix$</span>, are in two separate clusters. The response variable, <span class="math inline">$\dataVector$</span>, is sampled from a Gaussian process with an exponentiated quadratic covariance.</p>
<h2 id="gpy-a-gaussian-process-framework-in-python">GPy: A Gaussian Process Framework in Python</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Gaussian processes are a flexible tool for non-parametric analysis with uncertainty. The GPy software was started in Sheffield to provide a easy to use interface to GPs. One which allowed the user to focus on the modelling rather than the mathematics.</p>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gpy-software-magnify" class="magnify" onclick="magnifyFigure(&#39;gpy-software&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-software-caption" class="caption-frame">
<p>Figure: GPy is a BSD licensed software code base for implementing Gaussian process models in Python. It is designed for teaching and modelling. We welcome contributions which can be made through the Github repository <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></p>
</div>
</div>
<p>GPy is a BSD licensed software code base for implementing Gaussian process models in python. This allows GPs to be combined with a wide variety of software libraries.</p>
<p>The software itself is available on <a href="https://github.com/SheffieldML/GPy">GitHub</a> and the team welcomes contributions.</p>
<p>The aim for GPy is to be a probabilistic-style programming language, i.e. you specify the model rather than the algorithm. As well as a large range of covariance functions the software allows for non-Gaussian likelihoods, multivariate outputs, dimensionality reduction and approximations for larger data sets.</p>
<p>The documentation for GPy can be found <a href="https://gpy.readthedocs.io/en/latest/">here</a>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="op">%</span>pip install gpy</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> GPy</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>np.random.seed(<span class="dv">101</span>)</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>N <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>noise_var <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb4-3"><a href="#cb4-3"></a>X <span class="op">=</span> np.zeros((<span class="dv">50</span>, <span class="dv">1</span>))</span>
<span id="cb4-4"><a href="#cb4-4"></a>X[:<span class="dv">25</span>, :] <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">25</span>)[:,<span class="va">None</span>] <span class="co"># First cluster of inputs/covariates</span></span>
<span id="cb4-5"><a href="#cb4-5"></a>X[<span class="dv">25</span>:, :] <span class="op">=</span> np.linspace(<span class="dv">7</span>,<span class="dv">10</span>,<span class="dv">25</span>)[:,<span class="va">None</span>] <span class="co"># Second cluster of inputs/covariates</span></span>
<span id="cb4-6"><a href="#cb4-6"></a></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="co"># Sample response variables from a Gaussian process with exponentiated quadratic covariance.</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>k <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>)</span>
<span id="cb4-9"><a href="#cb4-9"></a>y <span class="op">=</span> np.random.multivariate_normal(np.zeros(N),k.K(X)<span class="op">+</span>np.eye(N)<span class="op">*</span>np.sqrt(noise_var)).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span></code></pre></div>
<p>First we perform a full Gaussian process regression on the data. We create a GP model, <code>m_full</code>, and fit it to the data, plotting the resulting fit.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>m_full <span class="op">=</span> GPy.models.GPRegression(X,y)</span>
<span id="cb5-2"><a href="#cb5-2"></a>_ <span class="op">=</span> m_full.optimize(messages<span class="op">=</span><span class="va">True</span>) <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<div class="figure">
<div id="sparse-demo-full-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-full-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="sparse-demo-full-gp-magnify" class="magnify" onclick="magnifyFigure(&#39;sparse-demo-full-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sparse-demo-full-gp-caption" class="caption-frame">
<p>Figure: Full Gaussian process fitted to the data set.</p>
</div>
</div>
<p>Now we set up the inducing variables, <span class="math inline"><strong>u</strong></span>. Each inducing variable has its own associated input index, <span class="math inline"><strong>Z</strong></span>, which lives in the same space as <span class="math inline">$\inputMatrix$</span>. Here we are using the true covariance function parameters to generate the fit.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>kern <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>)</span>
<span id="cb6-2"><a href="#cb6-2"></a>Z <span class="op">=</span> np.hstack(</span>
<span id="cb6-3"><a href="#cb6-3"></a>        (np.linspace(<span class="fl">2.5</span>,<span class="fl">4.</span>,<span class="dv">3</span>),</span>
<span id="cb6-4"><a href="#cb6-4"></a>        np.linspace(<span class="dv">7</span>,<span class="fl">8.5</span>,<span class="dv">3</span>)))[:,<span class="va">None</span>]</span>
<span id="cb6-5"><a href="#cb6-5"></a>m <span class="op">=</span> GPy.models.SparseGPRegression(X,y,kernel<span class="op">=</span>kern,Z<span class="op">=</span>Z)</span>
<span id="cb6-6"><a href="#cb6-6"></a>m.noise_var <span class="op">=</span> noise_var</span>
<span id="cb6-7"><a href="#cb6-7"></a>m.inducing_inputs.constrain_fixed()</span>
<span id="cb6-8"><a href="#cb6-8"></a>display(m)</span></code></pre></div>
<div class="figure">
<div id="sparse-demo-constrained-inducing-6-unlearned-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-constrained-inducing-6-unlearned-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="sparse-demo-constrained-inducing-6-unlearned-gp-magnify" class="magnify" onclick="magnifyFigure(&#39;sparse-demo-constrained-inducing-6-unlearned-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sparse-demo-constrained-inducing-6-unlearned-gp-caption" class="caption-frame">
<p>Figure: Sparse Gaussian process fitted with six inducing variables, no optimization of parameters or inducing variables.</p>
</div>
</div>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>_ <span class="op">=</span> m.optimize(messages<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-2"><a href="#cb7-2"></a>display(m)</span></code></pre></div>
<div class="figure">
<div id="sparse-demo-constrained-inducing-6-learned-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-constrained-inducing-6-learned-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="sparse-demo-constrained-inducing-6-learned-gp-magnify" class="magnify" onclick="magnifyFigure(&#39;sparse-demo-constrained-inducing-6-learned-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sparse-demo-constrained-inducing-6-learned-gp-caption" class="caption-frame">
<p>Figure: Gaussian process fitted with inducing variables fixed and parameters optimized</p>
</div>
</div>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>m.randomize()</span>
<span id="cb8-2"><a href="#cb8-2"></a>m.inducing_inputs.unconstrain()</span>
<span id="cb8-3"><a href="#cb8-3"></a>_ <span class="op">=</span> m.optimize(messages<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="figure">
<div id="sparse-demo-unconstrained-inducing-6-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-unconstrained-inducing-6-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="sparse-demo-unconstrained-inducing-6-gp-magnify" class="magnify" onclick="magnifyFigure(&#39;sparse-demo-unconstrained-inducing-6-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sparse-demo-unconstrained-inducing-6-gp-caption" class="caption-frame">
<p>Figure: Gaussian process fitted with location of inducing variables and parameters both optimized</p>
</div>
</div>
<p>Now we will vary the number of inducing points used to form the approximation.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>m.num_inducing<span class="op">=</span><span class="dv">8</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>m.randomize()</span>
<span id="cb9-3"><a href="#cb9-3"></a>M <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>m.set_Z(np.random.rand(M,<span class="dv">1</span>)<span class="op">*</span><span class="dv">12</span>)</span>
<span id="cb9-5"><a href="#cb9-5"></a></span>
<span id="cb9-6"><a href="#cb9-6"></a>_ <span class="op">=</span> m.optimize(messages<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="figure">
<div id="sparse-demo-sparse-inducing-8-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-sparse-inducing-8-gp.svg" width="80%" style=" ">
</object>
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-full-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="sparse-demo-sparse-inducing-8-magnify" class="magnify" onclick="magnifyFigure(&#39;sparse-demo-sparse-inducing-8&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sparse-demo-sparse-inducing-8-caption" class="caption-frame">
<p>Figure: Comparison of the full Gaussian process fit with a sparse Gaussian process using eight inducing varibles. Both inducing variables and parameters are optimized.</p>
</div>
</div>
<p>And we can compare the probability of the result to the full model.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="bu">print</span>(m.log_likelihood(), m_full.log_likelihood())</span></code></pre></div>
<h1 id="non-gaussian-likelihoods">Non Gaussian Likelihoods</h1>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/non-gaussian-gps.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/non-gaussian-gps.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>[Gaussian processes model functions. If our observation is a corrupted version of this function and the corruption process is <em>also</em> Gaussian, it is trivial to account for this. However, there are many circumstances where our observation may be non Gaussian. In these cases we need to turn to approximate inference techniques. As a simple illustration, we’ll use a dataset of binary observations of the language that is spoken in different regions of East-Timor. First we will load the data and a couple of libraries to visualize it.}</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="im">from</span> matplotlib.patches <span class="im">import</span> Polygon</span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="im">from</span> matplotlib.collections <span class="im">import</span> PatchCollection</span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="im">import</span> cPickle <span class="im">as</span> pickle</span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="im">import</span> urllib</span></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>urllib.urlretrieve(<span class="st">&#39;http://staffwww.dcs.sheffield.ac.uk/people/M.Zwiessele/gpss/lab2/EastTimor.pickle&#39;</span>, <span class="st">&#39;EastTimor2.pickle&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># Load the data</span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;./EastTimor2.pickle&quot;</span>,<span class="st">&quot;rb&quot;</span>) <span class="im">as</span> f:</span>
<span id="cb13-3"><a href="#cb13-3"></a>    X,y,polygons <span class="op">=</span> pickle.load(f)</span></code></pre></div>
<p>Now we will create a map of East Timor and, using GPy, plot the data on top of it. A classification model can be defined in a similar way to the regression model, but now using <code>GPy.models.GPClassification</code>. However, once we’ve define the model, we also need to update the approximation to the likelihood. This runs the Expectation propagation updates.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="im">import</span> GPy</span></code></pre></div>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="co">#Define the model</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>kern <span class="op">=</span> GPy.kern.RBF(<span class="dv">2</span>)</span>
<span id="cb15-3"><a href="#cb15-3"></a>m <span class="op">=</span> GPy.models.GPClassification(X,y, kernel<span class="op">=</span>kern)</span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="bu">print</span>(m)</span></code></pre></div>
<p>The decision boundary should be quite poor! However we haven’t optimized the model. Try the following:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>m.optimize()</span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="bu">print</span>(m)</span></code></pre></div>
<p>The optimization is based on the likelihood approximation that was made after we constructed the model. However, because we’ve now changed the model parameters the quality of that approximation has now probably deteriorated. To improve the model we should iterate between updating the Expectation propagation approximation and optimizing the model parameters.</p>
<h2 id="robust-regression-a-running-example">Robust Regression: A Running Example</h2>
<h2 id="olympic-marathon-data">Olympic Marathon Data</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-marathon-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-marathon-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardised distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
<p>The first thing we will do is load a standard data set for regression modelling. The data consists of the pace of Olympic Gold Medal Marathon winners for the Olympics from 1896 to present. First we load in the data and plot.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="op">%</span>pip install <span class="op">--</span>upgrade git<span class="op">+</span>https:<span class="op">//</span>github.com<span class="op">/</span>sods<span class="op">/</span>ods</span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>data <span class="op">=</span> pods.datasets.olympic_marathon_men()</span>
<span id="cb19-2"><a href="#cb19-2"></a>x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb19-3"><a href="#cb19-3"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb19-4"><a href="#cb19-4"></a></span>
<span id="cb19-5"><a href="#cb19-5"></a>offset <span class="op">=</span> y.mean()</span>
<span id="cb19-6"><a href="#cb19-6"></a>scale <span class="op">=</span> np.sqrt(y.var())</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-magnify" class="magnify" onclick="magnifyFigure(&#39;olympic-marathon&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-caption" class="caption-frame">
<p>Figure: Olympic marathon pace times since 1892.</p>
</div>
</div>
<p>Things to notice about the data include the outlier in 1904, in this year, the olympics was in St Louis, USA. Organizational problems and challenges with dust kicked up by the cars following the race meant that participants got lost, and only very few participants completed.</p>
<p>More recent years see more consistently quick marathons.</p>
<p>We already considered the olympic marathon data. In 1904 we noted there was an outlier example. Today we’ll see if we can deal with that outlier by considering a non-Gaussian likelihood. Noise sampled from a Student-<span class="math inline"><em>t</em></span> density is heavier tailed than that sampled from a Gaussian. However, it cannot be trivially assimilated into the Gaussian process. Below we use the <em>Laplace approximation</em> to incorporate this noise model.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>GPy.likelihoods.noise_model_constructors.student_t?</span></code></pre></div>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>GPy.models.GPRegression?</span></code></pre></div>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="co"># # make a student t likelihood with standard parameters</span></span>
<span id="cb22-2"><a href="#cb22-2"></a><span class="co"># t_distribution = GPy.likelihoods.noise_model_constructors.student_t(deg_free=5, sigma2=2)</span></span>
<span id="cb22-3"><a href="#cb22-3"></a><span class="co"># stu_t_likelihood = GPy.likelihoods.Laplace(Y.copy(), t_distribution)</span></span>
<span id="cb22-4"><a href="#cb22-4"></a></span>
<span id="cb22-5"><a href="#cb22-5"></a><span class="co"># kern = GPy.kern.RBF(1, lengthscale=10) + GPy.kern.Bias(1)</span></span>
<span id="cb22-6"><a href="#cb22-6"></a><span class="co"># model = GPy.models.GPRegression(X, Y, kernel=kern, likelihood=stu_t_likelihood)</span></span>
<span id="cb22-7"><a href="#cb22-7"></a></span>
<span id="cb22-8"><a href="#cb22-8"></a>t_distribution <span class="op">=</span> GPy.likelihoods.StudentT(deg_free<span class="op">=</span><span class="dv">5</span>, sigma2<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb22-9"><a href="#cb22-9"></a>laplace <span class="op">=</span> GPy.inference.latent_function_inference.Laplace()</span>
<span id="cb22-10"><a href="#cb22-10"></a></span>
<span id="cb22-11"><a href="#cb22-11"></a>kern <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, lengthscale<span class="op">=</span><span class="dv">5</span>) <span class="op">+</span> GPy.kern.Bias(<span class="dv">1</span>)</span>
<span id="cb22-12"><a href="#cb22-12"></a>model <span class="op">=</span> GPy.core.GP(X, Y, kernel<span class="op">=</span>kern, inference_method<span class="op">=</span>laplace, likelihood<span class="op">=</span>t_distribution)</span>
<span id="cb22-13"><a href="#cb22-13"></a>model.constrain_positive(<span class="st">&#39;t_noise&#39;</span>)</span>
<span id="cb22-14"><a href="#cb22-14"></a></span>
<span id="cb22-15"><a href="#cb22-15"></a>model.optimize()</span>
<span id="cb22-16"><a href="#cb22-16"></a><span class="bu">print</span>(model)</span></code></pre></div>
<h1 id="sparse-gp-classification">Sparse GP Classification</h1>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/non-gaussian-gps.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/non-gaussian-gps.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>In this section we’ll combine expectation propagation with the low rank approximation to build a simple image classification application. For this toy example we’ll classify whether or not the subject of the image is wearing glasses.</p>
<p>Correspond to whether the subject of the image is wearing glasses. Set up the ipython environment and download the data:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="im">from</span> scipy <span class="im">import</span> io</span></code></pre></div>
<p>First let’s retrieve some data. We will use the ORL faces data set, our objective will be to classify whether or not the subject in an image is wearing glasess.</p>
<p>Here’s a simple way to visualise the data. Each pixel in the image will become an input to the GP.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>data <span class="op">=</span> pods.datasets.olivetti_glasses()</span>
<span id="cb25-2"><a href="#cb25-2"></a>X <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb25-3"><a href="#cb25-3"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb25-4"><a href="#cb25-4"></a>Xtest <span class="op">=</span> data[<span class="st">&#39;Xtest&#39;</span>]</span>
<span id="cb25-5"><a href="#cb25-5"></a>ytest <span class="op">=</span> data[<span class="st">&#39;Ytest&#39;</span>]</span>
<span id="cb25-6"><a href="#cb25-6"></a><span class="bu">print</span>(data[<span class="st">&#39;info&#39;</span>], data[<span class="st">&#39;details&#39;</span>], data[<span class="st">&#39;citation&#39;</span>])</span></code></pre></div>
<div class="figure">
<div id="olivetti-glasses-image-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="'../slides/diagrams/datasets/olivetti-glasses-image.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="olivetti-glasses-image-magnify" class="magnify" onclick="magnifyFigure(&#39;olivetti-glasses-image&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olivetti-glasses-image-caption" class="caption-frame">
<p>Figure: Image from the Oivetti glasses data sets.</p>
</div>
</div>
<p>Next we choose some inducing inputs. Here we’ve chosen inducing inputs by applying k-means clustering to the training data. Think about whether this is a good scheme for choosing the inputs? Can you devise a better one?</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a>X.shape</span>
<span id="cb26-2"><a href="#cb26-2"></a>y.shape</span>
<span id="cb26-3"><a href="#cb26-3"></a><span class="bu">print</span>(X)</span></code></pre></div>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="im">from</span> scipy <span class="im">import</span> cluster</span></code></pre></div>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>M <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb28-2"><a href="#cb28-2"></a>X <span class="op">=</span> (X <span class="op">-</span> X.mean(<span class="dv">0</span>)[<span class="va">None</span>,:])<span class="op">/</span>X.std(<span class="dv">0</span>)[<span class="va">None</span>,:]</span>
<span id="cb28-3"><a href="#cb28-3"></a>Z <span class="op">=</span> np.random.permutation(X)[:M]</span></code></pre></div>
<p>Finally, we’re ready to build the classifier object.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a>kern <span class="op">=</span> GPy.kern.RBF(X.shape[<span class="dv">1</span>],lengthscale<span class="op">=</span><span class="dv">20</span>) <span class="op">+</span> GPy.kern.White(X.shape[<span class="dv">1</span>],<span class="fl">0.001</span>)</span>
<span id="cb29-2"><a href="#cb29-2"></a>m <span class="op">=</span> GPy.models.SparseGPClassification(X, y, kernel<span class="op">=</span>kern, Z<span class="op">=</span>Z)</span>
<span id="cb29-3"><a href="#cb29-3"></a>m.optimize()</span>
<span id="cb29-4"><a href="#cb29-4"></a><span class="bu">print</span>(m)</span></code></pre></div>
<div class="figure">
<div id="olivetti-inducing-variable-gradients-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="'../slides/diagrams/datasets/olivetti-inducing-variable-gradients.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="olivetti-inducing-variable-gradients-magnify" class="magnify" onclick="magnifyFigure(&#39;olivetti-inducing-variable-gradients&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olivetti-inducing-variable-gradients-caption" class="caption-frame">
<p>Figure: The gradients of the inducing variable.</p>
</div>
</div>
<h2 id="thanks">Thanks!</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/approximate-gps.mdtmp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/approximate-gps.mdtmp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>For more information on these subjects and more you might want to check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Alvarez:efficient10">
<p>Álvarez, Mauricio A., David Luengo, Michalis K. Titsias, and Neil D. Lawrence. 2010. “Efficient Multioutput Gaussian Processes Through Variational Inducing Kernels.” In, 9:25–32.</p>
</div>
<div id="ref-Thang:unifying17">
<p>Bui, Thang D., Josiah Yan, and Richard E. Turner. 2017. “A Unifying Framework for Gaussian Process Pseudo-Point Approximations Using Power Expectation Propagation.” <em>Journal of Machine Learning Research</em> 18 (104): 1–72. <a href="http://jmlr.org/papers/v18/16-603.html">http://jmlr.org/papers/v18/16-603.html</a>.</p>
</div>
<div id="ref-Csato:thesis02">
<p>Csató, Lehel. 2002. “Gaussian Processes — Iterative Sparse Approximations.” PhD thesis, Aston University.</p>
</div>
<div id="ref-Csato:sparse02">
<p>Csató, Lehel, and Manfred Opper. 2002. “Sparse on-Line Gaussian Processes.” <em>Neural Computation</em> 14 (3): 641–68.</p>
</div>
<div id="ref-Dai:gpu14">
<p>Dai, Zhenwen, Andreas Damianou, James Hensman, and Neil D. Lawrence. 2014. “Gaussian Process Models with Parallelization and GPU Acceleration.”</p>
</div>
<div id="ref-Damianou:thesis2015">
<p>Damianou, Andreas. 2015. “Deep Gaussian Processes and Variational Propagation of Uncertainty.” PhD thesis, University of Sheffield.</p>
</div>
<div id="ref-Damianou:deepgp13">
<p>Damianou, Andreas, and Neil D. Lawrence. 2013. “Deep Gaussian Processes.” In, 31:207–15.</p>
</div>
<div id="ref-Damianou:variational15">
<p>Damianou, Andreas, Michalis K. Titsias, and Neil D. Lawrence. 2016. “Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes.” <em>Journal of Machine Learning Research</em> 17.</p>
</div>
<div id="ref-Gal:distributed14">
<p>Gal, Yarin, Mark van der Wilk, and Carl E. Rasmussen. n.d. “Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models.” In.</p>
</div>
<div id="ref-Hensman:bigdata13">
<p>Hensman, James, Nicoló Fusi, and Neil D. Lawrence. n.d. “Gaussian Processes for Big Data.” In.</p>
</div>
<div id="ref-Hensman:nested14">
<p>Hensman, James, and Neil D. Lawrence. 2014. “Nested Variational Compression in Deep Gaussian Processes.” University of Sheffield.</p>
</div>
<div id="ref-Hensman:fast12">
<p>Hensman, James, Magnus Rattray, and Neil D. Lawrence. 2012. “Fast Variational Inference in the Conjugate Exponential Family.” In.</p>
</div>
<div id="ref-Hoffman:stochastic12">
<p>Hoffman, Matthew, David M. Blei, Chong Wang, and John Paisley. 2012. “Stochastic Variational Inference.” <em>arXiv Preprint arXiv:1206.7051</em>.</p>
</div>
<div id="ref-King:klcorrection06">
<p>King, Nathaniel J., and Neil D. Lawrence. n.d. “Fast Variational Inference for Gaussian Process Models Through KL-Correction.” In, 270–81.</p>
</div>
<div id="ref-Lawrence:larger07">
<p>Lawrence, Neil D. n.d. “Learning for Larger Datasets with the Gaussian Process Latent Variable Model.” In, 243–50.</p>
</div>
<div id="ref-Lawrence:ivm02">
<p>Lawrence, Neil D., Matthias Seeger, and Ralf Herbrich. n.d. “Fast Sparse Gaussian Process Methods: The Informative Vector Machine.” In, 625–32.</p>
</div>
<div id="ref-Lazaro:spectrum10">
<p>Lázaro-Gredilla, Miguel, Joaquin Quiñonero-Candela, and Carl Edward Rasmussen. 2010. “Sparse Spectrum Gaussian Processes.” <em>Journal of Machine Learning Research</em> 11: 1865–81.</p>
</div>
<div id="ref-Quinonero:unifying05">
<p>Quiñonero Candela, Joaquin, and Carl Edward Rasmussen. 2005. “A Unifying View of Sparse Approximate Gaussian Process Regression.” <em>Journal of Machine Learning Research</em> 6: 1939–59.</p>
</div>
<div id="ref-Salimbeni:doubly2017">
<p>Salimbeni, Hugh, and Marc Deisenroth. 2017. “Doubly Stochastic Variational Inference for Deep Gaussian Processes.” In <em>Advances in Neural Information Processing Systems 30</em>, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 4591–4602. Curran Associates, Inc. <a href="http://papers.nips.cc/paper/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.pdf">http://papers.nips.cc/paper/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.pdf</a>.</p>
</div>
<div id="ref-Seeger:auto17">
<p>Seeger, Matthias W., Asmus Hetzel, Zhenwen Dai, and Neil D. Lawrence. 2017. “Auto-Differentiating Linear Algebra.” <em>CoRR</em> abs/1710.08717. <a href="http://arxiv.org/abs/1710.08717">http://arxiv.org/abs/1710.08717</a>.</p>
</div>
<div id="ref-Seeger:fast03">
<p>Seeger, Matthias, Christopher K. I. Williams, and Neil D. Lawrence. n.d. “Fast Forward Selection to Speed up Sparse Gaussian Process Regression.” In.</p>
</div>
<div id="ref-Smola:sparsegp00">
<p>Smola, Alexander J., and Peter L. Bartlett. n.d. “Sparse Greedy Gaussian Process Regression.” In, 619–25.</p>
</div>
<div id="ref-Snelson:pseudo05">
<p>Snelson, Edward, and Zoubin Ghahramani. n.d. “Sparse Gaussian Processes Using Pseudo-Inputs.” In.</p>
</div>
<div id="ref-Titsias:variational09">
<p>Titsias, Michalis K. n.d. “Variational Learning of Inducing Variables in Sparse Gaussian Processes.” In, 567–74.</p>
</div>
<div id="ref-Williams:nystrom00">
<p>Williams, Christopher K. I., and Matthias Seeger. n.d. “Using the Nyström Method to Speed up Kernel Machines.” In, 682–88.</p>
</div>
</div>

