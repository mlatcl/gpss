---
title: "Multi-output Gaussian Processes"
venue: "Gaussian Process Summer School"
abstract: "<p>In this talk we review multi-output Gaussian processes. Introducing them initially through a Kalman filter representation of a GP.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: University of Cambridge
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orchid: 
week: 0
session: 3
reveal: 03-multi-output-gps.slides.html
ipynb: 03-multi-output-gps.ipynb
layout: lecture
categories:
- notes
---



<!-- To compile -->
<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h2 id="simple-kalman-filter">Simple Kalman Filter</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/simple-kalman-filter.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/simple-kalman-filter.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<ul>
<li><p>We have state vector <span class="math inline">$\inputMatrix = \left[\inputVector_1  \dots \inputVector_\latentDim\right] \in \mathbb{R}^{\top \times \latentDim}$</span> and if each state evolves independently we have <br /><span class="math display">$$      
\begin{align*}
  p(\inputMatrix) &amp;= \prod_{i=1}^\latentDim p(\inputVector_{:, i}) \\
 p(\inputVector_{:, i}) &amp;= \gaussianDist{\inputVector_{:, i}}{\zerosVector}{\kernelMatrix}.
\end{align*}
$$</span><br /></p></li>
<li><p>We want to obtain outputs through: <br /><span class="math display">$$
\dataVector_{i, :} = \mappingMatrix\inputVector_{i, :}
$$</span><br /></p></li>
</ul>
<h2 id="stacking-and-kronecker-products">Stacking and Kronecker Products</h2>
<ul>
<li>Represent with a ‘stacked’ system: <br /><span class="math display">$$
p(\inputVector) = \gaussianDist{\inputVector}{\zerosVector}{\eye \otimes \kernelMatrix}
$$</span><br /> where the stacking is placing each column of <span class="math inline">$\inputMatrix$</span> one on top of another as <br /><span class="math display">$$
\inputVector= \begin{bmatrix}
      \inputVector_{:, 1}\\
      \inputVector_{:, 2}\\
      \vdots\\
      \inputVector_{:, \latentDim}
    \end{bmatrix}
$$</span><br /></li>
</ul>
<h2 id="kronecker-product">Kronecker Product</h2>
<object class="svgplot " data="../slides/diagrams/kern/kronecker_illustrate.svg" width style=" ">
</object>
<object class="svgplot " data="../slides/diagrams/kern/kronecker_IK.svg" width style=" ">
</object>
<h2 id="stacking-and-kronecker-products-1">Stacking and Kronecker Products</h2>
<ul>
<li>Represent with a ‘stacked’ system: <br /><span class="math display">$$p
(\inputVector) = \gaussianDist{\inputVector}{\zerosVector}{\eye\otimes \kernelMatrix}
$$</span><br /> where the stacking is placing each column of <span class="math inline">$\inputMatrix$</span> one on top of another as <br /><span class="math display">$$
\inputVector= \begin{bmatrix}
      \inputVector_{:, 1}\\
      \inputVector_{:, 2}\\
      \vdots\\
      \inputVector_{:, \latentDim}
    \end{bmatrix}
    $$</span><br /></li>
</ul>
<h2 id="column-stacking">Column Stacking</h2>
<p>gpKalmanFilterKroneckerPlot2</p>
<p>For this stacking the marginal distribution over <em>time</em> is given by the block diagonals.</p>
<div class="figure">
<div id="kronecker-ik-highlighted-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/kern/kronecker_IK_highlighted005.svg" width="60%" style=" ">
</object>
</div>
<div id="kronecker-ik-highlighted-magnify" class="magnify" onclick="magnifyFigure(&#39;kronecker-ik-highlighted&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kronecker-ik-highlighted-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<h2 id="section"></h2>
<p>{Two Ways of Stacking</p>
<p>Can also stack each row of <span class="math inline">$\inputMatrix$</span> to form column vector: <br /><span class="math display">$$\inputVector= \begin{bmatrix}
      \inputVector_{1, :}\\
      \inputVector_{2, :}\\
      \vdots\\
      \inputVector_{{T}, :}
    \end{bmatrix}$$</span><br /> <br /><span class="math display">$$p(\inputVector) = \gaussianDist{\inputVector}{\zerosVector}{\kernelMatrix\otimes \eye}$$</span><br /></p>
<h2 id="section-1"></h2>
<p>{Row Stacking</p>
<p>gpKalmanFilterKroneckerPlot3</p>
<p><br />
For this stacking the marginal distribution over the latent <em>dimensions</em> is given by the block diagonals.</p>
<object class="svgplot " data="../slides/diagrams/kern/kronecker_KI_highlighted001.svg" width style=" ">
</object>
<object class="svgplot " data="../slides/diagrams/kern/kronecker_KI_highlighted002.svg" width style=" ">
</object>
<object class="svgplot " data="../slides/diagrams/kern/kronecker_KI_highlighted003.svg" width style=" ">
</object>
<object class="svgplot " data="../slides/diagrams/kern/kronecker_KI_highlighted004.svg" width style=" ">
</object>
<object class="svgplot " data="../slides/diagrams/kern/kronecker_KI_highlighted005.svg" width style=" ">
</object>
<div class="figure">
<div id="kronecker-ki-highlighted-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/kern/kronecker_KI_highlighted005.svg" width="60%" style=" ">
</object>
</div>
<div id="kronecker-ki-highlighted-magnify" class="magnify" onclick="magnifyFigure(&#39;kronecker-ki-highlighted&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kronecker-ki-highlighted-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<h2 id="mapping-from-latent-process-to-observed">Mapping from Latent Process to Observed</h2>
<object class="svgplot " data="../slides/diagrams/kern/kronecker_KI.svg" width style=" ">
</object>
<p>gpKalmanFilterKroneckerPlot4</p>
<h2 id="observed-process">Observed Process</h2>
<p>The observations are related to the latent points by a linear mapping matrix, <br /><span class="math display">$$
\dataVector_{i, :} = \mappingMatrix\inputVector_{i, :} + \noiseVector_{i, :}
$$</span><br /> <br /><span class="math display">$$
\noiseVector \sim \gaussianSamp{\zerosVector}{\dataStd^2\eye}
$$</span><br /></p>
<div class="figure">
<div id="kronecker-wx-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/kern/kronecker_WX.svg" width="60%" style=" ">
</object>
</div>
<div id="kronecker-wx-magnify" class="magnify" onclick="magnifyFigure(&#39;kronecker-wx&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kronecker-wx-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<h2 id="output-covariance">Output Covariance</h2>
<p>This leads to a covariance of the form <br /><span class="math display">$$
(\eye\otimes \mappingMatrix) (\kernelMatrix \otimes \eye) (\eye \otimes \mappingMatrix^\top) + \eye\dataStd^2
$$</span><br /> Using <span class="math inline">(<strong>A</strong> ⊗ <strong>B</strong>)(<strong>C</strong> ⊗ <strong>D</strong>) = <strong>A</strong><strong>C</strong> ⊗ <strong>B</strong><strong>D</strong></span> This leads to <br /><span class="math display">$$
\kernelMatrix\otimes {\mappingMatrix}{\mappingMatrix}^\top + \eye\dataStd^2
$$</span><br /> or <br /><span class="math display">$$
\dataVector\sim \gaussianSamp{\zerosVector}{\mappingMatrix\mappingMatrix^\top \otimes \kernelMatrix + \eye\dataStd^2}
$$</span><br /></p>
<h2 id="kernels-for-vector-valued-outputs-a-review">Kernels for Vector Valued Outputs: A Review</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/multi-output-kernels.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/multi-output-kernels.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<h2 id="kronecker-structure-gps">Kronecker Structure GPs</h2>
<ul>
<li><p>This Kronecker structure leads to several published models. <br /><span class="math display">$$
(\kernelMatrix(\inputVector,\inputVector^\prime))_{\dataIndex,\dataIndex^\prime}=\kernelScalar(\inputVector,\inputVector^\prime)\kernelScalar_T(\dataIndex,\dataIndex^\prime),
$$</span><br /> where <span class="math inline">$\kernelScalar$</span> has <span class="math inline">$\inputVector$</span> and <span class="math inline">$\kernelScalar_T$</span> has <span class="math inline">$\numData$</span> as inputs.</p></li>
<li><p>Can think of multiple output covariance functions as covariances with augmented input.</p></li>
<li><p>Alongside <span class="math inline">$\inputVector$</span> we also input the <span class="math inline">$\dataIndex$</span> associated with the <em>output</em> of interest.</p></li>
</ul>
<h2 id="separable-covariance-functions">Separable Covariance Functions</h2>
<ul>
<li><p>Taking <span class="math inline">$\coregionalizationMatrix= {\mappingMatrix}{\mappingMatrix}^\top$</span> we have a matrix expression across outputs. <br /><span class="math display">$$\kernelMatrix(\inputVector,\inputVector^\prime)=\kernelScalar(\inputVector,\inputVector^\prime)\coregionalizationMatrix,$$</span><br /> where <span class="math inline">$\coregionalizationMatrix$</span> is a <span class="math inline">$\dataDim\times \dataDim$</span> symmetric and positive semi-definite matrix.</p></li>
<li><p><span class="math inline">$\coregionalizationMatrix$</span> is called the <em>coregionalization</em> matrix.</p></li>
<li><p>We call this class of covariance functions <em>separable</em> due to their product structure.</p></li>
</ul>
<h2 id="sum-of-separable-covariance-functions">Sum of Separable Covariance Functions</h2>
<ul>
<li><p>In the same spirit a more general class of kernels is given by <br /><span class="math display">$$\kernelMatrix(\inputVector,\inputVector^\prime)=\sum_{{j}=1}^\latentDim\kernelScalar_{j}(\inputVector,\inputVector^\prime)\coregionalizationMatrix_{j}.$$</span><br /></p></li>
<li><p>This can also be written as <br /><span class="math display">$$\kernelMatrix(\inputMatrix, \inputMatrix) = \sum_{{j}=1}^\latentDim\coregionalizationMatrix_{j}\otimes \kernelScalar_{j}(\inputMatrix, \inputMatrix),$$</span><br /></p></li>
<li><p>This is like several Kalman filter-type models added together, but each one with a different set of latent functions.</p></li>
<li><p>We call this class of kernels sum of separable kernels (SoS kernels).</p></li>
</ul>
<h2 id="geostatistics">Geostatistics</h2>
<ul>
<li><p>Use of GPs in Geostatistics is called kriging.</p></li>
<li><p>These multi-output GPs pioneered in geostatistics: prediction over vector-valued output data is known as <em>cokriging</em>.</p></li>
<li><p>The model in geostatistics is known as the <em>linear model of coregionalization</em> (LMC, <span class="citation" data-cites="Journel:miningBook78">Journel and Huijbregts (1978)</span> <span class="citation" data-cites="Goovaerts:book97">Goovaerts (1997)</span>).</p></li>
<li><p>Most machine learning multitask models can be placed in the context of the LMC model.</p></li>
</ul>
<h2 id="weighted-sum-of-latent-functions">Weighted sum of Latent Functions</h2>
<ul>
<li><p>In the linear model of coregionalization (LMC) outputs are expressed as linear combinations of independent random functions.</p></li>
<li><p>In the LMC, each component <span class="math inline">$\mappingFunction_\dataIndex$</span> is expressed as a linear sum <br /><span class="math display">$$\mappingFunction_\dataIndex(\inputVector) = \sum_{{j}=1}^\latentDim{w}_{\dataIndex,{j}}{u}_{{j}}(\inputVector).$$</span><br /> where the latent functions are independent and have covariance functions <span class="math inline">$\kernelScalar_{{j}}(\inputVector,\inputVector^\prime)$</span>.</p></li>
<li><p>The processes <span class="math inline">$\{\mappingFunction_j(\inputVector)\}_{j=1}^\latentDim$</span> are independent for <span class="math inline">$\latentDim \neq {j}^\prime$</span>.</p></li>
</ul>
<h2 id="kalman-filter-special-case">Kalman Filter Special Case</h2>
<ul>
<li><p>The Kalman filter is an example of the LMC where <span class="math inline">${u}_i(\inputVector) \rightarrow {x}_i(t)$</span>.</p></li>
<li><p>I.e. we’ve moved form time input to a more general input space.</p></li>
<li><p>In matrix notation:</p>
<ol type="1">
<li>Kalman filter <br /><span class="math display">$$\mappingFunctionMatrix= {\mappingMatrix}\inputMatrix$$</span><br /></li>
<li>LMC <br /><span class="math display">$$\mappingFunctionMatrix= {\mappingMatrix}{\mathbf{U}}$$</span><br /> where the rows of these matrices <span class="math inline">${\mappingFunctionMatrix}$</span>, <span class="math inline">$\inputMatrix$</span>, <span class="math inline"><strong>U</strong></span> each contain <span class="math inline">$\latentDim$</span> samples from their corresponding functions at a different time (Kalman filter) or spatial location (LMC).</li>
</ol></li>
</ul>
<h2 id="intrinsic-coregionalization-model">Intrinsic Coregionalization Model</h2>
<ul>
<li><p>If one covariance used for latent functions (like in Kalman filter).</p></li>
<li><p>This is called the intrinsic coregionalization model (ICM, <span class="citation" data-cites="Goovaerts:book97">Goovaerts (1997)</span>).</p></li>
<li><p>The kernel matrix corresponding to a dataset <span class="math inline">$\inputMatrix$</span> takes the form <br /><span class="math display">$$
\kernelMatrix(\inputMatrix, \inputMatrix) =  \coregionalizationMatrix\otimes \kernelScalar(\inputMatrix, \inputMatrix).
$$</span><br /></p></li>
</ul>
<h2 id="autokrigeability">Autokrigeability</h2>
<ul>
<li><p>If outputs are noise-free, maximum likelihood is equivalent to independent fits of <span class="math inline">$\coregionalizationMatrix$</span> and <span class="math inline">$\kernelScalar(\inputVector, \inputVector^\prime)$</span> <span class="citation" data-cites="Helterbrand:universalCR94">(Helterbrand and Cressie 1994)</span>.</p></li>
<li><p>In geostatistics this is known as autokrigeability <span class="citation" data-cites="Wackernagel:multivariate03">(Wackernagel 2003)</span>.</p></li>
<li><p>In multitask learning its the cancellation of intertask transfer <span class="citation" data-cites="Bonilla:multi07">(Bonilla, Chai, and Williams, n.d.)</span>.</p></li>
</ul>
<h2 id="intrinsic-coregionalization-model-1">Intrinsic Coregionalization Model</h2>
<p><br /><span class="math display">$$
\kernelMatrix(\inputMatrix, \inputMatrix) =  \mappingVector\mappingVector^\top  \otimes \kernelScalar(\inputMatrix, \inputMatrix).
$$</span><br /></p>
<p><br /><span class="math display">$$
\mappingVector= \begin{bmatrix} 1 \\ 5\end{bmatrix}
$$</span><br /> <br /><span class="math display">$$
\coregionalizationMatrix= \begin{bmatrix} 1 &amp; 5\\ 5&amp;25\end{bmatrix}
$$</span><br /></p>
<p><img src="../../../multigp/tex/diagrams/icmCovarianceImage" alt="image" /><img src="../../../multigp/tex/diagrams/icmCovarianceSample1" alt="image" /><img src="../../../multigp/tex/diagrams/icmCovarianceSample2" alt="image" /><img src="../../../multigp/tex/diagrams/icmCovarianceSample3" alt="image" /><img src="../../../multigp/tex/diagrams/icmCovarianceSample4" alt="image" /></p>
<h2 id="intrinsic-coregionalization-model-covariance">Intrinsic Coregionalization Model Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/icm-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/icm-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<center>
<br /><span class="math display">$$\kernelScalar(i, j, \inputVector, \inputVector^\prime) = b_{i,j} \kernelScalar(\inputVector, \inputVector^\prime)$$</span><br />
</center>
<div class="figure">
<div id="icm-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/icm_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/icm_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="icm-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;icm-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="icm-covariance-plot-caption" class="caption-frame">
<p>Figure: Intrinsic coregionalization model covariance function.</p>
</div>
</div>
<h2 id="intrinsic-coregionalization-model-2">Intrinsic Coregionalization Model</h2>
<p><br /><span class="math display">$$
\kernelMatrix(\inputMatrix, \inputMatrix) =  \coregionalizationMatrix\otimes \kernelScalar(\inputMatrix, \inputMatrix).
$$</span><br /></p>
<p><br /><span class="math display">$$
\coregionalizationMatrix= \begin{bmatrix} 1 &amp; 0.5\\ 0.5&amp; 1.5\end{bmatrix}
$$</span><br /></p>
<p><img src="../../../multigp/tex/diagrams/icm2CovarianceImage" alt="image" /><img src="../../../multigp/tex/diagrams/icm2CovarianceSample1" alt="image" /><img src="../../../multigp/tex/diagrams/icm2CovarianceSample2" alt="image" /><img src="../../../multigp/tex/diagrams/icm2CovarianceSample3" alt="image" /><img src="../../../multigp/tex/diagrams/icm2CovarianceSample4" alt="image" /></p>
<h2 id="lmc-samples">LMC Samples</h2>
<p><br /><span class="math display">$$\kernelMatrix(\inputMatrix, \inputMatrix) = \coregionalizationMatrix_1 \otimes \kernelScalar_1(\inputMatrix, \inputMatrix) + \coregionalizationMatrix_2 \otimes \kernelScalar_2(\inputMatrix, \inputMatrix)$$</span><br /></p>
<p><br /><span class="math display">$$\coregionalizationMatrix_1 = \begin{bmatrix} 1.4 &amp; 0.5\\ 0.5&amp; 1.2\end{bmatrix}$$</span><br /> <br /><span class="math display">ℓ<sub>1</sub> = 1</span><br /> <br /><span class="math display">$$\coregionalizationMatrix_2 = \begin{bmatrix} 1 &amp; 0.5\\ 0.5&amp; 1.3\end{bmatrix}$$</span><br /> <br /><span class="math display">ℓ<sub>2</sub> = 0.2</span><br /></p>
<p><img src="../../../multigp/tex/diagrams/lmc2CovarianceImage" alt="image" /><img src="../../../multigp/tex/diagrams/lmc2CovarianceSample1" alt="image" /><img src="../../../multigp/tex/diagrams/lmc2CovarianceSample2" alt="image" /><img src="../../../multigp/tex/diagrams/lmc2CovarianceSample3" alt="image" /><img src="../../../multigp/tex/diagrams/lmc2CovarianceSample4" alt="image" /></p>
<h2 id="lmc-in-machine-learning-and-statistics">LMC in Machine Learning and Statistics</h2>
<ul>
<li><p>Used in machine learning for GPs for multivariate regression and in statistics for computer emulation of expensive multivariate computer codes.</p></li>
<li><p>Imposes the correlation of the outputs explicitly through the set of coregionalization matrices.</p></li>
<li><p>Setting <span class="math inline">$\coregionalizationMatrix = \eye_\dataDim$</span> assumes outputs are conditionally independent given the parameters <span class="math inline">$\parameterVector$</span>. <span class="citation" data-cites="Minka:learningtolearn97 Lawrence:learning04 Kai:multitask05">(Minka and Picard 1997; Lawrence and Platt 2004; Yu, Tresp, and Schwaighofer 2005)</span>.</p></li>
<li><p>More recent approaches for multiple output modeling are different versions of the linear model of coregionalization.</p></li>
</ul>
<h2 id="semiparametric-latent-factor-model">Semiparametric Latent Factor Model</h2>
<ul>
<li><p>Coregionalization matrices are rank 1 <span class="citation" data-cites="Teh:semiparametric05">Teh, Seeger, and Jordan (n.d.)</span>. rewrite equation as <br /><span class="math display">$$\kernelMatrix(\inputMatrix, \inputMatrix) = \sum_{{j}=1}^\latentDim\mappingVector_{:, {j}}\mappingVector^{\top}_{:, {j}} \otimes \kernelScalar_{j}(\inputMatrix, \inputMatrix).$$</span><br /></p></li>
<li><p>Like the Kalman filter, but each latent function has a <em>different</em> covariance.</p></li>
<li><p>Authors suggest using an exponentiated quadratic characteristic length-scale for each input dimension.</p></li>
</ul>
<h2 id="semi-parametric-latent-factor-covariance">Semi Parametric Latent Factor Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/slfm-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/slfm-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> teaching_plots <span class="im">as</span> plot</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> mlai</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>K, anim<span class="op">=</span>plot.animate_covariance_function(mlai.compute_kernel, </span>
<span id="cb2-2"><a href="#cb2-2"></a>                                         kernel<span class="op">=</span>slfm_cov, subkernel<span class="op">=</span>eq_cov,</span>
<span id="cb2-3"><a href="#cb2-3"></a>                                         W <span class="op">=</span> np.asarray([[<span class="dv">1</span>],[<span class="dv">5</span>]])</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">from</span> IPython.core.display <span class="im">import</span> HTML</span></code></pre></div>
<h2 id="semiparametric-latent-factor-model-samples">Semiparametric Latent Factor Model Samples</h2>
<p><br /><span class="math display">$$
\kernelMatrix(\inputMatrix, \inputMatrix) = \mappingVector_{:, 1}\mappingVector_{:, 1}^\top \otimes \kernelScalar_1(\inputMatrix, \inputMatrix) + \mappingVector_{:, 2} \mappingVector_{:, 2}^\top \otimes \kernelScalar_2(\inputMatrix, \inputMatrix)
$$</span><br /></p>
<p><br /><span class="math display">$$
\mappingVector_1 = \begin{bmatrix} 0.5 \\ 1\end{bmatrix}
$$</span><br /> <br /><span class="math display">$$
\mappingVector_2 = \begin{bmatrix} 1 \\ 0.5\end{bmatrix}
$$</span><br /></p>
<p><img src="../../../multigp/tex/diagrams/slfmCovarianceImage" alt="image" /><img src="../../../multigp/tex/diagrams/slfmCovarianceSample1" alt="image" /><img src="../../../multigp/tex/diagrams/slfmCovarianceSample2" alt="image" /><img src="../../../multigp/tex/diagrams/slfmCovarianceSample3" alt="image" /><img src="../../../multigp/tex/diagrams/slfmCovarianceSample4" alt="image" /></p>
<h2 id="gaussian-processes-for-multi-task-multi-output-and-multi-class">Gaussian processes for Multi-task, Multi-output and Multi-class</h2>
<ul>
<li><p><span class="citation" data-cites="Bonilla:multi07">Bonilla, Chai, and Williams (n.d.)</span> suggest ICM for multitask learning.</p></li>
<li><p>Use a PPCA form for <span class="math inline">$\coregionalizationMatrix$</span>: similar to our Kalman filter example.</p></li>
<li><p>Refer to the autokrigeability effect as the cancellation of inter-task transfer.</p></li>
<li><p>Also discuss the similarities between the multi-task GP and the ICM, and its relationship to the SLFM and the LMC.</p></li>
</ul>
<h2 id="multitask-classification">Multitask Classification</h2>
<ul>
<li><p>Mostly restricted to the case where the outputs are conditionally independent given the hyperparameters <span class="math inline"><strong>ϕ</strong></span> <span class="citation" data-cites="Minka:learningtolearn97 Williams:multiclass98 Lawrence:learning04 Seeger:multiple04 Kai:multitask05 Rasmussen:book06">(Minka and Picard 1997; Williams and Barber 1998; Lawrence and Platt 2004; Seeger and Jordan 2004; Yu, Tresp, and Schwaighofer 2005; Rasmussen and Williams 2006)</span>.</p></li>
<li><p>Intrinsic coregionalization model has been used in the multiclass scenario. <span class="citation" data-cites="Skolidis:multiclass11">Skolidis and Sanguinetti (2011)</span> use the intrinsic coregionalization model for classification, by introducing a probit noise model as the likelihood.</p></li>
<li><p>Posterior distribution is no longer analytically tractable: approximate inference is required.</p></li>
</ul>
<h2 id="computer-emulation">Computer Emulation</h2>
<ul>
<li><p>A statistical model used as a surrogate for a computationally expensive computer model.</p></li>
<li><p><span class="citation" data-cites="Higdon:high08">Higdon et al. (2008)</span> use the linear model of coregionalization to model images representing the evolution of the implosion of steel cylinders.</p></li>
<li><p>In <span class="citation" data-cites="Conti:multi09">Conti and O’Hagan (2009)</span> use the ICM to model a vegetation model: called the Sheffield Dynamic Global Vegetation Model <span class="citation" data-cites="Woodward:vegetation98">Woodward, Lomas, and Betts (1998)</span>.</p></li>
</ul>
<h2 id="modelling-multiple-outputs">Modelling Multiple Outputs</h2>
<h2 id="running-example">Running Example</h2>
<h2 id="olympic-sprint-data">Olympic Sprint Data</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-sprint-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-sprint-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Sprints for Men and Women</li>
<li>100m, 200m, 400m</li>
<li>In early years of olympics not all events run.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/ml/100m_final_start.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a> by <a href="https://www.staff.ncl.ac.uk/d.j.wilkinson/">Darren Wilkinson</a></small>
</td>
</tr>
</table>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="op">%</span>pip install <span class="op">--</span>upgrade git<span class="op">+</span>https:<span class="op">//</span>github.com<span class="op">/</span>sods<span class="op">/</span>ods</span></code></pre></div>
<p>The first think we will look at is a multiple output model. Our aim is to jointly model all <em>sprinting</em> events from olympics since 1896. Data is provided by Rogers &amp; Girolami’s “First Course in Machine Learning”. Firstly, let’s load in the data.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">import</span> pods</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>pods.datasets.authorize_download <span class="op">=</span> <span class="kw">lambda</span> x: <span class="va">True</span> <span class="co"># prevents requesting authorization for download.</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>data <span class="op">=</span> pods.datasets.olympic_sprints()</span>
<span id="cb6-3"><a href="#cb6-3"></a>X <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb6-4"><a href="#cb6-4"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb6-5"><a href="#cb6-5"></a><span class="bu">print</span>(data[<span class="st">&#39;info&#39;</span>], data[<span class="st">&#39;details&#39;</span>])</span></code></pre></div>
<p>When using data sets it’s good practice to cite the originators of the data, you can get information about the source of the data from <code>data['citation']</code></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="bu">print</span>(data[<span class="st">&#39;citation&#39;</span>])</span></code></pre></div>
<p>The data consists of all the male and female sprinting data for 100m, 200m and 400m since 1896 (six outputs in total). The ouput information can be found from: <code>data['output_info']</code></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="bu">print</span>(data[<span class="st">&#39;output_info&#39;</span>])</span></code></pre></div>
<p>In <code>GPy</code> we deal with multiple output data in a particular way. We specify the output we are interested in for modelling as an additional <em>input</em>. So whilst for this data, normally, the only input would be the year of the event. We additionally have an input giving the index of the output we are modelling. This can be seen from examining <code>data['X']</code>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="bu">print</span>(<span class="st">&#39;First column of X contains the olympic years.&#39;</span>)</span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="bu">print</span>(data[<span class="st">&#39;X&#39;</span>][:, <span class="dv">0</span>])</span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="bu">print</span>(<span class="st">&#39;Second column of X contains the event index.&#39;</span>)</span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="bu">print</span>(data[<span class="st">&#39;X&#39;</span>][:, <span class="dv">1</span>])</span></code></pre></div>
<p>Now let’s plot the data</p>
<div class="figure">
<div id="olympic-sprint-data-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/olympic-sprint-data.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-sprint-data-magnify" class="magnify" onclick="magnifyFigure(&#39;olympic-sprint-data&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-sprint-data-caption" class="caption-frame">
<p>Figure: Olympic sprint gold medal winning times from <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span>.</p>
</div>
</div>
<p>In the plot above red is women’s events, blue is men’s. Squares are 400 m, crosses 200m and circles 100m. Not all events were run in all years, for example the women’s 400 m only started in 1964.</p>
<h2 id="gaussian-process-fit">Gaussian Process Fit</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/olympic-sprint-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/olympic-sprint-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>We will perform a multi-output Gaussian process fit to the data, we’ll do this using the <a href="https://github.com/SheffieldML/GPy">GPy software</a>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="op">%</span>pip install gpy</span></code></pre></div>
<p>We will look at modelling the data using coregionalization approaches described in this morning’s lecture. We introduced these approaches through the Kronecker product. To indicate we want to construct a covariance function of this type in GPy we’ve overloaded the <code>**</code> operator. Stricly speaking this operator means to the power of (like <code>^</code> in MATLAB). But for covariance functions we’ve used it to indicate a tensor product. The linear models of coregionalization we introduced in the lecture were all based on combining a matrix with a standard covariance function. We can think of the matrix as a particular type of covariance function, whose elements are referenced using the event indices. I.e. <span class="math inline"><em>k</em>(0, 0)</span> references the first row and column of the coregionalization matrix. <span class="math inline"><em>k</em>(1, 0)</span> references the second row and first column of the coregionalization matrix. Under this set up, we want to build a covariance where the first column from the features (the years) is passed to a covariance function, and the second column from the features (the event number) is passed to the coregionalisation matrix. Let’s start by trying a intrinsic coregionalisation model (sometimes known as multitask Gaussian processes). Let’s start by checking the help for the <code>Coregionalize</code> covariance.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="im">import</span> GPy</span></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>GPy.kern.Coregionalize?</span></code></pre></div>
<p>The coregionalize matrix, <span class="math inline"><strong>B</strong></span>, is itself is constructed from two other matrices, <span class="math inline"><strong>B</strong> = <strong>W</strong><strong>W</strong><sup>⊤</sup> + diag(<strong>κ</strong>)</span>. This allows us to specify a low rank form for the coregionalization matrix. However, for our first example we want to specify that the matrix <span class="math inline"><strong>B</strong></span> is not constrained to have a low rank form.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>kern <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, lengthscale<span class="op">=</span><span class="dv">80</span>)<span class="op">**</span>GPy.kern.Coregionalize(<span class="dv">1</span>,output_dim<span class="op">=</span><span class="dv">6</span>, rank<span class="op">=</span><span class="dv">5</span>)</span></code></pre></div>
<p>Note here that the rank we specify is that of the <span class="math inline"><strong>W</strong><strong>W</strong><sup>⊤</sup></span> part. When this part is combined with the diagonal matrix from <span class="math inline"><strong>κ</strong></span> the matrix <span class="math inline"><strong>B</strong></span> is totally general. This covariance function can now be used in a standard Gaussian process regression model. Let’s build the model and optimize it.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>model <span class="op">=</span> GPy.models.GPRegression(X, y, kern)</span>
<span id="cb14-2"><a href="#cb14-2"></a>model.optimize()</span></code></pre></div>
<p>We can plot the results using the ability to ‘fix inputs’ in the <code>model.plot()</code> function. We can specify that column 1 should be fixed to event number 2 by passing <code>fixed_inputs = [(1, 2)]</code> to the model. To plot the results for all events on the same figure we also specify <code>fignum=1</code> in the loop as below.</p>
<div class="figure">
<div id="olympic-sprint-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/olympic-sprint-gp.svg" width style=" ">
</object>
</div>
<div id="olympic-sprint-gp-magnify" class="magnify" onclick="magnifyFigure(&#39;olympic-sprint-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-sprint-gp-caption" class="caption-frame">
<p>Figure: Gaussian process fit to the Olympic Sprint data.</p>
</div>
</div>
<p>There is a lot we can do with this model. First of all, each of the races is a different length, so the series have a different mean. We can include another coregionalization term to deal with the mean. Below we do this and reduce the rank of the coregionalization matrix to 1.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>kern1 <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, lengthscale<span class="op">=</span><span class="dv">80</span>)<span class="op">**</span>GPy.kern.Coregionalize(<span class="dv">1</span>, output_dim<span class="op">=</span><span class="dv">6</span>, rank<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-2"><a href="#cb15-2"></a>kern2 <span class="op">=</span> GPy.kern.Bias(<span class="dv">1</span>)<span class="op">**</span>GPy.kern.Coregionalize(<span class="dv">1</span>,output_dim<span class="op">=</span><span class="dv">6</span>, rank<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-3"><a href="#cb15-3"></a>kern <span class="op">=</span> kern1 <span class="op">+</span> kern2</span></code></pre></div>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>model <span class="op">=</span> GPy.models.GPRegression(X, y, kern)</span>
<span id="cb16-2"><a href="#cb16-2"></a>model.optimize()</span></code></pre></div>
<div class="figure">
<div id="olympic-sprint-lmc-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/olympic-sprint-lmc-gp.svg" width style=" ">
</object>
</div>
<div id="olympic-sprint-lmc-gp-magnify" class="magnify" onclick="magnifyFigure(&#39;olympic-sprint-lmc-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-sprint-lmc-gp-caption" class="caption-frame">
<p>Figure: Gaussian process fit to the Olympic Sprint data.</p>
</div>
</div>
<p>This is a simple form of the linear model of coregionalization. Note how confident the model is about what the women’s 400 m performance would have been. You might feel that the model is being over confident in this region. Perhaps we are forcing too much sharing of information between the sprints. We could return to the intrinsic coregionalization model and force the two base covariance functions to share the same coregionalization matrix.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>kern1 <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, lengthscale<span class="op">=</span><span class="dv">80</span>) <span class="op">+</span> GPy.kern.Bias(<span class="dv">1</span>)</span>
<span id="cb17-2"><a href="#cb17-2"></a>kern2 <span class="op">=</span> GPy.kern.Coregionalize(<span class="dv">1</span>, output_dim<span class="op">=</span><span class="dv">6</span>, rank<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb17-3"><a href="#cb17-3"></a>kern <span class="op">=</span> kern1<span class="op">**</span>kern2</span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>model <span class="op">=</span> GPy.models.GPRegression(X, y, kern)</span>
<span id="cb18-2"><a href="#cb18-2"></a>model.optimize()</span></code></pre></div>
<div class="figure">
<div id="olympic-sprint-icm-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/olympic-sprint-icm-gp.svg" width style=" ">
</object>
</div>
<div id="olympic-sprint-icm-gp-magnify" class="magnify" onclick="magnifyFigure(&#39;olympic-sprint-icm-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-sprint-icm-gp-caption" class="caption-frame">
<p>Figure: Gaussian process fit to the Olympic Sprint data.</p>
</div>
</div>
<p>Predictions in the multioutput case can be very effected by our covariance function <em>design</em>. This reflects the themes we saw on the first day where the importance of covariance function choice was emphasized at design time.</p>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Bonilla:multi07">
<p>Bonilla, Edwin V., Kian Ming Chai, and Christopher K. I. Williams. n.d. “Multi-Task Gaussian Process Prediction.” In.</p>
</div>
<div id="ref-Conti:multi09">
<p>Conti, Stefano, and Anthony O’Hagan. 2009. “Bayesian Emulation of Complex Multi-Output and Dynamic Computer Models.” <em>Journal of Statistical Planning and Inference</em> 140 (3): 640–51. <a href="https://doi.org/doi:10.1016/j.jspi.2009.08.006">https://doi.org/doi:10.1016/j.jspi.2009.08.006</a>.</p>
</div>
<div id="ref-Goovaerts:book97">
<p>Goovaerts, Pierre. 1997. <em>Geostatistics For Natural Resources Evaluation</em>. Oxford University Press.</p>
</div>
<div id="ref-Helterbrand:universalCR94">
<p>Helterbrand, Jeffrey D., and Noel A. C. Cressie. 1994. “Universal Cokriging Under Intrinsic Coregionalization.” <em>Mathematical Geology</em> 26 (2): 205–26.</p>
</div>
<div id="ref-Higdon:high08">
<p>Higdon, David M., Jim Gattiker, Brian Williams, and Maria Rightley. 2008. “Computer Model Calibration Using High Dimensional Output.” <em>Journal of the American Statistical Association</em> 103 (482): 570–83.</p>
</div>
<div id="ref-Journel:miningBook78">
<p>Journel, Andre G., and Charles J. Huijbregts. 1978. <em>Mining Geostatistics</em>. London: Academic Press.</p>
</div>
<div id="ref-Lawrence:learning04">
<p>Lawrence, Neil D., and John C. Platt. 2004. “Learning to Learn with the Informative Vector Machine.” In, 512–19. <a href="https://doi.org/10.1145/1015330.1015382">https://doi.org/10.1145/1015330.1015382</a>.</p>
</div>
<div id="ref-Minka:learningtolearn97">
<p>Minka, Thomas P., and Rosalind W. Picard. 1997. “Learning How to Learn Is Learning with Point Sets.” Available on-line. <a href="http://research.microsoft.com/en-us/um/people/minka/papers/point-sets.html">http://research.microsoft.com/en-us/um/people/minka/papers/point-sets.html</a>.</p>
</div>
<div id="ref-Rasmussen:book06">
<p>Rasmussen, Carl Edward, and Christopher K. I. Williams. 2006. <em>Gaussian Processes for Machine Learning</em>. Cambridge, MA: mit.</p>
</div>
<div id="ref-Rogers:book11">
<p>Rogers, Simon, and Mark Girolami. 2011. <em>A First Course in Machine Learning</em>. CRC Press.</p>
</div>
<div id="ref-Seeger:multiple04">
<p>Seeger, Matthias, and Michael I. Jordan. 2004. “Sparse Gaussian Process Classification with Multiple Classes.” 661. Department of Statistics, University of California at Berkeley.</p>
</div>
<div id="ref-Skolidis:multiclass11">
<p>Skolidis, Grigorios, and Guido Sanguinetti. 2011. “Bayesian Multitask Classification with Gaussian Process Priors.” <em>IEEE Transactions on Neural Networks</em> 22 (12): 2011–21.</p>
</div>
<div id="ref-Teh:semiparametric05">
<p>Teh, Yee Whye, Matthias Seeger, and Michael I. Jordan. n.d. “Semiparametric Latent Factor Models.” In, 333–40.</p>
</div>
<div id="ref-Wackernagel:multivariate03">
<p>Wackernagel, Hans. 2003. <em>Multivariate Geostatistics: An Introduction with Applications</em>. 3rd ed. springer.</p>
</div>
<div id="ref-Williams:multiclass98">
<p>Williams, Christopher K. I., and David Barber. 1998. “Bayesian Classification with Gaussian Processes.” <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 20 (12): 1342–51.</p>
</div>
<div id="ref-Woodward:vegetation98">
<p>Woodward, Ian, Mark R. Lomas, and Richard A. Betts. 1998. “Vegetation-Climate Feedbacks in a Greenhouse World.” <em>Philosophical Transactions: Biological Sciences</em> 353 (1365): 29–39.</p>
</div>
<div id="ref-Kai:multitask05">
<p>Yu, Kai, Volker Tresp, and Anton Schwaighofer. 2005. “Learning Gaussian Processes from Multiple Tasks.” In <em>Proceedings of the 22nd International Conference on Machine Learning (Icml 2005)</em>, 1012–9.</p>
</div>
</div>

