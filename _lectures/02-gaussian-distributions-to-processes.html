---
title: "Gaussian Distributions to Processes"
venue: "Gaussian Process Summer School"
abstract: "<p>In this sesson we go from the Gaussian distribution to the Gaussian process and in doing so we move from a finite system to an infinite system.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: University of Cambridge
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orchid: 
time: "null"
week: 0
session: 2
reveal: 02-gaussian-distributions-to-processes.slides.html
ipynb: 02-gaussian-distributions-to-processes.ipynb
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h2 id="two-dimensional-gaussian">Two Dimensional Gaussian</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/two-d-gaussian.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/two-d-gaussian.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Consider the distribution of height (in meters) of an adult male human population. We will approximate the marginal density of heights as a Gaussian density with mean given by <span class="math inline">1.7m</span> and a standard deviation of <span class="math inline">0.15m</span>, implying a variance of <span class="math inline">$\dataStd^2=0.0225$</span>, <br /><span class="math display">$$
  p(h) \sim \gaussianSamp{1.7}{0.0225}.
  $$</span><br /> Similarly, we assume that weights of the population are distributed a Gaussian density with a mean of <span class="math inline">75kg</span> and a standard deviation of <span class="math inline">6<em>k</em><em>g</em></span> (implying a variance of 36), <br /><span class="math display">$$
  p(w) \sim \gaussianSamp{75}{36}.
  $$</span><br /></p>
<div class="figure">
<div id="height-weight-gaussian-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/height_weight_gaussian.svg" width="70%" style=" ">
</object>
</div>
<div id="height-weight-gaussian-magnify" class="magnify" onclick="magnifyFigure(&#39;height-weight-gaussian&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="height-weight-gaussian-caption" class="caption-frame">
<p>Figure: Gaussian distributions for height and weight.</p>
</div>
</div>
<h2 id="independence-assumption">Independence Assumption</h2>
<p>First of all, we make an independence assumption, we assume that height and weight are independent. The definition of probabilistic independence is that the joint density, <span class="math inline"><em>p</em>(<em>w</em>, <em>h</em>)</span>, factorizes into its marginal densities, <br /><span class="math display"><em>p</em>(<em>w</em>, <em>h</em>) = <em>p</em>(<em>w</em>)<em>p</em>(<em>h</em>).</span><br /> Given this assumption we can sample from the joint distribution by independently sampling weights and heights.</p>
<div class="figure">
<div id="independent-height-weight-7-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/independent_height_weight007.svg" width="70%" style=" ">
</object>
</div>
<div id="independent-height-weight-7-magnify" class="magnify" onclick="magnifyFigure(&#39;independent-height-weight-7&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="independent-height-weight-7-caption" class="caption-frame">
<p>Figure: Samples from independent Gaussian variables that might represent heights and weights.</p>
</div>
</div>
<p>In reality height and weight are <em>not</em> independent. Taller people tend on average to be heavier, and heavier people are likely to be taller. This is reflected by the <em>body mass index</em>. A ratio suggested by one of the fathers of statistics, Adolphe Quetelet. Quetelet was interested in the notion of the <em>average man</em> and collected various statistics about people. He defined the BMI to be, <br /><span class="math display">$$
\text{BMI} = \frac{w}{h^2}
$$</span><br />To deal with this dependence we now introduce the notion of <em>correlation</em> to the multivariate Gaussian density.</p>
<h2 id="sampling-two-dimensional-variables">Sampling Two Dimensional Variables</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/two-d-gaussian-correlated-sample.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/two-d-gaussian-correlated-sample.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="figure">
<div id="correlated-height-weight-7-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/correlated_height_weight007.svg" width="70%" style=" ">
</object>
</div>
<div id="correlated-height-weight-7-magnify" class="magnify" onclick="magnifyFigure(&#39;correlated-height-weight-7&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="correlated-height-weight-7-caption" class="caption-frame">
<p>Figure: Samples from <em>correlated</em> Gaussian variables that might represent heights and weights.</p>
</div>
</div>
<h2 id="independent-gaussians">Independent Gaussians</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/two-d-gaussian-maths.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/two-d-gaussian-maths.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p><br /><span class="math display"><em>p</em>(<em>w</em>, <em>h</em>) = <em>p</em>(<em>w</em>)<em>p</em>(<em>h</em>)</span><br /></p>
<p><br /><span class="math display">$$
p(w, h) = \frac{1}{\sqrt{2\pi \dataStd_1^2}\sqrt{2\pi\dataStd_2^2}} \exp\left(-\frac{1}{2}\left(\frac{(w-\meanScalar_1)^2}{\dataStd_1^2} + \frac{(h-\meanScalar_2)^2}{\dataStd_2^2}\right)\right)
$$</span><br /></p>
<p><br /><span class="math display">$$
p(w, h) = \frac{1}{\sqrt{2\pi\dataStd_1^22\pi\dataStd_2^2}} \exp\left(-\frac{1}{2}\left(\begin{bmatrix}w \\ h\end{bmatrix} - \begin{bmatrix}\meanScalar_1 \\ \meanScalar_2\end{bmatrix}\right)^\top\begin{bmatrix}\dataStd_1^2&amp; 0\\0&amp;\dataStd_2^2\end{bmatrix}^{-1}\left(\begin{bmatrix}w \\ h\end{bmatrix} - \begin{bmatrix}\meanScalar_1 \\ \meanScalar_2\end{bmatrix}\right)\right)
$$</span><br /></p>
<p><br /><span class="math display">$$
p(\dataVector) = \frac{1}{\det{2\pi \mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\mathbf{D}^{-1}(\dataVector - \meanVector)\right)
$$</span><br /></p>
<h2 id="correlated-gaussian">Correlated Gaussian</h2>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">$\rotationMatrix$</span>.</p>
<p><br /><span class="math display">$$
p(\dataVector) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\mathbf{D}^{-1}(\dataVector - \meanVector)\right)
$$</span><br /></p>
<p><br /><span class="math display">$$
p(\dataVector) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\rotationMatrix^\top\dataVector - \rotationMatrix^\top\meanVector)^\top\mathbf{D}^{-1}(\rotationMatrix^\top\dataVector - \rotationMatrix^\top\meanVector)\right)
$$</span><br /></p>
<p><br /><span class="math display">$$
p(\dataVector) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\rotationMatrix\mathbf{D}^{-1}\rotationMatrix^\top(\dataVector - \meanVector)\right)
$$</span><br /> this gives a covariance matrix: <br /><span class="math display">$$
\covarianceMatrix^{-1} = \rotationMatrix \mathbf{D}^{-1} \rotationMatrix^\top
$$</span><br /></p>
<p><br /><span class="math display">$$
p(\dataVector) = \frac{1}{\det{2\pi\covarianceMatrix}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\covarianceMatrix^{-1} (\dataVector - \meanVector)\right)
$$</span><br /> this gives a covariance matrix: <br /><span class="math display">$$
\covarianceMatrix = \rotationMatrix \mathbf{D} \rotationMatrix^\top
$$</span><br /></p>
<p>Let’s first of all review the properties of the multivariate Gaussian distribution that make linear Gaussian models easier to deal with. We’ll return to the, perhaps surprising, result on the parameters within the nonlinearity, <span class="math inline">$\parameterVector$</span>, shortly.</p>
<p>To work with linear Gaussian models, to find the marginal likelihood all you need to know is the following rules. If <br /><span class="math display">$$
\dataVector = \mappingMatrix \inputVector + \noiseVector,
$$</span><br /> where <span class="math inline">$\dataVector$</span>, <span class="math inline">$\inputVector$</span> and <span class="math inline">$\noiseVector$</span> are vectors and we assume that <span class="math inline">$\inputVector$</span> and <span class="math inline">$\noiseVector$</span> are drawn from multivariate Gaussians, <br /><span class="math display">$$
\begin{align}
\inputVector &amp; \sim \gaussianSamp{\meanVector}{\covarianceMatrix}\\
\noiseVector &amp; \sim \gaussianSamp{\zerosVector}{\covarianceMatrixTwo}
\end{align}
$$</span><br /> then we know that <span class="math inline">$\dataVector$</span> is also drawn from a multivariate Gaussian with, <br /><span class="math display">$$
\dataVector \sim \gaussianSamp{\mappingMatrix\meanVector}{\mappingMatrix\covarianceMatrix\mappingMatrix^\top + \covarianceMatrixTwo}.
$$</span><br /></p>
<p>With appropriately defined covariance, <span class="math inline">$\covarianceMatrixTwo$</span>, this is actually the marginal likelihood for Factor Analysis, or Probabilistic Principal Component Analysis <span class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop 1999)</span>, because we integrated out the inputs (or <em>latent</em> variables they would be called in that case).</p>
<h2 id="linear-model-overview">Linear Model Overview</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-model-overview.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-model-overview.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>However, we are focussing on what happens in models which are non-linear in the inputs, whereas the above would be <em>linear</em> in the inputs. To consider these, we introduce a matrix, called the design matrix. We set each activation function computed at each data point to be <br /><span class="math display">$$
\activationScalar_{i,j} = \activationScalar(\mappingVector^{(1)}_{j}, \inputVector_{i})
$$</span><br /> and define the matrix of activations (known as the <em>design matrix</em> in statistics) to be, <br /><span class="math display">$$
\activationMatrix = 
\begin{bmatrix}
\activationScalar_{1, 1} &amp; \activationScalar_{1, 2} &amp; \dots &amp; \activationScalar_{1, \numHidden} \\
\activationScalar_{1, 2} &amp; \activationScalar_{1, 2} &amp; \dots &amp; \activationScalar_{1, \numData} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\activationScalar_{\numData, 1} &amp; \activationScalar_{\numData, 2} &amp; \dots &amp; \activationScalar_{\numData, \numHidden}
\end{bmatrix}.
$$</span><br /> By convention this matrix always has <span class="math inline">$\numData$</span> rows and <span class="math inline">$\numHidden$</span> columns, now if we define the vector of all noise corruptions, <span class="math inline">$\noiseVector = \left[\noiseScalar_1, \dots \noiseScalar_\numData\right]^\top$</span>.</p>
<p>If we define the prior distribution over the vector <span class="math inline">$\mappingVector$</span> to be Gaussian, <br /><span class="math display">$$
\mappingVector \sim \gaussianSamp{\zerosVector}{\alpha\eye},
$$</span><br /> then we can use rules of multivariate Gaussians to see that, <br /><span class="math display">$$
\dataVector \sim \gaussianSamp{\zerosVector}{\alpha \activationMatrix \activationMatrix^\top + \dataStd^2 \eye}.
$$</span><br /></p>
<p>In other words, our training data is distributed as a multivariate Gaussian, with zero mean and a covariance given by <br /><span class="math display">$$
\kernelMatrix = \alpha \activationMatrix \activationMatrix^\top + \dataStd^2 \eye.
$$</span><br /></p>
<p>This is an <span class="math inline">$\numData \times \numData$</span> size matrix. Its elements are in the form of a function. The maths shows that any element, index by <span class="math inline"><em>i</em></span> and <span class="math inline"><em>j</em></span>, is a function <em>only</em> of inputs associated with data points <span class="math inline"><em>i</em></span> and <span class="math inline"><em>j</em></span>, <span class="math inline">$\dataVector_i$</span>, <span class="math inline">$\dataVector_j$</span>. <span class="math inline">$\kernel_{i,j} = \kernel\left(\inputVector_i, \inputVector_j\right)$</span></p>
<p>If we look at the portion of this function associated only with <span class="math inline">$\mappingFunction(\cdot)$</span>, i.e. we remove the noise, then we can write down the covariance associated with our neural network, <br /><span class="math display">$$
\kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) = \alpha \activationVector\left(\mappingMatrix_1, \inputVector_i\right)^\top \activationVector\left(\mappingMatrix_1, \inputVector_j\right)
$$</span><br /> so the elements of the covariance or <em>kernel</em> matrix are formed by inner products of the rows of the <em>design matrix</em>.</p>
<h2 id="gaussian-process">Gaussian Process</h2>
<p>This is the essence of a Gaussian process. Instead of making assumptions about our density over each data point, <span class="math inline">$\dataScalar_i$</span> as i.i.d. we make a joint Gaussian assumption over our data. The covariance matrix is now a function of both the parameters of the activation function, <span class="math inline">$\mappingMatrixTwo$</span>, and the input variables, <span class="math inline">$\inputMatrix$</span>. This comes about through integrating out the parameters of the model, <span class="math inline">$\mappingVector$</span>.</p>
<h2 id="basis-functions">Basis Functions</h2>
<p>We can basically put anything inside the basis functions, and many people do. These can be deep kernels <span class="citation" data-cites="Cho:deep09">(Cho and Saul 2009)</span> or we can learn the parameters of a convolutional neural network inside there.</p>
<p>Viewing a neural network in this way is also what allows us to beform sensible <em>batch</em> normalizations <span class="citation" data-cites="Ioffe:batch15">(Ioffe and Szegedy 2015)</span>.</p>
<h2 id="bayesian-inference-by-rejection-sampling">Bayesian Inference by Rejection Sampling</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-intro-very-short.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-intro-very-short.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>One view of Bayesian inference is to assume we are given a mechanism for generating samples, where we assume that mechanism is representing on accurate view on the way we believe the world works.</p>
<p>This mechanism is known as our <em>prior</em> belief.</p>
<p>We combine our prior belief with our observations of the real world by discarding all those samples that are inconsistent with our prior. The <em>likelihood</em> defines mathematically what we mean by inconsistent with the prior. The higher the noise level in the likelihood, the looser the notion of consistent.</p>
<p>The samples that remain are considered to be samples from the <em>posterior</em>.</p>
<p>This approach to Bayesian inference is closely related to two sampling techniques known as <em>rejection sampling</em> and <em>importance sampling</em>. It is realized in practice in an approach known as <em>approximate Bayesian computation</em> (ABC) or likelihood-free inference.</p>
<p>In practice, the algorithm is often too slow to be practical, because most samples will be inconsistent with the data and as a result the mechanism has to be operated many times to obtain a few posterior samples.</p>
<p>However, in the Gaussian process case, when the likelihood also assumes Gaussian noise, we can operate this mechanism mathematically, and obtain the posterior density <em>analytically</em>. This is the benefit of Gaussian processes.</p>
<p>First we will load in two python functions for computing the covariance function.</p>
<p>Next we sample from a multivariate normal density (a multivariate Gaussian), using the covariance function as the covariance matrix.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a>plot.rejection_samples(kernel<span class="op">=</span>kernel, </span>
<span id="cb1-2"><a href="#cb1-2"></a>    diagrams<span class="op">=</span><span class="st">&#39;../slides/diagrams/gp&#39;</span>)</span></code></pre></div>
<div class="figure">
<div id="gp-rejection-samples-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gp-rejection-samples-magnify" class="magnify" onclick="magnifyFigure(&#39;gp-rejection-samples&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-rejection-samples-caption" class="caption-frame">
<p>Figure: One view of Bayesian inference is we have a machine for generating samples (the <em>prior</em>), and we discard all samples inconsistent with our data, leaving the samples of interest (the <em>posterior</em>). This is a rejection sampling view of Bayesian inference. The Gaussian process allows us to do this analytically by multiplying the <em>prior</em> by the <em>likelihood</em>.</p>
</div>
</div>
<h2 id="sampling-a-function">Sampling a Function</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpdistfunc.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpdistfunc.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>We will consider a Gaussian distribution with a particular structure of covariance matrix. We will generate <em>one</em> sample from a 25-dimensional Gaussian density. <br /><span class="math display">$$
\mappingFunctionVector=\left[\mappingFunction_{1},\mappingFunction_{2}\dots \mappingFunction_{25}\right].
$$</span><br /> in the figure below we plot these data on the <span class="math inline"><em>y</em></span>-axis against their <em>indices</em> on the <span class="math inline"><em>x</em></span>-axis.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">from</span> mlai <span class="im">import</span> Kernel</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">from</span> mlai <span class="im">import</span> polynomial_cov</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="im">from</span> mlai <span class="im">import</span> exponentiated_quadratic</span></code></pre></div>
<div class="figure">
<div id="gp-two-point-sample-1-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample008.svg" width="80%" style=" ">
</object>
</div>
<div id="gp-two-point-sample-1-magnify" class="magnify" onclick="magnifyFigure(&#39;gp-two-point-sample-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-two-point-sample-1-caption" class="caption-frame">
<p>Figure: A 25 dimensional correlated random variable (values ploted against index)</p>
</div>
</div>
<h3 id="sampling-a-function-from-a-gaussian">Sampling a Function from a Gaussian</h3>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gaussian-predict-index-one-and-two.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gaussian-predict-index-one-and-two.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="figure">
<div id="two-point-sample-one-two-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample001.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-one-two-magnify" class="magnify" onclick="magnifyFigure(&#39;two-point-sample-one-two&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="two-point-sample-one-two-caption" class="caption-frame">
<p>Figure: The joint Gaussian over <span class="math inline">$\mappingFunction_1$</span> and <span class="math inline">$\mappingFunction_2$</span> along with the conditional distribution of <span class="math inline">$\mappingFunction_2$</span> given <span class="math inline">$\mappingFunction_1$</span></p>
</div>
</div>
<h3 id="joint-density-of-f_1-and-f_2">Joint Density of <span class="math inline"><em>f</em><sub>1</sub></span> and <span class="math inline"><em>f</em><sub>2</sub></span></h3>
<div class="figure">
<div id="two-point-sample-one-two-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample012.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-one-two-magnify" class="magnify" onclick="magnifyFigure(&#39;two-point-sample-one-two&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="two-point-sample-one-two-caption" class="caption-frame">
<p>Figure: The joint Gaussian over <span class="math inline">$\mappingFunction_1$</span> and <span class="math inline">$\mappingFunction_2$</span> along with the conditional distribution of <span class="math inline">$\mappingFunction_2$</span> given <span class="math inline">$\mappingFunction_1$</span></p>
</div>
</div>
<h2 id="uluru">Uluru</h2>
<div class="figure">
<div id="uluru-as-probability-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/gp/799px-Uluru_Panorama.jpg" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="uluru-as-probability-magnify" class="magnify" onclick="magnifyFigure(&#39;uluru-as-probability&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="uluru-as-probability-caption" class="caption-frame">
<p>Figure: Uluru, the sacred rock in Australia. If we think of it as a probability density, viewing it from this side gives us one <em>marginal</em> from the density. Figuratively speaking, slicing through the rock would give a conditional density.</p>
</div>
</div>
<p>When viewing these contour plots, I sometimes find it helpful to think of Uluru, the prominent rock formation in Australia. The rock rises above the surface of the plane, just like a probability density rising above the zero line. The rock is three dimensional, but when we view Uluru from the classical position, we are looking at one side of it. This is equivalent to viewing the marginal density.</p>
<p>The joint density can be viewed from above, using contours. The conditional density is equivalent to <em>slicing</em> the rock. Uluru is a holy rock, so this has to be an imaginary slice. Imagine we cut down a vertical plane orthogonal to our view point (e.g. coming across our view point). This would give a profile of the rock, which when renormalized, would give us the conditional distribution, the value of conditioning would be the location of the slice in the direction we are facing.</p>
<h2 id="prediction-with-correlated-gaussians">Prediction with Correlated Gaussians</h2>
<p>Of course in practice, rather than manipulating mountains physically, the advantage of the Gaussian density is that we can perform these manipulations mathematically.</p>
<p>Prediction of <span class="math inline">$\mappingFunction_2$</span> given <span class="math inline">$\mappingFunction_1$</span> requires the <em>conditional density</em>, <span class="math inline">$p(\mappingFunction_2|\mappingFunction_1)$</span>.Another remarkable property of the Gaussian density is that this conditional distribution is <em>also</em> guaranteed to be a Gaussian density. It has the form, <br /><span class="math display">$$
p(\mappingFunction_2|\mappingFunction_1) = \gaussianDist{\mappingFunction_2}{\frac{\kernelScalar_{1, 2}}{\kernelScalar_{1, 1}}\mappingFunction_1}{ \kernelScalar_{2, 2} - \frac{\kernelScalar_{1,2}^2}{\kernelScalar_{1,1}}}
$$</span><br />where we have assumed that the covariance of the original joint density was given by <br /><span class="math display">$$
\kernelMatrix = \begin{bmatrix} \kernelScalar_{1, 1} &amp; \kernelScalar_{1, 2}\\ \kernelScalar_{2, 1} &amp; \kernelScalar_{2, 2}.\end{bmatrix}
$$</span><br /></p>
<p>Using these formulae we can determine the conditional density for any of the elements of our vector <span class="math inline">$\mappingFunctionVector$</span>. For example, the variable <span class="math inline">$\mappingFunction_8$</span> is less correlated with <span class="math inline">$\mappingFunction_1$</span> than <span class="math inline">$\mappingFunction_2$</span>. If we consider this variable we see the conditional density is more diffuse.</p>
<h3 id="joint-density-of-f_1-and-f_8">Joint Density of <span class="math inline"><em>f</em><sub>1</sub></span> and <span class="math inline"><em>f</em><sub>8</sub></span></h3>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gaussian-predict-index-one-and-eight.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gaussian-predict-index-one-and-eight.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="figure">
<div id="two-point-sample-13-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample013.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-13-magnify" class="magnify" onclick="magnifyFigure(&#39;two-point-sample-13&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="two-point-sample-13-caption" class="caption-frame">
<p>Figure: Sample from the joint Gaussian model, points indexed by 1 and 8 highlighted.</p>
</div>
</div>
<h3 id="prediction-of-mappingfunction_8-from-mappingfunction_1">Prediction of <span class="math inline">$\mappingFunction_{8}$</span> from <span class="math inline">$\mappingFunction_{1}$</span></h3>
<div class="figure">
<div id="two-point-sample-one-eight-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample017.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-one-eight-magnify" class="magnify" onclick="magnifyFigure(&#39;two-point-sample-one-eight&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="two-point-sample-one-eight-caption" class="caption-frame">
<p>Figure: The joint Gaussian over <span class="math inline">$\mappingFunction_1$</span> and <span class="math inline">$\mappingFunction_8$</span> along with the conditional distribution of <span class="math inline">$\mappingFunction_8$</span> given <span class="math inline">$\mappingFunction_1$</span></p>
</div>
</div>
<h2 id="where-did-this-covariance-matrix-come-from">Where Did This Covariance Matrix Come From?</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/computing-rbf-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/computing-rbf-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p><br /><span class="math display">$$
k(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\left\Vert \inputVector - \inputVector^\prime\right\Vert^2_2}{2\lengthScale^2}\right)$$</span><br /></p>
<object class="svgplot " data="../slides/diagrams/kern/computing_eq_three_covariance016.svg" width style=" ">
</object>
<object class="svgplot " data="../slides/diagrams/kern/computing_eq_three_covariance016.svg" width style=" ">
</object>
<div class="caption" style="">
Figure: Entrywise fill in of the covariance matrix from the covariance function.
</div>
<object class="svgplot " data="../slides/diagrams/kern/computing_eq_four_covariance027.svg" width style=" ">
</object>
<object class="svgplot " data="../slides/diagrams/kern/computing_eq_four_covariance027.svg" width style=" ">
</object>
<div class="caption" style="">
Figure: Entrywise fill in of the covariance matrix from the covariance function.
</div>
<object class="svgplot " data="../slides/diagrams/kern/computing_eq_three_2_covariance016.svg" width style=" ">
</object>
<object class="svgplot " data="../slides/diagrams/kern/computing_eq_three_2_covariance016.svg" width style=" ">
</object>
<div class="caption" style="">
Figure: Entrywise fill in of the covariance matrix from the covariance function.
</div>
<h2 id="polynomial-covariance">Polynomial Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/poly-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/poly-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha(w \inputVector^\top\inputVector^\prime + b)^d$$</span><br />
</center>
<div class="figure">
<div id="polynomial-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/polynomial_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/polynomial_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="polynomial-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;polynomial-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="polynomial-covariance-plot-caption" class="caption-frame">
<p>Figure: Polynomial covariance function.</p>
</div>
</div>
<h2 id="degenerate-covariance-functions">Degenerate Covariance Functions</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/rbf-basis-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/rbf-basis-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Any linear basis function can also be incorporated into a covariance function. For example, an RBF network is a type of neural network with a set of radial basis functions. Meaning, the basis funciton is radially symmetric. These basis functions take the form, <br /><span class="math display">$$
\basisFunction_k(\inputScalar) = \exp\left(-\frac{\ltwoNorm{\inputScalar-\meanScalar_k}^{2}}{\lengthScale^{2}}\right).
$$</span><br /> Given a set of parameters, <br /><span class="math display">$$
\meanVector = \begin{bmatrix} -1 \\ 0 \\ 1\end{bmatrix},
$$</span><br /> we can construct the corresponding covariance function, which has the form, <br /><span class="math display">$$
\kernelScalar\left(\inputVals,\inputVals^{\prime}\right)=\alpha\basisVector(\inputVals)^\top \basisVector(\inputVals^\prime).
$$</span><br /></p>
<h2 id="basis-function-covariance">Basis Function Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/basis-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/basis-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The fixed basis function covariance just comes from the properties of a multivariate Gaussian, if we decide <br /><span class="math display">$$
\mappingFunctionVector=\basisMatrix\mappingVector
$$</span><br /> and then we assume <br /><span class="math display">$$
\mappingVector \sim \gaussianSamp{\zerosVector}{\alpha\eye}
$$</span><br /> then it follows from the properties of a multivariate Gaussian that <br /><span class="math display">$$
\mappingFunctionVector \sim \gaussianSamp{\zerosVector}{\alpha\basisMatrix\basisMatrix^\top}
$$</span><br /> meaning that the vector of observations from the function is jointly distributed as a Gaussian process and the covariance matrix is <span class="math inline">$\kernelMatrix = \alpha\basisMatrix \basisMatrix^\top$</span>, each element of the covariance matrix can then be found as the inner product between two rows of the basis funciton matrix.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="im">from</span> mlai <span class="im">import</span> basis_cov</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="im">from</span> mlai <span class="im">import</span> radial</span></code></pre></div>
<center>
<br /><span class="math display">$$\kernel(\inputVector, \inputVector^\prime) = \basisVector(\inputVector)^\top \basisVector(\inputVector^\prime)$$</span><br />
</center>
<div class="figure">
<div id="basis-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/basis_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/basis_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="basis-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;basis-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="basis-covariance-plot-caption" class="caption-frame">
<p>Figure: A covariance function based on a non-linear basis given by <span class="math inline">$\basisVector(\inputVector)$</span>.</p>
</div>
</div>
<h2 id="selecting-number-and-location-of-basis">Selecting Number and Location of Basis</h2>
<p>In practice for a basis function model we need to choose both 1. the location of the basis functions 2. the number of basis functions</p>
<p>One very clever of finessing this problem is to choose to have <em>infinite</em> basis functions and place them <em>everywhere</em>. To show how this is possible, we will consider a one dimensional system, <span class="math inline">$\inputScalar$</span>, which should give the intuition of how to do this. However, these ideas also extend to multidimensional systems as shown in, for example, <span class="citation" data-cites="Williams:infinite96">Williams (n.d.)</span> and <span class="citation" data-cites="Neal:thesis94">Neal (1994)</span>.</p>
<p>We consider a one dimensional set up with exponentiated quadratic basis functions, <br /><span class="math display">$$
\basisFunction_k(\inputScalar_i) = \exp\left(\frac{\ltwoNorm{\inputScalar_i - \locationScalar_k}^2}{2\rbfWidth^2}\right)
$$</span><br /></p>
<p>To place these basis functions, we first define the basis function centers in terms of a starting point on the left of our input, <span class="math inline"><em>a</em></span>, and a finishing point, <span class="math inline"><em>b</em></span>. The gap between basis is given by <span class="math inline">$\Delta\locationScalar$</span>. The location of each basis is then given by <br /><span class="math display">$$\locationScalar_k = a+\Delta\locationScalar\cdot (k-1).$$</span><br /> The covariance function can then be given as <br /><span class="math display">$$
\kernelScalar\left(\inputScalar_i,\inputScalar_j\right) = \sum_{k=1}^\numBasisFunc \basisFunction_k(\inputScalar_i)\basisFunction_k(\inputScalar_j)
$$</span><br /> <br /><span class="math display">$$\begin{aligned}
    \kernelScalar\left(\inputScalar_i,\inputScalar_j\right) = &amp;\alpha^\prime\Delta\locationScalar \sum_{k=1}^{\numBasisFunc} \exp\Bigg(
      -\frac{\inputScalar_i^2 + \inputScalar_j^2}{2\rbfWidth^2}\\ 
   &amp; - \frac{2\left(a+\Delta\locationScalar\cdot (k-1)\right)
       \left(\inputScalar_i+\inputScalar_j\right) + 2\left(a+\Delta\locationScalar \cdot (k-1)\right)^2}{2\rbfWidth^2} \Bigg)
  \end{aligned}$$</span><br /> where we’ve also scaled the variance of the process by <span class="math inline">$\Delta\locationScalar$</span>.</p>
<p>A consequence of our definition is that the first and last basis function locations are given by <br /><span class="math display">$$
  \locationScalar_1=a \ \text{and}\  \locationScalar_\numBasisFunc=b \ \text{so}\ b= a+ \Delta\locationScalar\cdot(\numBasisFunc-1)
  $$</span><br /> This implies that the distance between <span class="math inline"><em>b</em></span> and <span class="math inline"><em>a</em></span> is given by <br /><span class="math display">$$
  b-a = \Delta\locationScalar (\numBasisFunc -1)
  $$</span><br /> and since the basis functions are separated by <span class="math inline">$\Delta\locationScalar$</span> the number of basis functions is given by <br /><span class="math display">$$
  \numBasisFunc = \frac{b-a}{\Delta \locationScalar} + 1
  $$</span><br /> The next step is to take the limit as <span class="math inline">$\Delta\locationScalar\rightarrow 0$</span> so <span class="math inline">$\numBasisFunc \rightarrow \infty$</span> where we have used <span class="math inline">$a + k\cdot\Delta\locationScalar\rightarrow \locationScalar$</span>.</p>
<p>Performing the integration gives <br /><span class="math display">$$\begin{aligned}
    \kernelScalar(\inputScalar_i,&amp;\inputScalar_j) = \alpha^\prime \sqrt{\pi\rbfWidth^2}
    \exp\left( -\frac{\left(\inputScalar_i-\inputScalar_j\right)^2}{4\rbfWidth^2}\right)\\ &amp;\times
    \frac{1}{2}\left[\text{erf}\left(\frac{\left(b - \frac{1}{2}\left(\inputScalar_i +
    \inputScalar_j\right)\right)}{\rbfWidth} \right)-
    \text{erf}\left(\frac{\left(a - \frac{1}{2}\left(\inputScalar_i +
       \inputScalar_j\right)\right)}{\rbfWidth} \right)\right],
    \end{aligned}$$</span><br />Now we take the limit as <span class="math inline"><em>a</em> →  − ∞</span> and <span class="math inline"><em>b</em> → ∞</span> <br /><span class="math display">$$\kernelScalar\left(\inputScalar_i,\inputScalar_j\right) = \alpha\exp\left(
    -\frac{\left(\inputScalar_i-\inputScalar_j\right)^2}{4\rbfWidth^2}\right).$$</span><br /> where <span class="math inline">$\alpha=\alpha^\prime \sqrt{\pi\rbfWidth^2}$</span>.</p>
<p>In conclusion, an RBF model with infinite basis functions is a Gaussian process with the exponentiated quadratic covariance function <br /><span class="math display">$$\kernelScalar\left(\inputScalar_i,\inputScalar_j\right) = \alpha \exp\left(
          -\frac{\left(\inputScalar_i-\inputScalar_j\right)^2}{4\rbfWidth^2}\right).$$</span><br /></p>
<p>Note that while the functional form of the basis function and the covariance function are similar, in general if we repeated this analysis for other basis functions the covariance function will have a very different form. For example the error function, <span class="math inline">erf( ⋅ )</span>, results in an <span class="math inline">$\asin(\cdot)$</span> form. See <span class="citation" data-cites="Williams:infinite96">Williams (n.d.)</span> for more details.</p>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Cho:deep09">
<p>Cho, Youngmin, and Lawrence K. Saul. 2009. “Kernel Methods for Deep Learning.” In <em>Advances in Neural Information Processing Systems 22</em>, edited by Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta, 342–50. Curran Associates, Inc. <a href="http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf</a>.</p>
</div>
<div id="ref-Ioffe:batch15">
<p>Ioffe, Sergey, and Christian Szegedy. 2015. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” In <em>Proceedings of the 32nd International Conference on Machine Learning</em>, edited by Francis Bach and David Blei, 37:448–56. Proceedings of Machine Learning Research. Lille, France: PMLR. <a href="http://proceedings.mlr.press/v37/ioffe15.html">http://proceedings.mlr.press/v37/ioffe15.html</a>.</p>
</div>
<div id="ref-Neal:thesis94">
<p>Neal, Radford M. 1994. “Bayesian Learning for Neural Networks.” PhD thesis, University of Toronto, Canada. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf</a>.</p>
</div>
<div id="ref-Tipping:probpca99">
<p>Tipping, Michael E., and Christopher M. Bishop. 1999. “Probabilistic Principal Component Analysis.” <em>Journal of the Royal Statistical Society, B</em> 6 (3): 611–22. <a href="https://doi.org/doi:10.1111/1467-9868.00196">https://doi.org/doi:10.1111/1467-9868.00196</a>.</p>
</div>
<div id="ref-Williams:infinite96">
<p>Williams, Christopher K. I. n.d. “Computing with Infinite Networks.” In.</p>
</div>
</div>

