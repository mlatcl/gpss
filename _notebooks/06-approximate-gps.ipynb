{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.cell .markdown}\n",
    "\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPy: A Gaussian Process Framework in Python\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/gpy-software.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/gpy-software.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Gaussian processes are a flexible tool for non-parametric analysis with\n",
    "uncertainty. The GPy software was started in Sheffield to provide a easy\n",
    "to use interface to GPs. One which allowed the user to focus on the\n",
    "modelling rather than the mathematics.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/./slides/diagrams//gp/gpy.png\" style=\"width:70%\">\n",
    "\n",
    "Figure: <i>GPy is a BSD licensed software code base for implementing\n",
    "Gaussian process models in Python. It is designed for teaching and\n",
    "modelling. We welcome contributions which can be made through the GitHub\n",
    "repository <https://github.com/SheffieldML/GPy></i>\n",
    "\n",
    "GPy is a BSD licensed software code base for implementing Gaussian\n",
    "process models in python. This allows GPs to be combined with a wide\n",
    "variety of software libraries.\n",
    "\n",
    "The software itself is available on\n",
    "[GitHub](https://github.com/SheffieldML/GPy) and the team welcomes\n",
    "contributions.\n",
    "\n",
    "The aim for GPy is to be a probabilistic-style programming language,\n",
    "i.e., you specify the model rather than the algorithm. As well as a\n",
    "large range of covariance functions the software allows for non-Gaussian\n",
    "likelihoods, multivariate outputs, dimensionality reduction and\n",
    "approximations for larger data sets.\n",
    "\n",
    "The documentation for GPy can be found\n",
    "[here](https://gpy.readthedocs.io/en/latest/).\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/./slides/diagrams//sparse-gps.jpg\" style=\"width:45%\">\n",
    "\n",
    "Figure: <i>In recent years, approximations for Gaussian process models\n",
    "haven’t been the most fashionable approach to machine learning. Image\n",
    "credit: Kai Arulkumaran</i>\n",
    "\n",
    "Inference in a Gaussian process has computational complexity of\n",
    "$\\mathcal{O}(n^3)$ and storage demands of $\\mathcal{O}(n^2)$. This is\n",
    "too large for many modern data sets.\n",
    "\n",
    "Low rank approximations allow us to work with Gaussian processes with\n",
    "computational complexity of $\\mathcal{O}(nm^2)$ and storage demands of\n",
    "$\\mathcal{O}(nm)$, where $m$ is a user chosen parameter.\n",
    "\n",
    "In machine learning, low rank approximations date back to Smola and\n",
    "Bartlett (n.d.), Williams and Seeger (n.d.), who considered the Nyström\n",
    "approximation and Csató and Opper (2002);Csató (2002) who considered low\n",
    "rank approximations in the context of on-line learning. Selection of\n",
    "active points for the approximation was considered by Seeger et al.\n",
    "(n.d.) and Snelson and Ghahramani (n.d.) first proposed that the active\n",
    "set could be optimized directly. Those approaches were reviewed by\n",
    "Quiñonero Candela and Rasmussen (2005) under a unifying likelihood\n",
    "approximation perspective. General rules for deriving the maximum\n",
    "likelihood for these sparse approximations were given in Lawrence\n",
    "(n.d.).\n",
    "\n",
    "Modern variational interpretations of these low rank approaches were\n",
    "first explored in Titsias (n.d.). A more modern summary which considers\n",
    "each of these approximations as an $\\alpha$-divergence is given by Bui\n",
    "et al. (2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Compression\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-variational-complexity.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-variational-complexity.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Inducing variables are a compression of the real observations. The basic\n",
    "idea is can I create a new data set that summarizes all the information\n",
    "in the original data set. If this data set is smaller, I’ve compressed\n",
    "the information in the original data set.\n",
    "\n",
    "Inducing variables can be thought of as pseudo-data, indeed in Snelson\n",
    "and Ghahramani (n.d.) they were referred to as *pseudo-points*.\n",
    "\n",
    "The only requirement for inducing variables is that they are jointly\n",
    "distributed as a Gaussian process with the original data. This means\n",
    "that they can be from the space $\\mathbf{ f}$ or a space that is related\n",
    "through a linear operator (see e.g. Álvarez et al. (2010)). For example\n",
    "we could choose to store the gradient of the function at particular\n",
    "points or a value from the frequency spectrum of the function\n",
    "(Lázaro-Gredilla et al., 2010)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Compression II\n",
    "\n",
    "Inducing variables don’t only allow for the compression of the\n",
    "non-parameteric information into a reduced data aset but they also allow\n",
    "for computational scaling of the algorithms through, for example\n",
    "stochastic variational approaches Hensman et al. (n.d.) or\n",
    "parallelization Gal et al. (n.d.),Dai et al. (2014), Seeger et al.\n",
    "(2017).\n",
    "\n",
    "We’ve seen how we go from parametric to non-parametric. The limit\n",
    "implies infinite dimensional $\\mathbf{ w}$. Gaussian processes are\n",
    "generally non-parametric: combine data with covariance function to get\n",
    "model. This representation *cannot* be summarized by a parameter vector\n",
    "of a fixed size.\n",
    "\n",
    "Parametric models have a representation that does not respond to\n",
    "increasing training set size. Bayesian posterior distributions over\n",
    "parameters contain the information about the training data, for example\n",
    "if we use use Bayes’ rule from training data, $$\n",
    "p\\left(\\mathbf{ w}|\\mathbf{ y}, \\mathbf{X}\\right),\n",
    "$$ to make predictions on test data $$\n",
    "p\\left(y_*|\\mathbf{X}_*, \\mathbf{ y}, \\mathbf{X}\\right) = \\int\n",
    "              p\\left(y_*|\\mathbf{ w},\\mathbf{X}_*\\right)p\\left(\\mathbf{ w}|\\mathbf{ y},\n",
    "                \\mathbf{X})\\text{d}\\mathbf{ w}\\right)\n",
    "$$ then $\\mathbf{ w}$ becomes a bottleneck for information about the\n",
    "training set to pass to the test set. The solution is to increase $m$ so\n",
    "that the bottleneck is so large that it no longer presents a problem.\n",
    "How big is big enough for $m$? Non-parametrics says\n",
    "$m\\rightarrow \\infty$.\n",
    "\n",
    "Now no longer possible to manipulate the model through the standard\n",
    "parametric form. However, it *is* possible to express *parametric* as\n",
    "GPs: $$\n",
    "k\\left(\\mathbf{ x}_i,\\mathbf{ x}_j\\right)=\\phi_:\\left(\\mathbf{ x}_i\\right)^\\top\\phi_:\\left(\\mathbf{ x}_j\\right).\n",
    "$$ These are known as degenerate covariance matrices. Their rank is at\n",
    "most $m$, non-parametric models have full rank covariance matrices. Most\n",
    "well known is the “linear kernel,” $$\n",
    "k(\\mathbf{ x}_i, \\mathbf{ x}_j) = \\mathbf{ x}_i^\\top\\mathbf{ x}_j.\n",
    "$$ For non-parametrics prediction at a new point, $\\mathbf{ f}_*$, is\n",
    "made by conditioning on $\\mathbf{ f}$ in the joint distribution. In GPs\n",
    "this involves combining the training data with the covariance function\n",
    "and the mean function. Parametric is a special case when conditional\n",
    "prediction can be summarized in a *fixed* number of parameters.\n",
    "Complexity of parametric model remains fixed regardless of the size of\n",
    "our training data set. For a non-parametric model the required number of\n",
    "parameters grows with the size of the training data.\n",
    "\n",
    "-   Everything we want to do with a GP involves marginalising\n",
    "    $\\mathbf{ f}$\n",
    "    -   Predictions\n",
    "    -   Marginal likelihood\n",
    "    -   Estimating covariance parameters\n",
    "-   The posterior of $\\mathbf{ f}$ is the central object. This means\n",
    "    inverting $\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}$.\n",
    "\n",
    "<img class=\"negate\" src=\"https://inverseprobability.com/talks/./slides/diagrams//cov_approx.png\" style=\"width:90%\">\n",
    "\n",
    "Figure: Figure originally from presentation by Ed Snelson at NIPS\n",
    "\n",
    "The Nystr\"om approximation takes the form, $$\n",
    "\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}\\approx \\mathbf{Q}_{\\mathbf{ f}\\mathbf{ f}}= \\mathbf{K}_{\\mathbf{ f}\\mathbf{ u}}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}^{-1}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ f}}\n",
    "$$ The idea is that instead of inverting\n",
    "$\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}$, we make a low rank (or Nyström)\n",
    "approximation, and invert $\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}$ instead.\n",
    "\n",
    "In the original Nystr\"om method the columns to incorporate are sampled\n",
    "from the complete set of columns (without replacement). In a kernel\n",
    "matrix each of these columns corresponds to a data point. In the\n",
    "Nystr\"om method these points are sometimes called *landmark* points.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "$$\\mathbf{X},\\,\\mathbf{ y}$$\n",
    "\n",
    "</td>\n",
    "<td width=\"70%\">\n",
    "\n",
    "<img class=\"negate\" src=\"https://inverseprobability.com/talks/./slides/diagrams//nomenclature1.png\" style=\"width:90%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "$$\\mathbf{X},\\,\\mathbf{ y}$$\n",
    "$${\\color{blue} f(\\mathbf{ x})} \\sim {\\mathcal GP}$$\n",
    "\n",
    "</td>\n",
    "<td width=\"70%\">\n",
    "\n",
    "<img class=\"negate\" src=\"https://inverseprobability.com/talks/./slides/diagrams//nomenclature2.png\" style=\"width:90%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "$$\\mathbf{X},\\,\\mathbf{ y}$$\n",
    "$$f(\\mathbf{ x}) \\sim {\\mathcal GP}$$$$p({\\color{blue} \\mathbf{ f}}) = \\mathcal{N}\\left(\\mathbf{0},\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}\\right)$$\n",
    "\n",
    "</td>\n",
    "<td width=\"70%\">\n",
    "\n",
    "<img class=\"negate\" src=\"https://inverseprobability.com/talks/./slides/diagrams//nomenclature3.png\" style=\"width:90%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "$$\n",
    "\\mathbf{X},\\,\\mathbf{ y}$$ $$f(\\mathbf{ x}) \\sim {\\mathcal GP}\n",
    "$$ $$\n",
    "p(\\mathbf{ f}) = \\mathcal{N}\\left(\\mathbf{0},\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}\\right)\n",
    "$$ $$p( \\mathbf{ f}|\\mathbf{ y},\\mathbf{X})\n",
    "$$\n",
    "\n",
    "</td>\n",
    "<td width=\"70%\">\n",
    "\n",
    "<img class=\"negate\" src=\"https://inverseprobability.com/talks/./slides/diagrams//nomenclature3a.png\" style=\"width:90%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Take an extra $m$ points on the function, $\\mathbf{ u}= f(\\mathbf{Z})$.\n",
    "$$p(\\mathbf{ y},\\mathbf{ f},\\mathbf{ u}) = p(\\mathbf{ y}|\\mathbf{ f}) p(\\mathbf{ f}|\\mathbf{ u}) p(\\mathbf{ u})$$\n",
    "\n",
    "<img class=\"negate\" src=\"https://inverseprobability.com/talks/./slides/diagrams//cov_inducing_withX.png\" style=\"width:60%\">\n",
    "\n",
    "Take and extra $M$ points on the function, $\\mathbf{ u}= f(\\mathbf{Z})$.\n",
    "$$p(\\mathbf{ y},\\mathbf{ f},\\mathbf{ u}) = p(\\mathbf{ y}|\\mathbf{ f}) p(\\mathbf{ f}|\\mathbf{ u}) p(\\mathbf{ u})$$\n",
    "$$\\begin{aligned}\n",
    "    p(\\mathbf{ y}|\\mathbf{ f}) &= \\mathcal{N}\\left(\\mathbf{ y}|\\mathbf{ f},\\sigma^2 \\mathbf{I}\\right)\\\\\n",
    "    p(\\mathbf{ f}|\\mathbf{ u}) &= \\mathcal{N}\\left(\\mathbf{ f}| \\mathbf{K}_{\\mathbf{ f}\\mathbf{ u}}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}^{-1}\\mathbf{ u}, \\tilde{\\mathbf{K}}\\right)\\\\\n",
    "    p(\\mathbf{ u}) &= \\mathcal{N}\\left(\\mathbf{ u}| \\mathbf{0},\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}\\right)\n",
    "  \\end{aligned}$$\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "$$\\mathbf{X},\\,\\mathbf{ y}$$ $$f(\\mathbf{ x}) \\sim {\\mathcal GP}$$\n",
    "$$p(\\mathbf{ f}) = \\mathcal{N}\\left(\\mathbf{0},\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}\\right)$$\n",
    "$$p(\\mathbf{ f}|\\mathbf{ y},\\mathbf{X})$$\n",
    "\n",
    "</td>\n",
    "<td width=\"70%\">\n",
    "\n",
    "<img class=\"negate\" src=\"https://inverseprobability.com/talks/./slides/diagrams//nomenclature4\" style=\"width:90%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "                           &\\qquad\\mathbf{Z}, \\mathbf{ u}\\\\                      &p({\\color{red} \\mathbf{ u}})  = \\mathcal{N}\\left(\\mathbf{0},\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}\\right)\\end{align}\n",
    "$$\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "$$\\mathbf{X},\\,\\mathbf{ y}$$ $$f(\\mathbf{ x}) \\sim {\\mathcal GP}$$\n",
    "$$p(\\mathbf{ f}) = \\mathcal{N}\\left(\\mathbf{0},\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}\\right)$$\n",
    "$$p(\\mathbf{ f}|\\mathbf{ y},\\mathbf{X})$$\n",
    "$$p(\\mathbf{ u})  = \\mathcal{N}\\left(\\mathbf{0},\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}\\right)$$\n",
    "$$\\widetilde p({\\color{red}\\mathbf{ u}}|\\mathbf{ y},\\mathbf{X})$$\n",
    "\n",
    "</td>\n",
    "<td width=\"70%\">\n",
    "\n",
    "<img class=\"negate\" src=\"https://inverseprobability.com/talks/./slides/diagrams//nomenclature5.png\" style=\"width:90%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Instead of doing $$\n",
    "p(\\mathbf{ f}|\\mathbf{ y},\\mathbf{X}) = \\frac{p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f}|\\mathbf{X})}{\\int p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f}|\\mathbf{X}){\\text{d}\\mathbf{ f}}}\n",
    "$$ We’ll do $$\n",
    "p(\\mathbf{ u}|\\mathbf{ y},\\mathbf{Z}) = \\frac{p(\\mathbf{ y}|\\mathbf{ u})p(\\mathbf{ u}|\\mathbf{Z})}{\\int p(\\mathbf{ y}|\\mathbf{ u})p(\\mathbf{ u}|\\mathbf{Z}){\\text{d}\\mathbf{ u}}}\n",
    "$$\n",
    "\n",
    "<!--Flexible Parametric Approximation-->\n",
    "\n",
    "-   Date back to {Williams and Seeger (n.d.); Smola and Bartlett (n.d.);\n",
    "    Csató and Opper (2002); Seeger et al. (n.d.); Snelson and Ghahramani\n",
    "    (n.d.)}. See {Quiñonero Candela and Rasmussen (2005); Bui et\n",
    "    al. (2017)} for reviews.\n",
    "-   We follow variational perspective of {Titsias (n.d.)}.\n",
    "-   This is an augmented variable method, followed by a collapsed\n",
    "    variational approximation {King and Lawrence (n.d.); Hensman et\n",
    "    al. (2012)}.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"60%\">\n",
    "</td>\n",
    "<td width=\"40%\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Bound on $p(\\mathbf{ y}|\\mathbf{ u})$\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/larger-variational.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/larger-variational.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The conditional density of the data given the inducing points can be\n",
    "*lower* bounded variationally $$\n",
    "\\begin{aligned}\n",
    "    \\log p(\\mathbf{ y}|\\mathbf{ u}) & = \\log \\int p(\\mathbf{ y}|\\mathbf{ f}) p(\\mathbf{ f}|\\mathbf{ u}) \\text{d}\\mathbf{ f}\\\\ & = \\int q(\\mathbf{ f}) \\log \\frac{p(\\mathbf{ y}|\\mathbf{ f}) p(\\mathbf{ f}|\\mathbf{ u})}{q(\\mathbf{ f})}\\text{d}\\mathbf{ f}+ \\text{KL}\\left( q(\\mathbf{ f})\\,\\|\\,p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u}) \\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The key innovation from Titsias (n.d.) was to then make a particular\n",
    "choice for $q(\\mathbf{ f})$. If we set\n",
    "$q(\\mathbf{ f})=p(\\mathbf{ f}|\\mathbf{ u})$, $$\n",
    "  \\log p(\\mathbf{ y}|\\mathbf{ u}) \\geq \\int p(\\mathbf{ f}|\\mathbf{ u}) \\log p(\\mathbf{ y}|\\mathbf{ f})\\text{d}\\mathbf{ f}.\n",
    "  $$ $$\n",
    "  p(\\mathbf{ y}|\\mathbf{ u}) \\geq \\exp \\int p(\\mathbf{ f}|\\mathbf{ u}) \\log p(\\mathbf{ y}|\\mathbf{ f})\\text{d}\\mathbf{ f}.\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Compression in Inducing Variables\n",
    "\n",
    "Maximizing the lower bound minimizes the Kullback-Leibler divergence (or\n",
    "*information gain*) between our approximating density,\n",
    "$p(\\mathbf{ f}|\\mathbf{ u})$ and the true posterior density,\n",
    "$p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u})$.\n",
    "\n",
    "$$\n",
    "  \\text{KL}\\left( p(\\mathbf{ f}|\\mathbf{ u})\\,\\|\\,p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u}) \\right) = \\int p(\\mathbf{ f}|\\mathbf{ u}) \\log \\frac{p(\\mathbf{ f}|\\mathbf{ u})}{p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u})}\\text{d}\\mathbf{ u}\n",
    "  $$\n",
    "\n",
    "This bound is minimized when the information stored about $\\mathbf{ y}$\n",
    "is already stored in $\\mathbf{ u}$. In other words, maximizing the bound\n",
    "seeks an *optimal compression* from the *information gain* perspective.\n",
    "\n",
    "For the case where $\\mathbf{ u}= \\mathbf{ f}$ the bound is exact\n",
    "($\\mathbf{ f}$ $d$-separates $\\mathbf{ y}$ from $\\mathbf{ u}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of Inducing Variables\n",
    "\n",
    "The quality of the resulting bound is determined by the choice of the\n",
    "inducing variables. You are free to choose whichever heuristics you like\n",
    "for the inducing variables, as long as they are drawn jointly from a\n",
    "valid Gaussian process, i.e. such that $$\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{ f}\\\\\n",
    "\\mathbf{ u}\n",
    "\\end{bmatrix} \\sim \\mathcal{N}\\left(\\mathbf{0},\\mathbf{K}\\right)\n",
    "$$ where the kernel matrix itself can be decomposed into $$\n",
    "\\mathbf{K}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}& \\mathbf{K}_{\\mathbf{ f}\\mathbf{ u}}\\\\\n",
    "\\mathbf{K}_{\\mathbf{ u}\\mathbf{ f}}& \\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}\n",
    "\\end{bmatrix}\n",
    "$$ Choosing the inducing variables amounts to specifying\n",
    "$\\mathbf{K}_{\\mathbf{ f}\\mathbf{ u}}$ and\n",
    "$\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}$ such that $\\mathbf{K}$ remains\n",
    "positive definite. The typical choice is to choose $\\mathbf{ u}$ in the\n",
    "same domain as $\\mathbf{ f}$, associating each inducing output, $u_i$\n",
    "with a corresponding input location $\\mathbf{ z}$. However, more\n",
    "imaginative choices are absolutely possible. In particular, if\n",
    "$\\mathbf{ u}$ is related to $\\mathbf{ f}$ through a linear operator (see\n",
    "e.g. Álvarez et al. (2010)), then valid\n",
    "$\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}$ and\n",
    "$\\mathbf{K}_{\\mathbf{ u}\\mathbf{ f}}$ can be constructed. For example we\n",
    "could choose to store the gradient of the function at particular points\n",
    "or a value from the frequency spectrum of the function (Lázaro-Gredilla\n",
    "et al., 2010)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Compression II\n",
    "\n",
    "Inducing variables don’t only allow for the compression of the\n",
    "non-parameteric information into a reduced data set but they also allow\n",
    "for computational scaling of the algorithms through, for example\n",
    "stochastic variational approaches(Hensman et al., n.d.; Hoffman et al.,\n",
    "2012) or parallelization (Dai et al., 2014; Gal et al., n.d.; Seeger et\n",
    "al., 2017).\n",
    "\n",
    "-   If the likelihood, $p(\\mathbf{ y}|\\mathbf{ f})$, factorizes\n",
    "\n",
    "-   \\<8-\\> Then the bound factorizes.\n",
    "\n",
    "-   \\<10-\\> Now need a choice of distributions for $\\mathbf{ f}$ and\n",
    "    $\\mathbf{ y}|\\mathbf{ f}$ …\n",
    "\n",
    "-   Choose to go a different way.\n",
    "\n",
    "-   Introduce a set of auxiliary variables, $\\mathbf{ u}$, which are $m$\n",
    "    in length.\n",
    "\n",
    "-   They are like “artificial data.”\n",
    "\n",
    "-   Used to *induce* a distribution: $q(\\mathbf{ u}|\\mathbf{ y})$\n",
    "\n",
    "-   Introduce variable set which is *finite* dimensional. $$\n",
    "    p(\\mathbf{ y}^*|\\mathbf{ y}) \\approx \\int p(\\mathbf{ y}^*|\\mathbf{ u}) q(\\mathbf{ u}|\\mathbf{ y}) \\text{d}\\mathbf{ u}\n",
    "    $$\n",
    "\n",
    "-   But dimensionality of $\\mathbf{ u}$ can be changed to improve\n",
    "    approximation.\n",
    "\n",
    "-   Model for our data, $\\mathbf{ y}$\n",
    "\n",
    "    <table>\n",
    "    <tr>\n",
    "    <td width>\n",
    "\n",
    "    $$p(\\mathbf{ y})$$\n",
    "\n",
    "    </td>\n",
    "    <td width>\n",
    "\n",
    "    <img src=\"https://inverseprobability.com/talks/./slides/diagrams//gp/py.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "    </td>\n",
    "    </tr>\n",
    "    </table>\n",
    "\n",
    "-   Prior density over $\\mathbf{ f}$. Likelihood relates data,\n",
    "    $\\mathbf{ y}$, to $\\mathbf{ f}$.\n",
    "\n",
    "    <table>\n",
    "    <tr>\n",
    "    <td width>\n",
    "\n",
    "    $$p(\\mathbf{ y})=\\int p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f})\\text{d}\\mathbf{ f}$$\n",
    "\n",
    "    </td>\n",
    "    <td width>\n",
    "\n",
    "    <img src=\"https://inverseprobability.com/talks/./slides/diagrams//gp/pygfpf.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "    </td>\n",
    "    </tr>\n",
    "    </table>\n",
    "\n",
    "-   Prior density over $\\mathbf{ f}$. Likelihood relates data,\n",
    "    $\\mathbf{ y}$, to $\\mathbf{ f}$.\n",
    "\n",
    "    <table>\n",
    "    <tr>\n",
    "    <td width>\n",
    "\n",
    "    $$p(\\mathbf{ y})=\\int p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ u}|\\mathbf{ f})p(\\mathbf{ f})\\text{d}\\mathbf{ f}\\text{d}\\mathbf{ u}$$\n",
    "\n",
    "    </td>\n",
    "    <td width>\n",
    "\n",
    "    <img src=\"https://inverseprobability.com/talks/./slides/diagrams//gp/pygfpugfpf.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "    </td>\n",
    "    </tr>\n",
    "    </table>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width>\n",
    "\n",
    "$$p(\\mathbf{ y})=\\int \\int p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f}|\\mathbf{ u})\\text{d}\\mathbf{ f}p(\\mathbf{ u})\\text{d}\\mathbf{ u}$$\n",
    "\n",
    "</td>\n",
    "<td width>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/./slides/diagrams//gp/pygfpfgupu.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width>\n",
    "\n",
    "$$p(\\mathbf{ y})=\\int \\int p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f}|\\mathbf{ u})\\text{d}\\mathbf{ f}p(\\mathbf{ u})\\text{d}\\mathbf{ u}$$\n",
    "\n",
    "</td>\n",
    "<td width>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/./slides/diagrams//gp/pygfpfgupu2.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width>\n",
    "\n",
    "$$p(\\mathbf{ y}|\\mathbf{ u})=\\int p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f}|\\mathbf{ u})\\text{d}\\mathbf{ f}$$\n",
    "\n",
    "</td>\n",
    "<td width>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/./slides/diagrams//gp/pygfpfgu.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width>\n",
    "\n",
    "$$p(\\mathbf{ y}|\\mathbf{ u})$$\n",
    "\n",
    "</td>\n",
    "<td width>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/./slides/diagrams//gp/pygu.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width>\n",
    "\n",
    "$$p(\\mathbf{ y}|\\boldsymbol{ \\theta})$$\n",
    "\n",
    "</td>\n",
    "<td width>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/./slides/diagrams//gp/pygtheta.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "-   Replace true $p(\\mathbf{ u}|\\mathbf{ y})$ with approximation\n",
    "    $q(\\mathbf{ u}|\\mathbf{ y})$.\n",
    "-   Minimize KL divergence between approximation and truth.\n",
    "-   This is similar to the Bayesian posterior distribution.\n",
    "-   But it’s placed over a set of ‘pseudo-observations.’\n",
    "\n",
    "$$\\mathbf{ f}, \\mathbf{ u}\\sim \\mathcal{N}\\left(\\mathbf{0},\\begin{bmatrix}\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}& \\mathbf{K}_{\\mathbf{ f}\\mathbf{ u}}\\\\\\mathbf{K}_{\\mathbf{ u}\\mathbf{ f}}& \\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}\\end{bmatrix}\\right)$$\n",
    "$$\\mathbf{ y}|\\mathbf{ f}= \\prod_{i} \\mathcal{N}\\left(f,\\sigma^2\\right)$$\n",
    "\n",
    "<!--Variational Compression-->\n",
    "\n",
    "For Gaussian likelihoods:\n",
    "\n",
    "Define:\n",
    "$$q_{i, i} = \\text{var}_{p(f_i|\\mathbf{ u})}\\left( f_i \\right) = \\left\\langle f_i^2\\right\\rangle_{p(f_i|\\mathbf{ u})} - \\left\\langle f_i\\right\\rangle_{p(f_i|\\mathbf{ u})}^2$$\n",
    "We can write: $$c_i = \\exp\\left(-{\\frac{q_{i,i}}{2\\sigma^2}}\\right)$$ If\n",
    "joint distribution of $p(\\mathbf{ f}, \\mathbf{ u})$ is Gaussian then:\n",
    "$$q_{i, i} = k_{i, i} - \\mathbf{ k}_{i, \\mathbf{ u}}^\\top \\mathbf{K}_{\\mathbf{ u}, \\mathbf{ u}}^{-1} \\mathbf{ k}_{i, \\mathbf{ u}}$$\n",
    "\n",
    "$c_i$ is not a function of $\\mathbf{ u}$ but *is* a function of\n",
    "$\\mathbf{X}_\\mathbf{ u}$.\n",
    "\n",
    "<!--frame end-->\n",
    "<!--frame start-->\n",
    "\n",
    "-   The sum of $q_{i,i}$ is the *total conditional variance*.\n",
    "\n",
    "-   If conditional density $p(\\mathbf{ f}|\\mathbf{ u})$ is Gaussian then\n",
    "    it has covariance\n",
    "    $$\\mathbf{Q} = \\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}} - \\mathbf{K}_{\\mathbf{ f}\\mathbf{ u}}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}^{-1} \\mathbf{K}_{\\mathbf{ u}\\mathbf{ f}}$$\n",
    "\n",
    "-   $\\text{tr}\\left(\\mathbf{Q}\\right) = \\sum_{i}q_{i,i}$ is known as\n",
    "    total variance.\n",
    "\n",
    "-   Because it is on conditional distribution we call it *total\n",
    "    conditional variance*.\n",
    "\n",
    "<!--frame end-->\n",
    "<!--frame start-->\n",
    "\n",
    "-   Measure the ‘capacity of a density.’\n",
    "\n",
    "-   Determinant of covariance represents ‘volume’ of density.\n",
    "\n",
    "-   log determinant is entropy: sum of *log* eigenvalues of covariance.\n",
    "\n",
    "-   trace of covariance is total variance: sum of eigenvalues of\n",
    "    covariance.\n",
    "\n",
    "-   $\\lambda > \\log \\lambda$ then total conditional variance upper\n",
    "    bounds entropy.\n",
    "\n",
    "<!--frame end-->\n",
    "<!--frame start-->\n",
    "\n",
    "Exponentiated total variance bounds determinant.\n",
    "$$\\det{\\mathbf{Q}} < \\exp \\text{tr}\\left(\\mathbf{Q}\\right)$$ Because\n",
    "$$\\prod_{i=1}^k \\lambda_i < \\prod_{i=1}^k \\exp(\\lambda_i)$$ where\n",
    "$\\{\\lambda_i\\}_{i=1}^k$ are the *positive* eigenvalues of $\\mathbf{Q}$\n",
    "This in turn implies\n",
    "$$\\det{\\mathbf{Q}} < \\prod_{i=1}^k \\exp\\left(q_{i,i}\\right)$$\n",
    "\n",
    "<!--frame end-->\n",
    "<!--frame start-->\n",
    "\n",
    "-   Conditional density $p(\\mathbf{ f}|\\mathbf{ u})$ can be seen as a\n",
    "    *communication channel*.\n",
    "\n",
    "-   Normally we have:\n",
    "    $$\\text{Transmitter} \\stackrel{\\mathbf{ u}}{\\rightarrow} \\begin{smallmatrix}p(\\mathbf{ f}|\\mathbf{ u}) \\\\ \\text{Channel}\\end{smallmatrix} \\stackrel{\\mathbf{ f}}{\\rightarrow} \\text{Receiver}$$\n",
    "    and we control $p(\\mathbf{ u})$ (the source density).\n",
    "\n",
    "-   *Here* we can also control the transmission channel\n",
    "    $p(\\mathbf{ f}|\\mathbf{ u})$.\n",
    "\n",
    "<!--frame end-->\n",
    "<!--frame start-->\n",
    "\n",
    "Substitute variational bound into marginal likelihood:\n",
    "$$p(\\mathbf{ y})\\geq \\prod_{i=1}^nc_i \\int \\mathcal{N}\\left(\\mathbf{ y}|\\left\\langle\\mathbf{ f}\\right\\rangle,\\sigma^2\\mathbf{I}\\right)p(\\mathbf{ u}) \\text{d}\\mathbf{ u}$$\n",
    "Note that:\n",
    "$$\\left\\langle\\mathbf{ f}\\right\\rangle_{p(\\mathbf{ f}|\\mathbf{ u})} = \\mathbf{K}_{\\mathbf{ f}, \\mathbf{ u}} \\mathbf{K}_{\\mathbf{ u}, \\mathbf{ u}}^{-1}\\mathbf{ u}$$\n",
    "is *linearly* dependent on $\\mathbf{ u}$.\n",
    "\n",
    "<!--frame end-->\n",
    "<!--frame start-->\n",
    "\n",
    "Making the marginalization of $\\mathbf{ u}$ straightforward. In the\n",
    "Gaussian case:\n",
    "$$p(\\mathbf{ u}) = \\mathcal{N}\\left(\\mathbf{ u}|\\mathbf{0},\\mathbf{K}_{\\mathbf{ u},\\mathbf{ u}}\\right)$$\n",
    "\n",
    "<!--frame end-->\n",
    "\n",
    "$$\\log p(\\mathbf{ y}|\\mathbf{ u}) = \\log\\int p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f}|\\mathbf{ u},\\mathbf{X})\\text{d}\\mathbf{ f}$$\n",
    "\n",
    "$$\\log p(\\mathbf{ y}|\\mathbf{ u}) = \\log \\mathbb{E}_{p(\\mathbf{ f}|\\mathbf{ u},\\mathbf{X})}\\left[p(\\mathbf{ y}|\\mathbf{ f})\\right]$$\n",
    "$$\\log p(\\mathbf{ y}|\\mathbf{ u}) \\geq  \\mathbb{E}_{p(\\mathbf{ f}|\\mathbf{ u},\\mathbf{X})}\\left[\\log p(\\mathbf{ y}|\\mathbf{ f})\\right]\\triangleq \\log\\widetilde p(\\mathbf{ y}|\\mathbf{ u})$$\n",
    "\n",
    "No inversion of $\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}$ required\n",
    "\n",
    "Titsias (n.d.)\n",
    "\n",
    "$$p(\\mathbf{ y}|\\mathbf{ u}) = \\frac{p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f}|\\mathbf{ u})}{p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u})}$$\n",
    "$$\\log p(\\mathbf{ y}|\\mathbf{ u}) = \\log p(\\mathbf{ y}|\\mathbf{ f}) + \\log \\frac{p(\\mathbf{ f}|\\mathbf{ u})}{p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u})}$$\n",
    "$$\\log p(\\mathbf{ y}|\\mathbf{ u}) = \\bbE_{p(\\mathbf{ f}|\\mathbf{ u})}\\big[\\log p(\\mathbf{ y}|\\mathbf{ f})\\big] + \\bbE_{p(\\mathbf{ f}|\\mathbf{ u})}\\big[\\log \\frac{p(\\mathbf{ f}|\\mathbf{ u})}{p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u})}\\big]$$\n",
    "$$\\log p(\\mathbf{ y}|\\mathbf{ u}) = \\widetilde p(\\mathbf{ y}|\\mathbf{ u}) + \\textsc{KL}[p(\\mathbf{ f}|\\mathbf{ u})||p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u})]$$\n",
    "\n",
    "No inversion of $\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}$ required\n",
    "\n",
    "$$\\widetilde p(\\mathbf{ y}|\\mathbf{ u})  = \\prod_{i=1}^n\\widetilde p(y_i|\\mathbf{ u})$$\n",
    "$$\\widetilde p(y|\\mathbf{ u}) = \\mathcal{N}\\left(y|\\mathbf{ k}_{fu}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}^{-1}\\mathbf{ u},\\sigma^2\\right) \\,{\\color{red}\\exp\\left\\{-\\tfrac{1}{2\\sigma^2}\\left(k_{ff}- \\mathbf{ k}_{fu}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}^{-1}\\mathbf{ k}_{uf}\\right)\\right\\}}$$\n",
    "\n",
    "A straightforward likelihood approximation, and a penalty term\n",
    "\n",
    "$$\\widetilde p(\\mathbf{ u}|\\mathbf{ y},\\mathbf{Z}) = \\frac{\\widetilde p(\\mathbf{ y}|\\mathbf{ u})p(\\mathbf{ u}|\\mathbf{Z})}{\\int \\widetilde p(\\mathbf{ y}|\\mathbf{ u})p(\\mathbf{ u}|\\mathbf{Z})\\text{d}{\\mathbf{ u}}}$$\n",
    "\n",
    "-   Computing the posterior costs $\\mathcal{O}(nm^2)$\n",
    "\n",
    "-   We also get a lower bound of the marginal likelihood\n",
    "\n",
    "$${\\color{red}\\sum_{i=1}^n-\\tfrac{1}{2\\sigma^2}\\left(k_{ff}- \\mathbf{ k}_{fu}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}^{-1}\\mathbf{ k}_{uf}\\right)}$$\n",
    "\n",
    "$${\\color{red}\\sum_{i=1}^n-\\tfrac{1}{2\\sigma^2}\\left(k_{ff}- \\mathbf{ k}_{fu}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}^{-1}\\mathbf{ k}_{uf}\\right)}$$\n",
    "\n",
    "<!--![image](../../../gp/tex/diagrams/cov_approx){width=\"60.00000%\"}-->\n",
    "<!--![image](../../../gp/tex/diagrams/cov_approx_opt){width=\"60.00000%\"}-->\n",
    "\n",
    "It’s easy to show that as $\\mathbf{Z}\\to \\mathbf{X}$:\n",
    "\n",
    "-   $\\mathbf{ u}\\to \\mathbf{ f}$ (and the posterior is exact)\n",
    "\n",
    "-   The penalty term is zero.\n",
    "\n",
    "-   The cost returns to $\\mathcal{O}(n^3)$\n",
    "\n",
    "-     \n",
    "\n",
    "-   \n",
    "\n",
    "So far we:\n",
    "\n",
    "-   introduced $\\mathbf{Z}, \\mathbf{ u}$\n",
    "\n",
    "-   approximated the intergral over $\\mathbf{ f}$ variationally\n",
    "\n",
    "-   captured the information in $\\widetilde p(\\mathbf{ u}|\\mathbf{ y})$\n",
    "\n",
    "-   obtained a lower bound on the marginal likeihood\n",
    "\n",
    "-   saw the effect of the penalty term\n",
    "\n",
    "-   prediction for new points\n",
    "\n",
    "Omitted details:\n",
    "\n",
    "-   optimization of the covariance parameters using the bound\n",
    "\n",
    "-   optimization of Z (simultaneously)\n",
    "\n",
    "-   the form of $\\widetilde p(\\mathbf{ u}|\\mathbf{ y})$\n",
    "\n",
    "-   historical approximations\n",
    "\n",
    "Subset selection\n",
    "\n",
    "Lawrence et al. (n.d.)\n",
    "\n",
    "-   Random or systematic\n",
    "\n",
    "-   Set $\\mathbf{Z}$ to subset of $\\mathbf{X}$\n",
    "\n",
    "-   Set $\\mathbf{ u}$ to subset of $\\mathbf{ f}$\n",
    "\n",
    "-   Approximation to $p(\\mathbf{ y}|\\mathbf{ u})$:\n",
    "\n",
    "    -   \\$ p(\\_i) = p(\\_i\\_i) i\\$\n",
    "\n",
    "    -   \\$ p(\\_i) = 1 i\\$\n",
    "\n",
    "Quiñonero Candela and Rasmussen (2005)\n",
    "\n",
    "{Deterministic Training Conditional (DTC)}\n",
    "\n",
    "-   Approximation to $p(\\mathbf{ y}|\\mathbf{ u})$:\n",
    "\n",
    "    -   \\$ p(\\_i) = (\\_i, \\[\\_i\\])\\$\n",
    "\n",
    "-   As our variational formulation, but without penalty\n",
    "\n",
    "Optimization of $\\mathbf{Z}$ is difficult\n",
    "\n",
    "Fully Independent Training Conditional\n",
    "\n",
    "Snelson and Ghahramani (n.d.)\n",
    "\n",
    "-   Approximation to $p(\\mathbf{ y}|\\mathbf{ u})$:\n",
    "\n",
    "-   \\$ p() = \\_i p(\\_i) \\$\n",
    "\n",
    "Optimization of $\\mathbf{Z}$ is still difficult, and there are some\n",
    "weird heteroscedatic effects\n",
    "\n",
    "-   GP-LVM Provides probabilistic non-linear dimensionality reduction.\n",
    "-   How to select the dimensionality?\n",
    "-   Need to estimate marginal likelihood.\n",
    "-   In standard GP-LVM it increases with increasing $q$.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"40%\">\n",
    "\n",
    "**Bayesian GP-LVM**\n",
    "\n",
    "-   Start with a standard GP-LVM.\n",
    "-   Apply standard latent variable approach:\n",
    "    -   Define Gaussian prior over , $\\mathbf{Z}$.\n",
    "    -   Integrate out .\n",
    "    -   Unfortunately integration is intractable.\n",
    "\n",
    "</td>\n",
    "<td width=\"60%\">\n",
    "<center>\n",
    "\n",
    "{ }\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Variational Approach Fails\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/variational-bayes-gplvm-long.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/variational-bayes-gplvm-long.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "-   Standard variational bound has the form: $$\n",
    "    \\mathcal{L}= \\left\\langle\\log p(\\mathbf{ y}|\\mathbf{Z})\\right\\rangle_{q(\\mathbf{Z})} + \\text{KL}\\left( q(\\mathbf{Z})\\,\\|\\,p(\\mathbf{Z}) \\right)\n",
    "    $$\n",
    "\n",
    "The standard variational approach would require the expectation of\n",
    "$\\log p(\\mathbf{ y}|\\mathbf{Z})$ under $q(\\mathbf{Z})$. $$\n",
    "  \\begin{align}\n",
    "  \\log p(\\mathbf{ y}|\\mathbf{Z}) = & -\\frac{1}{2}\\mathbf{ y}^\\top\\left(\\mathbf{K}_{\\mathbf{ f}, \\mathbf{ f}}+\\sigma^2\\mathbf{I}\\right)^{-1}\\mathbf{ y}\\\\ & -\\frac{1}{2}\\log \\det{\\mathbf{K}_{\\mathbf{ f}, \\mathbf{ f}}+\\sigma^2 \\mathbf{I}} -\\frac{n}{2}\\log 2\\pi\n",
    "  \\end{align}\n",
    "  $$ But this is extremely difficult to compute because\n",
    "$\\mathbf{K}_{\\mathbf{ f}, \\mathbf{ f}}$ is dependent on $\\mathbf{Z}$ and\n",
    "it appears in the inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Bayesian GP-LVM\n",
    "\n",
    "The alternative approach is to consider the collapsed variational bound\n",
    "(used for low rank (sparse is a misnomer) Gaussian process\n",
    "approximations. $$\n",
    "    p(\\mathbf{ y})\\geq \\prod_{i=1}^nc_i \\int \\mathcal{N}\\left(\\mathbf{ y}|\\left\\langle\\mathbf{ f}\\right\\rangle,\\sigma^2\\mathbf{I}\\right)p(\\mathbf{ u}) \\text{d}\\mathbf{ u}\n",
    "  $$ $$\n",
    "    p(\\mathbf{ y}|\\mathbf{Z})\\geq \\prod_{i=1}^nc_i \\int \\mathcal{N}\\left(\\mathbf{ y}|\\left\\langle\\mathbf{ f}\\right\\rangle_{p(\\mathbf{ f}|\\mathbf{ u}, \\mathbf{Z})},\\sigma^2\\mathbf{I}\\right)p(\\mathbf{ u}) \\text{d}\\mathbf{ u}\n",
    "  $$ $$\n",
    "      \\int p(\\mathbf{ y}|\\mathbf{Z})p(\\mathbf{Z}) \\text{d}\\mathbf{Z}\\geq \\int \\prod_{i=1}^nc_i \\mathcal{N}\\left(\\mathbf{ y}|\\left\\langle\\mathbf{ f}\\right\\rangle_{p(\\mathbf{ f}|\\mathbf{ u}, \\mathbf{Z})},\\sigma^2\\mathbf{I}\\right) p(\\mathbf{Z})\\text{d}\\mathbf{Z}p(\\mathbf{ u}) \\text{d}\\mathbf{ u}\n",
    "  $$\n",
    "\n",
    "To integrate across $\\mathbf{Z}$ we apply the lower bound to the inner\n",
    "integral. $$\n",
    "    \\begin{align}\n",
    "    \\int \\prod_{i=1}^nc_i \\mathcal{N}\\left(\\mathbf{ y}|\\left\\langle\\mathbf{ f}\\right\\rangle_{p(\\mathbf{ f}|\\mathbf{ u}, \\mathbf{Z})},\\sigma^2\\mathbf{I}\\right) p(\\mathbf{Z})\\text{d}\\mathbf{Z}\\geq & \\left\\langle\\sum_{i=1}^n\\log  c_i\\right\\rangle_{q(\\mathbf{Z})}\\\\ & +\\left\\langle\\log\\mathcal{N}\\left(\\mathbf{ y}|\\left\\langle\\mathbf{ f}\\right\\rangle_{p(\\mathbf{ f}|\\mathbf{ u}, \\mathbf{Z})},\\sigma^2\\mathbf{I}\\right)\\right\\rangle_{q(\\mathbf{Z})}\\\\& + \\text{KL}\\left( q(\\mathbf{Z})\\,\\|\\,p(\\mathbf{Z}) \\right)    \n",
    "    \\end{align}\n",
    "  $$ \\* Which is analytically tractable for Gaussian $q(\\mathbf{Z})$ and\n",
    "some covariance functions.\n",
    "\n",
    "-   Need expectations under $q(\\mathbf{Z})$ of: $$\n",
    "    \\log c_i = \\frac{1}{2\\sigma^2} \\left[k_{i, i} - \\mathbf{ k}_{i, \\mathbf{ u}}^\\top \\mathbf{K}_{\\mathbf{ u}, \\mathbf{ u}}^{-1} \\mathbf{ k}_{i, \\mathbf{ u}}\\right]\n",
    "    $$ and $$\n",
    "    \\log \\mathcal{N}\\left(\\mathbf{ y}|\\left\\langle\\mathbf{ f}\\right\\rangle_{p(\\mathbf{ f}|\\mathbf{ u},\\mathbf{Y})},\\sigma^2\\mathbf{I}\\right) = -\\frac{1}{2}\\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^2}\\left(y_i - \\mathbf{K}_{\\mathbf{ f}, \\mathbf{ u}}\\mathbf{K}_{\\mathbf{ u},\\mathbf{ u}}^{-1}\\mathbf{ u}\\right)^2\n",
    "    $$\n",
    "\n",
    "-   This requires the expectations $$\n",
    "    \\left\\langle\\mathbf{K}_{\\mathbf{ f},\\mathbf{ u}}\\right\\rangle_{q(\\mathbf{Z})}\n",
    "    $$ and $$\n",
    "    \\left\\langle\\mathbf{K}_{\\mathbf{ f},\\mathbf{ u}}\\mathbf{K}_{\\mathbf{ u},\\mathbf{ u}}^{-1}\\mathbf{K}_{\\mathbf{ u},\\mathbf{ f}}\\right\\rangle_{q(\\mathbf{Z})}\n",
    "    $$ which can be computed analytically for some covariance functions\n",
    "    (Damianou et al., 2016) or through sampling (Damianou, 2015;\n",
    "    Salimbeni and Deisenroth, 2017).\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip0\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Andreas Damianou\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"https://inverseprobability.com/talks/./slides/diagrams//people/andreas-damianou.png\" clip-path=\"url(#clip0)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "  \n",
    "\n",
    "Damianou and Lawrence (2013)\n",
    "\n",
    "  \n",
    "\n",
    "-   Augment each layer with inducing variables $\\mathbf{ u}_i$.\n",
    "\n",
    "-   Apply variational compression, where\n",
    "    $$\\tilde p(\\mathbf{ h}_i|\\mathbf{ u}_i,\\mathbf{ h}_{i-1})\n",
    "        = \\mathcal{N}\\left(\\mathbf{ h}_i|\\mathbf{K}_{\\mathbf{ h}_{i}\\mathbf{ u}_{i}}\\mathbf{K}_{\\mathbf{ u}_i\\mathbf{ u}_i}^{-1}\\mathbf{ u}_i,\\sigma^2_i\\mathbf{I}\\right).$$\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip1\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "James Hensman\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"https://inverseprobability.com/talks/./slides/diagrams//people/james-hensman.png\" clip-path=\"url(#clip1)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "  \n",
    "\n",
    "Hensman and Lawrence (2014)\n",
    "\n",
    "-   By sustaining explicity distributions over inducing variables James\n",
    "    Hensman has developed a nested variant of variational compression.\n",
    "\n",
    "-   Exciting thing: it mathematically looks like a deep neural network,\n",
    "    but with inducing variables in the place of basis functions.\n",
    "\n",
    "-   Additional complexity control term in the objective function.\n",
    "\n",
    "$${\\only<1>{\\color{red}}\\log \\mathcal{N}\\left(\\mathbf{ y}|{\\only<2->{\\color{blue}}{\\boldsymbol\n",
    "          \\Psi}_{\\ell}}\\mathbf{K}_{\\mathbf{ u}_{\\ell}\n",
    "          \\mathbf{ u}_{\\ell}}^{-1}{\\mathbf\n",
    "          m}_\\ell,\\sigma^2_\\ell\\mathbf{I}\\right)}$$ where\n",
    "\n",
    "For Gaussian likelihoods:\n",
    "\n",
    "Define:\n",
    "$$q_{i, i} = \\text{var}_{p(f_i|\\mathbf{ u})}\\left( f_i \\right) = \\left\\langle f_i^2\\right\\rangle_{p(f_i|\\mathbf{ u})} - \\left\\langle f_i\\right\\rangle_{p(f_i|\\mathbf{ u})}^2$$\n",
    "We can write: $$c_i = \\exp\\left(-{\\frac{q_{i,i}}{2\\sigma^2}}\\right)$$ If\n",
    "joint distribution of $p(\\mathbf{ f}, \\mathbf{ u})$ is Gaussian then:\n",
    "$$q_{i, i} = k_{i, i} - \\mathbf{ k}_{i, \\mathbf{ u}}^\\top \\mathbf{K}_{\\mathbf{ u}, \\mathbf{ u}}^{-1} \\mathbf{ k}_{i, \\mathbf{ u}}$$\n",
    "\n",
    "$c_i$ is not a function of $\\mathbf{ u}$ but *is* a function of\n",
    "$\\mathbf{X}_\\mathbf{ u}$.\n",
    "\n",
    "Substitute variational bound into marginal likelihood:\n",
    "$$p(\\mathbf{ y})\\geq \\prod_{i=1}^nc_i \\int \\mathcal{N}\\left(\\mathbf{ y}|\\left\\langle\\mathbf{ f}\\right\\rangle,\\sigma^2\\mathbf{I}\\right)p(\\mathbf{ u}) \\text{d}\\mathbf{ u}$$\n",
    "Note that:\n",
    "$$\\left\\langle\\mathbf{ f}\\right\\rangle_{p(\\mathbf{ f}|\\mathbf{ u})} = \\mathbf{K}_{\\mathbf{ f}, \\mathbf{ u}} \\mathbf{K}_{\\mathbf{ u}, \\mathbf{ u}}^{-1}\\mathbf{ u}$$\n",
    "is *linearly* dependent on $\\mathbf{ u}$.\n",
    "\n",
    "Making the marginalization of $\\mathbf{ u}$ straightforward. In the\n",
    "Gaussian case:\n",
    "$$p(\\mathbf{ u}) = \\mathcal{N}\\left(\\mathbf{ u}|\\mathbf{0},\\mathbf{K}_{\\mathbf{ u},\\mathbf{ u}}\\right)$$\n",
    "\n",
    "-   Thang and Turner paper\n",
    "\n",
    "-   Joint Gaussianity is analytic, but not flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Regression Problem\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/inducing-variables-demo.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/inducing-variables-demo.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Here we set up a simple one dimensional regression problem. The input\n",
    "locations, $\\mathbf{X}$, are in two separate clusters. The response\n",
    "variable, $\\mathbf{ y}$, is sampled from a Gaussian process with an\n",
    "exponentiated quadratic covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "noise_var = 0.01\n",
    "X = np.zeros((50, 1))\n",
    "X[:25, :] = np.linspace(0,3,25)[:,None] # First cluster of inputs/covariates\n",
    "X[25:, :] = np.linspace(7,10,25)[:,None] # Second cluster of inputs/covariates\n",
    "\n",
    "# Sample response variables from a Gaussian process with exponentiated quadratic covariance.\n",
    "k = GPy.kern.RBF(1)\n",
    "y = np.random.multivariate_normal(np.zeros(N),k.K(X)+np.eye(N)*np.sqrt(noise_var)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we perform a full Gaussian process regression on the data. We\n",
    "create a GP model, `m_full`, and fit it to the data, plotting the\n",
    "resulting fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(X,y)\n",
    "_ = m_full.optimize(messages=True) # Optimize parameters of covariance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai\n",
    "import mlai.plot \n",
    "from mlai.gp_tutorial import gpplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_full, ax=ax, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2)\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='sparse-demo-full-gp.svg',\n",
    "                  directory='./gp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/./slides/diagrams//gp/sparse-demo-full-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Full Gaussian process fitted to the data set.</i>\n",
    "\n",
    "Now we set up the inducing variables, $\\mathbf{u}$. Each inducing\n",
    "variable has its own associated input index, $\\mathbf{Z}$, which lives\n",
    "in the same space as $\\mathbf{X}$. Here we are using the true covariance\n",
    "function parameters to generate the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = GPy.kern.RBF(1)\n",
    "Z = np.hstack(\n",
    "        (np.linspace(2.5,4.,3),\n",
    "        np.linspace(7,8.5,3)))[:,None]\n",
    "m = GPy.models.SparseGPRegression(X,y,kernel=kern,Z=Z)\n",
    "m.noise_var = noise_var\n",
    "m.inducing_inputs.constrain_fixed()\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, ax=ax, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2, xlim=xlim, ylim=ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='sparse-demo-constrained-inducing-6-unlearned-gp.svg', \n",
    "                  directory='./gp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/./slides/diagrams//gp/sparse-demo-constrained-inducing-6-unlearned-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Sparse Gaussian process fitted with six inducing variables,\n",
    "no optimization of parameters or inducing variables.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = m.optimize(messages=True)\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, ax=ax, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2, xlim=xlim, ylim=ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='sparse-demo-constrained-inducing-6-learned-gp.svg',\n",
    "                  directory='./gp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/./slides/diagrams//gp/sparse-demo-constrained-inducing-6-learned-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gaussian process fitted with inducing variables fixed and\n",
    "parameters optimized</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.randomize()\n",
    "m.inducing_inputs.unconstrain()\n",
    "_ = m.optimize(messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, ax=ax, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2,xlim=xlim, ylim=ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='sparse-demo-unconstrained-inducing-6-gp.svg', \n",
    "                  directory='./gp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/./slides/diagrams//gp/sparse-demo-unconstrained-inducing-6-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gaussian process fitted with location of inducing variables\n",
    "and parameters both optimized</i>\n",
    "\n",
    "Now we will vary the number of inducing points used to form the\n",
    "approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.num_inducing=8\n",
    "m.randomize()\n",
    "M = 8\n",
    "m.set_Z(np.random.rand(M,1)*12)\n",
    "\n",
    "_ = m.optimize(messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, ax=ax, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2, xlim=xlim, ylim=ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='sparse-demo-sparse-inducing-8-gp.svg', \n",
    "                  directory='./gp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/./slides/diagrams//gp/sparse-demo-sparse-inducing-8-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "<img src=\"https://inverseprobability.com/talks/./slides/diagrams//gp/sparse-demo-full-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Comparison of the full Gaussian process fit with a sparse\n",
    "Gaussian process using eight inducing varibles. Both inducing variables\n",
    "and parameters are optimized.</i>\n",
    "\n",
    "And we can compare the probability of the result to the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.log_likelihood(), m_full.log_likelihood())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks!\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Álvarez, M.A., Luengo, D., Titsias, M.K., Lawrence, N.D., 2010.\n",
    "Efficient multioutput Gaussian processes through variational inducing\n",
    "kernels. pp. 25–32.\n",
    "\n",
    "Bui, T.D., Yan, J., Turner, R.E., 2017. A unifying framework for\n",
    "Gaussian process pseudo-point approximations using power expectation\n",
    "propagation. Journal of Machine Learning Research 18, 1–72.\n",
    "\n",
    "Csató, L., 2002. Gaussian processes — iterative sparse approximations\n",
    "(PhD thesis). Aston University.\n",
    "\n",
    "Csató, L., Opper, M., 2002. Sparse on-line Gaussian processes. Neural\n",
    "Computation 14, 641–668.\n",
    "\n",
    "Dai, Z., Damianou, A., Hensman, J., Lawrence, N.D., 2014. Gaussian\n",
    "process models with parallelization and GPU acceleration.\n",
    "\n",
    "Damianou, A., 2015. Deep Gaussian processes and variational propagation\n",
    "of uncertainty (PhD thesis). University of Sheffield.\n",
    "\n",
    "Damianou, A., Lawrence, N.D., 2013. Deep Gaussian processes. pp.\n",
    "207–215.\n",
    "\n",
    "Damianou, A., Titsias, M.K., Lawrence, N.D., 2016. Variational inference\n",
    "for latent variables and uncertain inputs in Gaussian processes. Journal\n",
    "of Machine Learning Research 17.\n",
    "\n",
    "Gal, Y., Wilk, M. van der, Rasmussen, C.E., n.d. Distributed variational\n",
    "inference in sparse Gaussian process regression and latent variable\n",
    "models.\n",
    "\n",
    "Hensman, J., Fusi, N., Lawrence, N.D., n.d. Gaussian processes for big\n",
    "data.\n",
    "\n",
    "Hensman, J., Lawrence, N.D., 2014. Nested variational compression in\n",
    "deep Gaussian processes. University of Sheffield.\n",
    "\n",
    "Hensman, J., Rattray, M., Lawrence, N.D., 2012. Fast variational\n",
    "inference in the conjugate exponential family.\n",
    "\n",
    "Hoffman, M., Blei, D.M., Wang, C., Paisley, J., 2012. Stochastic\n",
    "variational inference, arXiv preprint arXiv:1206.7051.\n",
    "\n",
    "King, N.J., Lawrence, N.D., n.d. Fast variational inference for Gaussian\n",
    "Process models through KL-correction. pp. 270–281.\n",
    "\n",
    "Lawrence, N.D., n.d. Learning for larger datasets with the Gaussian\n",
    "process latent variable model. pp. 243–250.\n",
    "\n",
    "Lawrence, N.D., Seeger, M., Herbrich, R., n.d. Fast sparse Gaussian\n",
    "process methods: The informative vector machine. pp. 625–632.\n",
    "\n",
    "Lázaro-Gredilla, M., Quiñonero-Candela, J., Rasmussen, C.E., 2010.\n",
    "Sparse spectrum gaussian processes. Journal of Machine Learning Research\n",
    "11, 1865–1881.\n",
    "\n",
    "Quiñonero Candela, J., Rasmussen, C.E., 2005. A unifying view of sparse\n",
    "approximate Gaussian process regression. Journal of Machine Learning\n",
    "Research 6, 1939–1959.\n",
    "\n",
    "Salimbeni, H., Deisenroth, M., 2017. Doubly stochastic variational\n",
    "inference for deep Gaussian processes, in: Guyon, I., Luxburg, U.V.,\n",
    "Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R.\n",
    "(Eds.), Advances in Neural Information Processing Systems 30. Curran\n",
    "Associates, Inc., pp. 4591–4602.\n",
    "\n",
    "Seeger, M.W., Hetzel, A., Dai, Z., Lawrence, N.D., 2017.\n",
    "Auto-differentiating linear algebra. CoRR abs/1710.08717.\n",
    "\n",
    "Seeger, M., Williams, C.K.I., Lawrence, N.D., n.d. Fast forward\n",
    "selection to speed up sparse Gaussian process regression.\n",
    "\n",
    "Smola, A.J., Bartlett, P.L., n.d. Sparse greedy Gaussian process\n",
    "regression. pp. 619–625.\n",
    "\n",
    "Snelson, E., Ghahramani, Z., n.d. Sparse Gaussian processes using\n",
    "pseudo-inputs.\n",
    "\n",
    "Titsias, M.K., n.d. Variational learning of inducing variables in sparse\n",
    "Gaussian processes. pp. 567–574.\n",
    "\n",
    "Williams, C.K.I., Seeger, M., n.d. Using the Nyström method to speed up\n",
    "kernel machines. pp. 682–688."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
