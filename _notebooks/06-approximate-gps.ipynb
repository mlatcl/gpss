{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate Gaussian Processes\n",
    "==============================\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), University of\n",
    "\n",
    "Cambridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\tk}[1]{}\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\uniformDist}[3]{\\mathcal{U}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\uniformSamp}[2]{\\mathcal{U}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/sparse-gps.jpg\" style=\"width:45%\">\n",
    "\n",
    "Figure: <i>In recent years, approximations for Gaussian process models\n",
    "haven’t been the most fashionable approach to machine learning. Image\n",
    "credit: Kai Arulkumaran</i>\n",
    "\n",
    "Inference in a Gaussian process has computational complexity of\n",
    "$\\mathcal{O}(n^3)$ and storage demands of $\\mathcal{O}(n^2)$. This is\n",
    "too large for many modern data sets.\n",
    "\n",
    "Low rank approximations allow us to work with Gaussian processes with\n",
    "computational complexity of $\\mathcal{O}(nm^2)$ and storage demands of\n",
    "$\\mathcal{O}(nm)$, where $m$ is a user chosen parameter.\n",
    "\n",
    "In machine learning, low rank approximations date back to Smola and\n",
    "Bartlett (n.d.), Williams and Seeger (n.d.), who considered the Nyström\n",
    "approximation and Csató and Opper (2002);Csató (2002) who considered low\n",
    "rank approximations in the context of on-line learning. Selection of\n",
    "active points for the approximation was considered by Seeger et al.\n",
    "(n.d.) and Snelson and Ghahramani (n.d.) first proposed that the active\n",
    "set could be optimized directly. Those approaches were reviewed by\n",
    "Quiñonero Candela and Rasmussen (2005) under a unifying likelihood\n",
    "approximation perspective. General rules for deriving the maximum\n",
    "likelihood for these sparse approximations were given in Lawrence\n",
    "(n.d.).\n",
    "\n",
    "Modern variational interpretations of these low rank approaches were\n",
    "first explored in Titsias (n.d.). A more modern summary which considers\n",
    "each of these approximations as an $\\alpha$-divergence is given by Bui\n",
    "et al. (2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Compression\n",
    "-----------------------\n",
    "\n",
    "Inducing variables are a compression of the real observations. The basic\n",
    "idea is can I create a new data set that summarizes all the information\n",
    "in the original data set. If this data set is smaller, I’ve compressed\n",
    "the information in the original data set.\n",
    "\n",
    "Inducing variables can be thought of as pseudo-data, indeed in Snelson\n",
    "and Ghahramani (n.d.) they were referred to as *pseudo-points*.\n",
    "\n",
    "The only requirement for inducing variables is that they are jointly\n",
    "distributed as a Gaussian process with the original data. This means\n",
    "that they can be from the space $\\mathbf{ f}$ or a space that is related\n",
    "through a linear operator (see e.g. Álvarez et al. (2010)). For example\n",
    "we could choose to store the gradient of the function at particular\n",
    "points or a value from the frequency spectrum of the function\n",
    "(Lázaro-Gredilla et al., 2010)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Compression II\n",
    "--------------------------\n",
    "\n",
    "Inducing variables don’t only allow for the compression of the\n",
    "non-parameteric information into a reduced data aset but they also allow\n",
    "for computational scaling of the algorithms through, for example\n",
    "stochastic variational approaches Hensman et al. (n.d.) or\n",
    "parallelization Gal et al. (n.d.),Dai et al. (2014), Seeger et al.\n",
    "(2017).\n",
    "\n",
    "We’ve seen how we go from parametric to non-parametric. The limit\n",
    "implies infinite dimensional $\\mathbf{ w}$. Gaussian processes are\n",
    "generally non-parametric: combine data with covariance function to get\n",
    "model. This representation *cannot* be summarized by a parameter vector\n",
    "of a fixed size.\n",
    "\n",
    "Parametric models have a representation that does not respond to\n",
    "increasing training set size. Bayesian posterior distributions over\n",
    "parameters contain the information about the training data, for example\n",
    "if we use use Bayes’ rule from training data, $$\n",
    "p\\left(\\mathbf{ w}|\\mathbf{ y}, \\mathbf{X}\\right),\n",
    "$$ to make predictions on test data $$\n",
    "p\\left(y_*|\\mathbf{X}_*, \\mathbf{ y}, \\mathbf{X}\\right) = \\int\n",
    "              p\\left(y_*|\\mathbf{ w},\\mathbf{X}_*\\right)p\\left(\\mathbf{ w}|\\mathbf{ y},\n",
    "                \\mathbf{X})\\text{d}\\mathbf{ w}\\right)\n",
    "$$ then $\\mathbf{ w}$ becomes a bottleneck for information about the\n",
    "training set to pass to the test set. The solution is to increase $m$ so\n",
    "that the bottleneck is so large that it no longer presents a problem.\n",
    "How big is big enough for $m$? Non-parametrics says\n",
    "$m\\rightarrow \\infty$.\n",
    "\n",
    "Now no longer possible to manipulate the model through the standard\n",
    "parametric form. However, it *is* possible to express *parametric* as\n",
    "GPs: $$\n",
    "k\\left(\\mathbf{ x}_i,\\mathbf{ x}_j\\right)=\\phi_:\\left(\\mathbf{ x}_i\\right)^\\top\\phi_:\\left(\\mathbf{ x}_j\\right).\n",
    "$$ These are known as degenerate covariance matrices. Their rank is at\n",
    "most $m$, non-parametric models have full rank covariance matrices. Most\n",
    "well known is the “linear kernel”, $$\n",
    "k(\\mathbf{ x}_i, \\mathbf{ x}_j) = \\mathbf{ x}_i^\\top\\mathbf{ x}_j.\n",
    "$$ For non-parametrics prediction at a new point, $\\mathbf{ f}_*$, is\n",
    "made by conditioning on $\\mathbf{ f}$ in the joint distribution. In GPs\n",
    "this involves combining the training data with the covariance function\n",
    "and the mean function. Parametric is a special case when conditional\n",
    "prediction can be summarized in a *fixed* number of parameters.\n",
    "Complexity of parametric model remains fixed regardless of the size of\n",
    "our training data set. For a non-parametric model the required number of\n",
    "parameters grows with the size of the training data.\n",
    "\n",
    "-   Everything we want to do with a GP involves marginalising\n",
    "    $\\mathbf{ f}$\n",
    "    -   Predictions\n",
    "    -   Marginal likelihood\n",
    "    -   Estimating covariance parameters\n",
    "-   The posterior of $\\mathbf{ f}$ is the central object. This means\n",
    "    inverting $\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}$.\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/cov_approx.png\" style=\"width:90%\">\n",
    "\n",
    "Figure: Figure originally from presentation by Ed Snelson at NIPS\n",
    "\n",
    "The Nystr\"om approximation takes the form, $$\n",
    "\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}\\approx \\mathbf{Q}_{\\mathbf{ f}\\mathbf{ f}}= \\mathbf{K}_{\\mathbf{ f}\\mathbf{ u}}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}^{-1}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ f}}\n",
    "$$ The idea is that instead of inverting\n",
    "$\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}$, we make a low rank (or Nyström)\n",
    "approximation, and invert $\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}$ instead.\n",
    "\n",
    "In the original Nystr\"om method the columns to incorporate are sampled\n",
    "from the complete set of columns (without replacement). In a kernel\n",
    "matrix each of these columns corresponds to a data point. In the\n",
    "Nystr\"om method these points are sometimes called *landmark* points.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "$$\\mathbf{X},\\,\\mathbf{ y}$$\n",
    "\n",
    "</td>\n",
    "<td width=\"70%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/nomenclature1.png\" style=\"width:90%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "$$\\mathbf{X},\\,\\mathbf{ y}$$\n",
    "$${\\color{blue} f(\\mathbf{ x})} \\sim {\\mathcal GP}$$\n",
    "\n",
    "</td>\n",
    "<td width=\"70%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/nomenclature2.png\" style=\"width:90%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "$$\\mathbf{X},\\,\\mathbf{ y}$$\n",
    "$$f(\\mathbf{ x}) \\sim {\\mathcal GP}$$$$p({\\color{blue} \\mathbf{ f}}) = \\mathcal{N}\\left(\\mathbf{0},\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}\\right)$$\n",
    "\n",
    "</td>\n",
    "<td width=\"70%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/nomenclature3.png\" style=\"width:90%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "$$\n",
    "\\mathbf{X},\\,\\mathbf{ y}$$ $$f(\\mathbf{ x}) \\sim {\\mathcal GP}\n",
    "$$ $$\n",
    "p(\\mathbf{ f}) = \\mathcal{N}\\left(\\mathbf{0},\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}\\right)\n",
    "$$ $$p( \\mathbf{ f}|\\mathbf{ y},\\mathbf{X})\n",
    "$$\n",
    "\n",
    "</td>\n",
    "<td width=\"70%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/nomenclature3a.png\" style=\"width:90%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Take an extra $m$ points on the function, $\\mathbf{ u}= f(\\mathbf{Z})$.\n",
    "$$p(\\mathbf{ y},\\mathbf{ f},\\mathbf{ u}) = p(\\mathbf{ y}|\\mathbf{ f}) p(\\mathbf{ f}|\\mathbf{ u}) p(\\mathbf{ u})$$\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/cov_inducing_withX.png\" style=\"width:60%\">\n",
    "\n",
    "Take and extra $M$ points on the function, $\\mathbf{ u}= f(\\mathbf{Z})$.\n",
    "$$p(\\mathbf{ y},\\mathbf{ f},\\mathbf{ u}) = p(\\mathbf{ y}|\\mathbf{ f}) p(\\mathbf{ f}|\\mathbf{ u}) p(\\mathbf{ u})$$\n",
    "$$\\begin{aligned}\n",
    "    p(\\mathbf{ y}|\\mathbf{ f}) &= \\mathcal{N}\\left(\\mathbf{ y}|\\mathbf{ f},\\sigma^2 \\mathbf{I}\\right)\\\\\n",
    "    p(\\mathbf{ f}|\\mathbf{ u}) &= \\mathcal{N}\\left(\\mathbf{ f}| \\mathbf{K}_{\\mathbf{ f}\\mathbf{ u}}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}^{-1}\\mathbf{ u}, \\tilde{\\mathbf{K}}\\right)\\\\\n",
    "    p(\\mathbf{ u}) &= \\mathcal{N}\\left(\\mathbf{ u}| \\mathbf{0},\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}\\right)\n",
    "  \\end{aligned}$$\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "$$\\mathbf{X},\\,\\mathbf{ y}$$ $$f(\\mathbf{ x}) \\sim {\\mathcal GP}$$\n",
    "$$p(\\mathbf{ f}) = \\mathcal{N}\\left(\\mathbf{0},\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}\\right)$$\n",
    "$$p(\\mathbf{ f}|\\mathbf{ y},\\mathbf{X})$$\n",
    "\n",
    "</td>\n",
    "<td width=\"70%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/nomenclature4\" style=\"width:90%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "                           &\\qquad\\mathbf{Z}, \\mathbf{ u}\\\\                      &p({\\color{red} \\mathbf{ u}})  = \\mathcal{N}\\left(\\mathbf{0},\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}\\right)\\end{align}\n",
    "$$\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "$$\\mathbf{X},\\,\\mathbf{ y}$$ $$f(\\mathbf{ x}) \\sim {\\mathcal GP}$$\n",
    "$$p(\\mathbf{ f}) = \\mathcal{N}\\left(\\mathbf{0},\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}\\right)$$\n",
    "$$p(\\mathbf{ f}|\\mathbf{ y},\\mathbf{X})$$\n",
    "$$p(\\mathbf{ u})  = \\mathcal{N}\\left(\\mathbf{0},\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}\\right)$$\n",
    "$$\\widetilde p({\\color{red}\\mathbf{ u}}|\\mathbf{ y},\\mathbf{X})$$\n",
    "\n",
    "</td>\n",
    "<td width=\"70%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/nomenclature5.png\" style=\"width:90%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Instead of doing $$\n",
    "p(\\mathbf{ f}|\\mathbf{ y},\\mathbf{X}) = \\frac{p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f}|\\mathbf{X})}{\\int p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f}|\\mathbf{X}){\\text{d}\\mathbf{ f}}}\n",
    "$$ We’ll do $$\n",
    "p(\\mathbf{ u}|\\mathbf{ y},\\mathbf{Z}) = \\frac{p(\\mathbf{ y}|\\mathbf{ u})p(\\mathbf{ u}|\\mathbf{Z})}{\\int p(\\mathbf{ y}|\\mathbf{ u})p(\\mathbf{ u}|\\mathbf{Z}){\\text{d}\\mathbf{ u}}}\n",
    "$$\n",
    "\n",
    "<!--Flexible Parametric Approximation-->\n",
    "\n",
    "-   Date back to {Williams and Seeger (n.d.); Smola and Bartlett (n.d.);\n",
    "    Csató and Opper (2002); Seeger et al. (n.d.); Snelson and Ghahramani\n",
    "    (n.d.)}. See {Quiñonero Candela and Rasmussen (2005); Bui et\n",
    "    al. (2017)} for reviews.\n",
    "-   We follow variational perspective of {Titsias (n.d.)}.\n",
    "-   This is an augmented variable method, followed by a collapsed\n",
    "    variational approximation {King and Lawrence (n.d.); Hensman et\n",
    "    al. (2012)}.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"60%\">\n",
    "</td>\n",
    "<td width=\"40%\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Bound on $p(\\mathbf{ y}|\\mathbf{ u})$\n",
    "-------------------------------------------------\n",
    "\n",
    "The conditional density of the data given the inducing points can be\n",
    "*lower* bounded variationally $$\n",
    "\\begin{aligned}\n",
    "    \\log p(\\mathbf{ y}|\\mathbf{ u}) & = \\log \\int p(\\mathbf{ y}|\\mathbf{ f}) p(\\mathbf{ f}|\\mathbf{ u}) \\text{d}\\mathbf{ f}\\\\ & = \\int q(\\mathbf{ f}) \\log \\frac{p(\\mathbf{ y}|\\mathbf{ f}) p(\\mathbf{ f}|\\mathbf{ u})}{q(\\mathbf{ f})}\\text{d}\\mathbf{ f}+ \\text{KL}\\left( q(\\mathbf{ f})\\,\\|\\,p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u}) \\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The key innovation from Titsias (n.d.) was to then make a particular\n",
    "choice for $q(\\mathbf{ f})$. If we set\n",
    "$q(\\mathbf{ f})=p(\\mathbf{ f}|\\mathbf{ u})$, $$\n",
    "  \\log p(\\mathbf{ y}|\\mathbf{ u}) \\geq \\int p(\\mathbf{ f}|\\mathbf{ u}) \\log p(\\mathbf{ y}|\\mathbf{ f})\\text{d}\\mathbf{ f}.\n",
    "  $$ $$\n",
    "  p(\\mathbf{ y}|\\mathbf{ u}) \\geq \\exp \\int p(\\mathbf{ f}|\\mathbf{ u}) \\log p(\\mathbf{ y}|\\mathbf{ f})\\text{d}\\mathbf{ f}.\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal Compression in Inducing Variables\n",
    "-----------------------------------------\n",
    "\n",
    "Maximizing the lower bound minimizes the Kullback-Leibler divergence (or\n",
    "*information gain*) between our approximating density,\n",
    "$p(\\mathbf{ f}|\\mathbf{ u})$ and the true posterior density,\n",
    "$p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u})$.\n",
    "\n",
    "$$\n",
    "  \\text{KL}\\left( p(\\mathbf{ f}|\\mathbf{ u})\\,\\|\\,p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u}) \\right) = \\int p(\\mathbf{ f}|\\mathbf{ u}) \\log \\frac{p(\\mathbf{ f}|\\mathbf{ u})}{p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u})}\\text{d}\\mathbf{ u}\n",
    "  $$\n",
    "\n",
    "This bound is minimized when the information stored about $\\mathbf{ y}$\n",
    "is already stored in $\\mathbf{ u}$. In other words, maximizing the bound\n",
    "seeks an *optimal compression* from the *information gain* perspective.\n",
    "\n",
    "For the case where $\\mathbf{ u}= \\mathbf{ f}$ the bound is exact\n",
    "($\\mathbf{ f}$ $d$-separates $\\mathbf{ y}$ from $\\mathbf{ u}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice of Inducing Variables\n",
    "----------------------------\n",
    "\n",
    "The quality of the resulting bound is determined by the choice of the\n",
    "inducing variables. You are free to choose whichever heuristics you like\n",
    "for the inducing variables, as long as they are drawn jointly from a\n",
    "valid Gaussian process, i.e. such that $$\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{ f}\\\\\n",
    "\\mathbf{ u}\n",
    "\\end{bmatrix} \\sim \\mathcal{N}\\left(\\mathbf{0},\\mathbf{K}\\right)\n",
    "$$ where the kernel matrix itself can be decomposed into $$\n",
    "\\mathbf{K}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}& \\mathbf{K}_{\\mathbf{ f}\\mathbf{ u}}\\\\\n",
    "\\mathbf{K}_{\\mathbf{ u}\\mathbf{ f}}& \\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}\n",
    "\\end{bmatrix}\n",
    "$$ Choosing the inducing variables amounts to specifying\n",
    "$\\mathbf{K}_{\\mathbf{ f}\\mathbf{ u}}$ and\n",
    "$\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}$ such that $\\mathbf{K}$ remains\n",
    "positive definite. The typical choice is to choose $\\mathbf{ u}$ in the\n",
    "same domain as $\\mathbf{ f}$, associating each inducing output, $u_i$\n",
    "with a corresponding input location $\\mathbf{ z}$. However, more\n",
    "imaginative choices are absolutely possible. In particular, if\n",
    "$\\mathbf{ u}$ is related to $\\mathbf{ f}$ through a linear operator (see\n",
    "e.g. Álvarez et al. (2010)), then valid\n",
    "$\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}$ and\n",
    "$\\mathbf{K}_{\\mathbf{ u}\\mathbf{ f}}$ can be constructed. For example we\n",
    "could choose to store the gradient of the function at particular points\n",
    "or a value from the frequency spectrum of the function (Lázaro-Gredilla\n",
    "et al., 2010)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Compression II\n",
    "--------------------------\n",
    "\n",
    "Inducing variables don’t only allow for the compression of the\n",
    "non-parameteric information into a reduced data set but they also allow\n",
    "for computational scaling of the algorithms through, for example\n",
    "stochastic variational approaches(Hensman et al., n.d.; Hoffman et al.,\n",
    "2012) or parallelization (Dai et al., 2014; Gal et al., n.d.; Seeger et\n",
    "al., 2017).\n",
    "\n",
    "-   If the likelihood, $p(\\mathbf{ y}|\\mathbf{ f})$, factorizes\n",
    "\n",
    "-   \\<8-\\> Then the bound factorizes.\n",
    "\n",
    "-   \\<10-\\> Now need a choice of distributions for $\\mathbf{ f}$ and\n",
    "    $\\mathbf{ y}|\\mathbf{ f}$ …\n",
    "\n",
    "-   Choose to go a different way.\n",
    "\n",
    "-   Introduce a set of auxiliary variables, $\\mathbf{ u}$, which are $m$\n",
    "    in length.\n",
    "\n",
    "-   They are like “artificial data”.\n",
    "\n",
    "-   Used to *induce* a distribution: $q(\\mathbf{ u}|\\mathbf{ y})$\n",
    "\n",
    "-   Introduce variable set which is *finite* dimensional. $$\n",
    "    p(\\mathbf{ y}^*|\\mathbf{ y}) \\approx \\int p(\\mathbf{ y}^*|\\mathbf{ u}) q(\\mathbf{ u}|\\mathbf{ y}) \\text{d}\\mathbf{ u}\n",
    "    $$\n",
    "\n",
    "-   But dimensionality of $\\mathbf{ u}$ can be changed to improve\n",
    "    approximation.\n",
    "\n",
    "-   Model for our data, $\\mathbf{ y}$\n",
    "\n",
    "    <table>\n",
    "    <tr>\n",
    "    <td width>\n",
    "\n",
    "    $$p(\\mathbf{ y})$$\n",
    "\n",
    "    </td>\n",
    "    <td width>\n",
    "\n",
    "    <img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/py.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "    </td>\n",
    "    </tr>\n",
    "    </table>\n",
    "\n",
    "-   Prior density over $\\mathbf{ f}$. Likelihood relates data,\n",
    "    $\\mathbf{ y}$, to $\\mathbf{ f}$.\n",
    "\n",
    "    <table>\n",
    "    <tr>\n",
    "    <td width>\n",
    "\n",
    "    $$p(\\mathbf{ y})=\\int p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f})\\text{d}\\mathbf{ f}$$\n",
    "\n",
    "    </td>\n",
    "    <td width>\n",
    "\n",
    "    <img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/pygfpf.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "    </td>\n",
    "    </tr>\n",
    "    </table>\n",
    "\n",
    "-   Prior density over $\\mathbf{ f}$. Likelihood relates data,\n",
    "    $\\mathbf{ y}$, to $\\mathbf{ f}$.\n",
    "\n",
    "    <table>\n",
    "    <tr>\n",
    "    <td width>\n",
    "\n",
    "    $$p(\\mathbf{ y})=\\int p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ u}|\\mathbf{ f})p(\\mathbf{ f})\\text{d}\\mathbf{ f}\\text{d}\\mathbf{ u}$$\n",
    "\n",
    "    </td>\n",
    "    <td width>\n",
    "\n",
    "    <img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/pygfpugfpf.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "    </td>\n",
    "    </tr>\n",
    "    </table>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width>\n",
    "\n",
    "$$p(\\mathbf{ y})=\\int \\int p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f}|\\mathbf{ u})\\text{d}\\mathbf{ f}p(\\mathbf{ u})\\text{d}\\mathbf{ u}$$\n",
    "\n",
    "</td>\n",
    "<td width>\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/pygfpfgupu.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width>\n",
    "\n",
    "$$p(\\mathbf{ y})=\\int \\int p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f}|\\mathbf{ u})\\text{d}\\mathbf{ f}p(\\mathbf{ u})\\text{d}\\mathbf{ u}$$\n",
    "\n",
    "</td>\n",
    "<td width>\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/pygfpfgupu2.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width>\n",
    "\n",
    "$$p(\\mathbf{ y}|\\mathbf{ u})=\\int p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f}|\\mathbf{ u})\\text{d}\\mathbf{ f}$$\n",
    "\n",
    "</td>\n",
    "<td width>\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/pygfpfgu.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width>\n",
    "\n",
    "$$p(\\mathbf{ y}|\\mathbf{ u})$$\n",
    "\n",
    "</td>\n",
    "<td width>\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/pygu.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width>\n",
    "\n",
    "$$p(\\mathbf{ y}|\\boldsymbol{ \\theta})$$\n",
    "\n",
    "</td>\n",
    "<td width>\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/pygtheta.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "-   Replace true $p(\\mathbf{ u}|\\mathbf{ y})$ with approximation\n",
    "    $q(\\mathbf{ u}|\\mathbf{ y})$.\n",
    "-   Minimize KL divergence between approximation and truth.\n",
    "-   This is similar to the Bayesian posterior distribution.\n",
    "-   But it’s placed over a set of ‘pseudo-observations’.\n",
    "\n",
    "$$\\mathbf{ f}, \\mathbf{ u}\\sim \\mathcal{N}\\left(\\mathbf{0},\\begin{bmatrix}\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}& \\mathbf{K}_{\\mathbf{ f}\\mathbf{ u}}\\\\\\mathbf{K}_{\\mathbf{ u}\\mathbf{ f}}& \\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}\\end{bmatrix}\\right)$$\n",
    "$$\\mathbf{ y}|\\mathbf{ f}= \\prod_{i} \\mathcal{N}\\left(f,\\sigma^2\\right)$$\n",
    "\n",
    "<!--Variational Compression-->\n",
    "\n",
    "For Gaussian likelihoods:\n",
    "\n",
    "Define:\n",
    "$$q_{i, i} = \\text{var}_{p(f_i|\\mathbf{ u})}\\left( f_i \\right) = \\left<f_i^2\\right>_{p(f_i|\\mathbf{ u})} - \\left<f_i\\right>_{p(f_i|\\mathbf{ u})}^2$$\n",
    "We can write: $$c_i = \\exp\\left(-{\\frac{q_{i,i}}{2\\sigma^2}}\\right)$$ If\n",
    "joint distribution of $p(\\mathbf{ f}, \\mathbf{ u})$ is Gaussian then:\n",
    "$$q_{i, i} = k_{i, i} - \\mathbf{ k}_{i, \\mathbf{ u}}^\\top \\mathbf{K}_{\\mathbf{ u}, \\mathbf{ u}}^{-1} \\mathbf{ k}_{i, \\mathbf{ u}}$$\n",
    "\n",
    "$c_i$ is not a function of $\\mathbf{ u}$ but *is* a function of\n",
    "$\\mathbf{X}_\\mathbf{ u}$.\n",
    "\n",
    "<!--frame end-->\n",
    "<!--frame start-->\n",
    "\n",
    "-   The sum of $q_{i,i}$ is the *total conditional variance*.\n",
    "\n",
    "-   If conditional density $p(\\mathbf{ f}|\\mathbf{ u})$ is Gaussian then\n",
    "    it has covariance\n",
    "    $$\\mathbf{Q} = \\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}} - \\mathbf{K}_{\\mathbf{ f}\\mathbf{ u}}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}^{-1} \\mathbf{K}_{\\mathbf{ u}\\mathbf{ f}}$$\n",
    "\n",
    "-   $\\text{tr}\\left(\\mathbf{Q}\\right) = \\sum_{i}q_{i,i}$ is known as\n",
    "    total variance.\n",
    "\n",
    "-   Because it is on conditional distribution we call it *total\n",
    "    conditional variance*.\n",
    "\n",
    "<!--frame end-->\n",
    "<!--frame start-->\n",
    "\n",
    "-   Measure the ‘capacity of a density’.\n",
    "\n",
    "-   Determinant of covariance represents ‘volume’ of density.\n",
    "\n",
    "-   log determinant is entropy: sum of *log* eigenvalues of covariance.\n",
    "\n",
    "-   trace of covariance is total variance: sum of eigenvalues of\n",
    "    covariance.\n",
    "\n",
    "-   $\\lambda > \\log \\lambda$ then total conditional variance upper\n",
    "    bounds entropy.\n",
    "\n",
    "<!--frame end-->\n",
    "<!--frame start-->\n",
    "\n",
    "Exponentiated total variance bounds determinant.\n",
    "$$\\det{\\mathbf{Q}} < \\exp \\text{tr}\\left(\\mathbf{Q}\\right)$$ Because\n",
    "$$\\prod_{i=1}^k \\lambda_i < \\prod_{i=1}^k \\exp(\\lambda_i)$$ where\n",
    "$\\{\\lambda_i\\}_{i=1}^k$ are the *positive* eigenvalues of $\\mathbf{Q}$\n",
    "This in turn implies\n",
    "$$\\det{\\mathbf{Q}} < \\prod_{i=1}^k \\exp\\left(q_{i,i}\\right)$$\n",
    "\n",
    "<!--frame end-->\n",
    "<!--frame start-->\n",
    "\n",
    "-   Conditional density $p(\\mathbf{ f}|\\mathbf{ u})$ can be seen as a\n",
    "    *communication channel*.\n",
    "\n",
    "-   Normally we have:\n",
    "    $$\\text{Transmitter} \\stackrel{\\mathbf{ u}}{\\rightarrow} \\begin{smallmatrix}p(\\mathbf{ f}|\\mathbf{ u}) \\\\ \\text{Channel}\\end{smallmatrix} \\stackrel{\\mathbf{ f}}{\\rightarrow} \\text{Receiver}$$\n",
    "    and we control $p(\\mathbf{ u})$ (the source density).\n",
    "\n",
    "-   *Here* we can also control the transmission channel\n",
    "    $p(\\mathbf{ f}|\\mathbf{ u})$.\n",
    "\n",
    "<!--frame end-->\n",
    "<!--frame start-->\n",
    "\n",
    "Substitute variational bound into marginal likelihood:\n",
    "$$p(\\mathbf{ y})\\geq \\prod_{i=1}^nc_i \\int \\mathcal{N}\\left(\\mathbf{ y}|\\left<\\mathbf{ f}\\right>,\\sigma^2\\mathbf{I}\\right)p(\\mathbf{ u}) \\text{d}\\mathbf{ u}$$\n",
    "Note that:\n",
    "$$\\left<\\mathbf{ f}\\right>_{p(\\mathbf{ f}|\\mathbf{ u})} = \\mathbf{K}_{\\mathbf{ f}, \\mathbf{ u}} \\mathbf{K}_{\\mathbf{ u}, \\mathbf{ u}}^{-1}\\mathbf{ u}$$\n",
    "is *linearly* dependent on $\\mathbf{ u}$.\n",
    "\n",
    "<!--frame end-->\n",
    "<!--frame start-->\n",
    "\n",
    "Making the marginalization of $\\mathbf{ u}$ straightforward. In the\n",
    "Gaussian case:\n",
    "$$p(\\mathbf{ u}) = \\mathcal{N}\\left(\\mathbf{ u}|\\mathbf{0},\\mathbf{K}_{\\mathbf{ u},\\mathbf{ u}}\\right)$$\n",
    "\n",
    "<!--frame end-->\n",
    "\n",
    "$$\\log p(\\mathbf{ y}|\\mathbf{ u}) = \\log\\int p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f}|\\mathbf{ u},\\mathbf{X})\\text{d}\\mathbf{ f}$$\n",
    "\n",
    "$$\\log p(\\mathbf{ y}|\\mathbf{ u}) = \\log \\mathbb{E}_{p(\\mathbf{ f}|\\mathbf{ u},\\mathbf{X})}\\left[p(\\mathbf{ y}|\\mathbf{ f})\\right]$$\n",
    "$$\\log p(\\mathbf{ y}|\\mathbf{ u}) \\geq  \\mathbb{E}_{p(\\mathbf{ f}|\\mathbf{ u},\\mathbf{X})}\\left[\\log p(\\mathbf{ y}|\\mathbf{ f})\\right]\\triangleq \\log\\widetilde p(\\mathbf{ y}|\\mathbf{ u})$$\n",
    "\n",
    "No inversion of $\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}$ required\n",
    "\n",
    "(Titsias, n.d.){style=“text-align:right”}\n",
    "$$p(\\mathbf{ y}|\\mathbf{ u}) = \\frac{p(\\mathbf{ y}|\\mathbf{ f})p(\\mathbf{ f}|\\mathbf{ u})}{p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u})}$$\n",
    "$$\\log p(\\mathbf{ y}|\\mathbf{ u}) = \\log p(\\mathbf{ y}|\\mathbf{ f}) + \\log \\frac{p(\\mathbf{ f}|\\mathbf{ u})}{p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u})}$$\n",
    "$$\\log p(\\mathbf{ y}|\\mathbf{ u}) = \\bbE_{p(\\mathbf{ f}|\\mathbf{ u})}\\big[\\log p(\\mathbf{ y}|\\mathbf{ f})\\big] + \\bbE_{p(\\mathbf{ f}|\\mathbf{ u})}\\big[\\log \\frac{p(\\mathbf{ f}|\\mathbf{ u})}{p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u})}\\big]$$\n",
    "$$\\log p(\\mathbf{ y}|\\mathbf{ u}) = \\widetilde p(\\mathbf{ y}|\\mathbf{ u}) + \\textsc{KL}[p(\\mathbf{ f}|\\mathbf{ u})||p(\\mathbf{ f}|\\mathbf{ y}, \\mathbf{ u})]$$\n",
    "\n",
    "No inversion of $\\mathbf{K}_{\\mathbf{ f}\\mathbf{ f}}$ required\n",
    "\n",
    "$$\\widetilde p(\\mathbf{ y}|\\mathbf{ u})  = \\prod_{i=1}^n\\widetilde p(y_i|\\mathbf{ u})$$\n",
    "$$\\widetilde p(y|\\mathbf{ u}) = \\mathcal{N}\\left(y|\\mathbf{ k}_{fu}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}^{-1}\\mathbf{ u},\\sigma^2\\right) \\,{\\color{red}\\exp\\left\\{-\\tfrac{1}{2\\sigma^2}\\left(k_{ff}- \\mathbf{ k}_{fu}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}^{-1}\\mathbf{ k}_{uf}\\right)\\right\\}}$$\n",
    "\n",
    "A straightforward likelihood approximation, and a penalty term\n",
    "\n",
    "$$\\widetilde p(\\mathbf{ u}|\\mathbf{ y},\\mathbf{Z}) = \\frac{\\widetilde p(\\mathbf{ y}|\\mathbf{ u})p(\\mathbf{ u}|\\mathbf{Z})}{\\int \\widetilde p(\\mathbf{ y}|\\mathbf{ u})p(\\mathbf{ u}|\\mathbf{Z})\\text{d}{\\mathbf{ u}}}$$\n",
    "\n",
    "-   Computing the posterior costs $\\mathcal{O}(nm^2)$\n",
    "\n",
    "-   We also get a lower bound of the marginal likelihood\n",
    "\n",
    "$${\\color{red}\\sum_{i=1}^n-\\tfrac{1}{2\\sigma^2}\\left(k_{ff}- \\mathbf{ k}_{fu}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}^{-1}\\mathbf{ k}_{uf}\\right)}$$\n",
    "\n",
    "$${\\color{red}\\sum_{i=1}^n-\\tfrac{1}{2\\sigma^2}\\left(k_{ff}- \\mathbf{ k}_{fu}\\mathbf{K}_{\\mathbf{ u}\\mathbf{ u}}^{-1}\\mathbf{ k}_{uf}\\right)}$$\n",
    "\n",
    "<!--![image](../../../gp/tex/diagrams/cov_approx){width=\"60.00000%\"}-->\n",
    "<!--![image](../../../gp/tex/diagrams/cov_approx_opt){width=\"60.00000%\"}-->\n",
    "\n",
    "It’s easy to show that as $\\mathbf{Z}\\to \\mathbf{X}$:\n",
    "\n",
    "-   $\\mathbf{ u}\\to \\mathbf{ f}$ (and the posterior is exact)\n",
    "\n",
    "-   The penalty term is zero.\n",
    "\n",
    "-   The cost returns to $\\mathcal{O}(n^3)$\n",
    "\n",
    "-     \n",
    "\n",
    "-   \n",
    "\n",
    "So far we:\n",
    "\n",
    "-   introduced $\\mathbf{Z}, \\mathbf{ u}$\n",
    "\n",
    "-   approximated the intergral over $\\mathbf{ f}$ variationally\n",
    "\n",
    "-   captured the information in $\\widetilde p(\\mathbf{ u}|\\mathbf{ y})$\n",
    "\n",
    "-   obtained a lower bound on the marginal likeihood\n",
    "\n",
    "-   saw the effect of the penalty term\n",
    "\n",
    "-   prediction for new points\n",
    "\n",
    "Omitted details:\n",
    "\n",
    "-   optimization of the covariance parameters using the bound\n",
    "\n",
    "-   optimization of Z (simultaneously)\n",
    "\n",
    "-   the form of $\\widetilde p(\\mathbf{ u}|\\mathbf{ y})$\n",
    "\n",
    "-   historical approximations\n",
    "\n",
    "Subset selection (Lawrence et al., n.d.){style=“text-align:right”}\n",
    "\n",
    "-   Random or systematic\n",
    "\n",
    "-   Set $\\mathbf{Z}$ to subset of $\\mathbf{X}$\n",
    "\n",
    "-   Set $\\mathbf{ u}$ to subset of $\\mathbf{ f}$\n",
    "\n",
    "-   Approximation to $p(\\mathbf{ y}|\\mathbf{ u})$:\n",
    "\n",
    "    -   \\$ p(\\_i) = p(\\_i\\_i) i\\$\n",
    "\n",
    "    -   \\$ p(\\_i) = 1 i\\$\n",
    "\n",
    "(Quiñonero Candela and Rasmussen, 2005){style=“text-align:right”}\n",
    "{Deterministic Training Conditional (DTC)}\n",
    "\n",
    "-   Approximation to $p(\\mathbf{ y}|\\mathbf{ u})$:\n",
    "\n",
    "    -   \\$ p(\\_i) = (\\_i, \\[\\_i\\])\\$\n",
    "\n",
    "-   As our variational formulation, but without penalty\n",
    "\n",
    "Optimization of $\\mathbf{Z}$ is difficult\n",
    "\n",
    "Fully Independent Training Conditional (Snelson and Ghahramani,\n",
    "n.d.){style=“text-align:right”}\n",
    "\n",
    "-   Approximation to $p(\\mathbf{ y}|\\mathbf{ u})$:\n",
    "\n",
    "-   \\$ p() = \\_i p(\\_i) \\$\n",
    "\n",
    "Optimization of $\\mathbf{Z}$ is still difficult, and there are some\n",
    "weird heteroscedatic effects\n",
    "\n",
    "-   GP-LVM Provides probabilistic non-linear dimensionality reduction.\n",
    "-   How to select the dimensionality?\n",
    "-   Need to estimate marginal likelihood.\n",
    "-   In standard GP-LVM it increases with increasing $q$.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"40%\">\n",
    "\n",
    "**Bayesian GP-LVM**\n",
    "\n",
    "-   Start with a standard GP-LVM.\n",
    "-   Apply standard latent variable approach:\n",
    "    -   Define Gaussian prior over , $\\mathbf{Z}$.\n",
    "    -   Integrate out .\n",
    "    -   Unfortunately integration is intractable.\n",
    "\n",
    "</td>\n",
    "<td width=\"60%\">\n",
    "<center>\n",
    "\n",
    "{ }\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Variational Approach Fails\n",
    "-----------------------------------\n",
    "\n",
    "-   Standard variational bound has the form: $$\n",
    "    \\mathcal{L}= \\left<\\log p(\\mathbf{ y}|\\mathbf{Z})\\right>_{q(\\mathbf{Z})} + \\text{KL}\\left( q(\\mathbf{Z})\\,\\|\\,p(\\mathbf{Z}) \\right)\n",
    "    $$\n",
    "\n",
    "The standard variational approach would require the expectation of\n",
    "$\\log p(\\mathbf{ y}|\\mathbf{Z})$ under $q(\\mathbf{Z})$. $$\n",
    "  \\begin{align}\n",
    "  \\log p(\\mathbf{ y}|\\mathbf{Z}) = & -\\frac{1}{2}\\mathbf{ y}^\\top\\left(\\mathbf{K}_{\\mathbf{ f}, \\mathbf{ f}}+\\sigma^2\\mathbf{I}\\right)^{-1}\\mathbf{ y}\\\\ & -\\frac{1}{2}\\log \\det{\\mathbf{K}_{\\mathbf{ f}, \\mathbf{ f}}+\\sigma^2 \\mathbf{I}} -\\frac{n}{2}\\log 2\\pi\n",
    "  \\end{align}\n",
    "  $$ But this is extremely difficult to compute because\n",
    "$\\mathbf{K}_{\\mathbf{ f}, \\mathbf{ f}}$ is dependent on $\\mathbf{Z}$ and\n",
    "it appears in the inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Bayesian GP-LVM\n",
    "---------------------------\n",
    "\n",
    "The alternative approach is to consider the collapsed variational bound\n",
    "(used for low rank (sparse is a misnomer) Gaussian process\n",
    "approximations. $$\n",
    "    p(\\mathbf{ y})\\geq \\prod_{i=1}^nc_i \\int \\mathcal{N}\\left(\\mathbf{ y}|\\left<\\mathbf{ f}\\right>,\\sigma^2\\mathbf{I}\\right)p(\\mathbf{ u}) \\text{d}\\mathbf{ u}\n",
    "  $$ $$\n",
    "    p(\\mathbf{ y}|\\mathbf{Z})\\geq \\prod_{i=1}^nc_i \\int \\mathcal{N}\\left(\\mathbf{ y}|\\left<\\mathbf{ f}\\right>_{p(\\mathbf{ f}|\\mathbf{ u}, \\mathbf{Z})},\\sigma^2\\mathbf{I}\\right)p(\\mathbf{ u}) \\text{d}\\mathbf{ u}\n",
    "  $$ $$\n",
    "      \\int p(\\mathbf{ y}|\\mathbf{Z})p(\\mathbf{Z}) \\text{d}\\mathbf{Z}\\geq \\int \\prod_{i=1}^nc_i \\mathcal{N}\\left(\\mathbf{ y}|\\left<\\mathbf{ f}\\right>_{p(\\mathbf{ f}|\\mathbf{ u}, \\mathbf{Z})},\\sigma^2\\mathbf{I}\\right) p(\\mathbf{Z})\\text{d}\\mathbf{Z}p(\\mathbf{ u}) \\text{d}\\mathbf{ u}\n",
    "  $$\n",
    "\n",
    "To integrate across $\\mathbf{Z}$ we apply the lower bound to the inner\n",
    "integral. $$\n",
    "    \\begin{align}\n",
    "    \\int \\prod_{i=1}^nc_i \\mathcal{N}\\left(\\mathbf{ y}|\\left<\\mathbf{ f}\\right>_{p(\\mathbf{ f}|\\mathbf{ u}, \\mathbf{Z})},\\sigma^2\\mathbf{I}\\right) p(\\mathbf{Z})\\text{d}\\mathbf{Z}\\geq & \\left<\\sum_{i=1}^n\\log  c_i\\right>_{q(\\mathbf{Z})}\\\\ & +\\left<\\log\\mathcal{N}\\left(\\mathbf{ y}|\\left<\\mathbf{ f}\\right>_{p(\\mathbf{ f}|\\mathbf{ u}, \\mathbf{Z})},\\sigma^2\\mathbf{I}\\right)\\right>_{q(\\mathbf{Z})}\\\\& + \\text{KL}\\left( q(\\mathbf{Z})\\,\\|\\,p(\\mathbf{Z}) \\right)    \n",
    "    \\end{align}\n",
    "  $$ \\* Which is analytically tractable for Gaussian $q(\\mathbf{Z})$ and\n",
    "some covariance functions.\n",
    "\n",
    "-   Need expectations under $q(\\mathbf{Z})$ of: $$\n",
    "    \\log c_i = \\frac{1}{2\\sigma^2} \\left[k_{i, i} - \\mathbf{ k}_{i, \\mathbf{ u}}^\\top \\mathbf{K}_{\\mathbf{ u}, \\mathbf{ u}}^{-1} \\mathbf{ k}_{i, \\mathbf{ u}}\\right]\n",
    "    $$ and $$\n",
    "    \\log \\mathcal{N}\\left(\\mathbf{ y}|\\left<\\mathbf{ f}\\right>_{p(\\mathbf{ f}|\\mathbf{ u},\\mathbf{Y})},\\sigma^2\\mathbf{I}\\right) = -\\frac{1}{2}\\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^2}\\left(y_i - \\mathbf{K}_{\\mathbf{ f}, \\mathbf{ u}}\\mathbf{K}_{\\mathbf{ u},\\mathbf{ u}}^{-1}\\mathbf{ u}\\right)^2\n",
    "    $$\n",
    "\n",
    "-   This requires the expectations $$\n",
    "    \\left<\\mathbf{K}_{\\mathbf{ f},\\mathbf{ u}}\\right>_{q(\\mathbf{Z})}\n",
    "    $$ and $$\n",
    "    \\left<\\mathbf{K}_{\\mathbf{ f},\\mathbf{ u}}\\mathbf{K}_{\\mathbf{ u},\\mathbf{ u}}^{-1}\\mathbf{K}_{\\mathbf{ u},\\mathbf{ f}}\\right>_{q(\\mathbf{Z})}\n",
    "    $$ which can be computed analytically for some covariance functions\n",
    "    (Damianou et al., 2016) or through sampling (Damianou, 2015;\n",
    "    Salimbeni and Deisenroth, 2017).\n",
    "\n",
    "<span style=\"text-align:right\">\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip0\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Andreas Damianou\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/andreas-damianou.png\" clip-path=\"url(#clip0)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "</span>  \n",
    "(Damianou and Lawrence, 2013){style=“text-align:right”}  \n",
    "\n",
    "-   Augment each layer with inducing variables $\\mathbf{ u}_i$.\n",
    "\n",
    "-   Apply variational compression, where\n",
    "    $$\\tilde p(\\mathbf{ h}_i|\\mathbf{ u}_i,\\mathbf{ h}_{i-1})\n",
    "        = \\mathcal{N}\\left(\\mathbf{ h}_i|\\mathbf{K}_{\\mathbf{ h}_{i}\\mathbf{ u}_{i}}\\mathbf{K}_{\\mathbf{ u}_i\\mathbf{ u}_i}^{-1}\\mathbf{ u}_i,\\sigma^2_i\\mathbf{I}\\right).$$\n",
    "\n",
    "<span style=\"text-align:right\">\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip1\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "James Hensman\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/james-hensman.png\" clip-path=\"url(#clip1)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "</span>  \n",
    "(Hensman and Lawrence, 2014){style=“text-align:right”}\n",
    "\n",
    "-   By sustaining explicity distributions over inducing variables James\n",
    "    Hensman has developed a nested variant of variational compression.\n",
    "\n",
    "-   Exciting thing: it mathematically looks like a deep neural network,\n",
    "    but with inducing variables in the place of basis functions.\n",
    "\n",
    "-   Additional complexity control term in the objective function.\n",
    "\n",
    "$${\\only<1>{\\color{red}}\\log \\mathcal{N}\\left(\\mathbf{ y}|{\\only<2->{\\color{blue}}{\\boldsymbol\n",
    "          \\Psi}_{\\ell}}\\mathbf{K}_{\\mathbf{ u}_{\\ell}\n",
    "          \\mathbf{ u}_{\\ell}}^{-1}{\\mathbf\n",
    "          m}_\\ell,\\sigma^2_\\ell\\mathbf{I}\\right)}$$ where\n",
    "\n",
    "For Gaussian likelihoods:\n",
    "\n",
    "Define:\n",
    "$$q_{i, i} = \\text{var}_{p(f_i|\\mathbf{ u})}\\left( f_i \\right) = \\left<f_i^2\\right>_{p(f_i|\\mathbf{ u})} - \\left<f_i\\right>_{p(f_i|\\mathbf{ u})}^2$$\n",
    "We can write: $$c_i = \\exp\\left(-{\\frac{q_{i,i}}{2\\sigma^2}}\\right)$$ If\n",
    "joint distribution of $p(\\mathbf{ f}, \\mathbf{ u})$ is Gaussian then:\n",
    "$$q_{i, i} = k_{i, i} - \\mathbf{ k}_{i, \\mathbf{ u}}^\\top \\mathbf{K}_{\\mathbf{ u}, \\mathbf{ u}}^{-1} \\mathbf{ k}_{i, \\mathbf{ u}}$$\n",
    "\n",
    "$c_i$ is not a function of $\\mathbf{ u}$ but *is* a function of\n",
    "$\\mathbf{X}_\\mathbf{ u}$.\n",
    "\n",
    "Substitute variational bound into marginal likelihood:\n",
    "$$p(\\mathbf{ y})\\geq \\prod_{i=1}^nc_i \\int \\mathcal{N}\\left(\\mathbf{ y}|\\left<\\mathbf{ f}\\right>,\\sigma^2\\mathbf{I}\\right)p(\\mathbf{ u}) \\text{d}\\mathbf{ u}$$\n",
    "Note that:\n",
    "$$\\left<\\mathbf{ f}\\right>_{p(\\mathbf{ f}|\\mathbf{ u})} = \\mathbf{K}_{\\mathbf{ f}, \\mathbf{ u}} \\mathbf{K}_{\\mathbf{ u}, \\mathbf{ u}}^{-1}\\mathbf{ u}$$\n",
    "is *linearly* dependent on $\\mathbf{ u}$.\n",
    "\n",
    "Making the marginalization of $\\mathbf{ u}$ straightforward. In the\n",
    "Gaussian case:\n",
    "$$p(\\mathbf{ u}) = \\mathcal{N}\\left(\\mathbf{ u}|\\mathbf{0},\\mathbf{K}_{\\mathbf{ u},\\mathbf{ u}}\\right)$$\n",
    "\n",
    "-   Thang and Turner paper\n",
    "\n",
    "-   Joint Gaussianity is analytic, but not flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Simple Regression Problem\n",
    "---------------------------\n",
    "\n",
    "Here we set up a simple one dimensional regression problem. The input\n",
    "locations, $\\mathbf{X}$, are in two separate clusters. The response\n",
    "variable, $\\mathbf{ y}$, is sampled from a Gaussian process with an\n",
    "exponentiated quadratic covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPy: A Gaussian Process Framework in Python\n",
    "-------------------------------------------\n",
    "\n",
    "Gaussian processes are a flexible tool for non-parametric analysis with\n",
    "uncertainty. The GPy software was started in Sheffield to provide a easy\n",
    "to use interface to GPs. One which allowed the user to focus on the\n",
    "modelling rather than the mathematics.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gpy.png\" style=\"width:70%\">\n",
    "\n",
    "Figure: <i>GPy is a BSD licensed software code base for implementing\n",
    "Gaussian process models in Python. It is designed for teaching and\n",
    "modelling. We welcome contributions which can be made through the Github\n",
    "repository\n",
    "<a href=\"https://github.com/SheffieldML/GPy\" class=\"uri\">https://github.com/SheffieldML/GPy</a></i>\n",
    "\n",
    "GPy is a BSD licensed software code base for implementing Gaussian\n",
    "process models in python. This allows GPs to be combined with a wide\n",
    "variety of software libraries.\n",
    "\n",
    "The software itself is available on\n",
    "[GitHub](https://github.com/SheffieldML/GPy) and the team welcomes\n",
    "contributions.\n",
    "\n",
    "The aim for GPy is to be a probabilistic-style programming language,\n",
    "i.e. you specify the model rather than the algorithm. As well as a large\n",
    "range of covariance functions the software allows for non-Gaussian\n",
    "likelihoods, multivariate outputs, dimensionality reduction and\n",
    "approximations for larger data sets.\n",
    "\n",
    "The documentation for GPy can be found\n",
    "[here](https://gpy.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "noise_var = 0.01\n",
    "X = np.zeros((50, 1))\n",
    "X[:25, :] = np.linspace(0,3,25)[:,None] # First cluster of inputs/covariates\n",
    "X[25:, :] = np.linspace(7,10,25)[:,None] # Second cluster of inputs/covariates\n",
    "\n",
    "# Sample response variables from a Gaussian process with exponentiated quadratic covariance.\n",
    "k = GPy.kern.RBF(1)\n",
    "y = np.random.multivariate_normal(np.zeros(N),k.K(X)+np.eye(N)*np.sqrt(noise_var)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we perform a full Gaussian process regression on the data. We\n",
    "create a GP model, `m_full`, and fit it to the data, plotting the\n",
    "resulting fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(X,y)\n",
    "_ = m_full.optimize(messages=True) # Optimize parameters of covariance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai\n",
    "import teaching_plots as plot \n",
    "from gp_tutorial import gpplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_full, ax=ax, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2)\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='./gp/sparse-demo-full-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/sparse-demo-full-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Full Gaussian process fitted to the data set.</i>\n",
    "\n",
    "Now we set up the inducing variables, $\\mathbf{u}$. Each inducing\n",
    "variable has its own associated input index, $\\mathbf{Z}$, which lives\n",
    "in the same space as $\\mathbf{X}$. Here we are using the true covariance\n",
    "function parameters to generate the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = GPy.kern.RBF(1)\n",
    "Z = np.hstack(\n",
    "        (np.linspace(2.5,4.,3),\n",
    "        np.linspace(7,8.5,3)))[:,None]\n",
    "m = GPy.models.SparseGPRegression(X,y,kernel=kern,Z=Z)\n",
    "m.noise_var = noise_var\n",
    "m.inducing_inputs.constrain_fixed()\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, ax=ax, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2, xlim=xlim, ylim=ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='./gp/sparse-demo-constrained-inducing-6-unlearned-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/sparse-demo-constrained-inducing-6-unlearned-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Sparse Gaussian process fitted with six inducing variables,\n",
    "no optimization of parameters or inducing variables.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = m.optimize(messages=True)\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, ax=ax, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2, xlim=xlim, ylim=ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='./gp/sparse-demo-constrained-inducing-6-learned-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/sparse-demo-constrained-inducing-6-learned-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gaussian process fitted with inducing variables fixed and\n",
    "parameters optimized</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.randomize()\n",
    "m.inducing_inputs.unconstrain()\n",
    "_ = m.optimize(messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, ax=ax, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2,xlim=xlim, ylim=ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='./gp/sparse-demo-unconstrained-inducing-6-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/sparse-demo-unconstrained-inducing-6-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gaussian process fitted with location of inducing variables\n",
    "and parameters both optimized</i>\n",
    "\n",
    "Now we will vary the number of inducing points used to form the\n",
    "approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.num_inducing=8\n",
    "m.randomize()\n",
    "M = 8\n",
    "m.set_Z(np.random.rand(M,1)*12)\n",
    "\n",
    "_ = m.optimize(messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, ax=ax, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2, xlim=xlim, ylim=ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='./gp/sparse-demo-sparse-inducing-8-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/sparse-demo-sparse-inducing-8-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/sparse-demo-full-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Comparison of the full Gaussian process fit with a sparse\n",
    "Gaussian process using eight inducing varibles. Both inducing variables\n",
    "and parameters are optimized.</i>\n",
    "\n",
    "And we can compare the probability of the result to the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.log_likelihood(), m_full.log_likelihood())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non Gaussian Likelihoods\n",
    "========================\n",
    "\n",
    "\\[Gaussian processes model functions. If our observation is a corrupted\n",
    "version of this function and the corruption process is *also* Gaussian,\n",
    "it is trivial to account for this. However, there are many circumstances\n",
    "where our observation may be non Gaussian. In these cases we need to\n",
    "turn to approximate inference techniques. As a simple illustration,\n",
    "we’ll use a dataset of binary observations of the language that is\n",
    "spoken in different regions of East-Timor. First we will load the data\n",
    "and a couple of libraries to visualize it.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "import cPickle as pickle\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.urlretrieve('http://staffwww.dcs.sheffield.ac.uk/people/M.Zwiessele/gpss/lab2/EastTimor.pickle', 'EastTimor2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open(\"./EastTimor2.pickle\",\"rb\") as f:\n",
    "    X,y,polygons = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a map of East Timor and, using GPy, plot the data on\n",
    "top of it. A classification model can be defined in a similar way to the\n",
    "regression model, but now using `GPy.models.GPClassification`. However,\n",
    "once we’ve define the model, we also need to update the approximation to\n",
    "the likelihood. This runs the Expectation propagation updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize a map of East-Timor\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for p in polygons:\n",
    "    ax.add_collection(PatchCollection([Polygon(p)],facecolor=\"#F4A460\"))\n",
    "ax.set_xlim(124.,127.5)\n",
    "ax.set_ylim(-9.6,-8.1)\n",
    "ax.set_xlabel(\"longitude\")\n",
    "ax.set_ylabel(\"latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model\n",
    "kern = GPy.kern.RBF(2)\n",
    "m = GPy.models.GPClassification(X,y, kernel=kern)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary should be quite poor! However we haven’t optimized\n",
    "the model. Try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.optimize()\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization is based on the likelihood approximation that was made\n",
    "after we constructed the model. However, because we’ve now changed the\n",
    "model parameters the quality of that approximation has now probably\n",
    "deteriorated. To improve the model we should iterate between updating\n",
    "the Expectation propagation approximation and optimizing the model\n",
    "parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robust Regression: A Running Example\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olympic Marathon Data\n",
    "---------------------\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "\n",
    "-   Gold medal times for Olympic Marathon since 1896.\n",
    "-   Marathons before 1924 didn’t have a standardised distance.\n",
    "-   Present results using pace per km.\n",
    "-   In 1904 Marathon was badly organised leading to very slow times.\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/Stephen_Kiprotich.jpg\" style=\"width:100%\">\n",
    "<small>Image from Wikimedia Commons\n",
    "<a href=\"http://bit.ly/16kMKHQ\" class=\"uri\">http://bit.ly/16kMKHQ</a></small>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The first thing we will do is load a standard data set for regression\n",
    "modelling. The data consists of the pace of Olympic Gold Medal Marathon\n",
    "winners for the Olympics from 1896 to present. First we load in the data\n",
    "and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade git+https://github.com/sods/ods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "offset = y.mean()\n",
    "scale = np.sqrt(y.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = (1875,2030)\n",
    "ylim = (2.5, 6.5)\n",
    "yhat = (y-offset)/scale\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "mlai.write_figure(figure=fig, \n",
    "                  filename='olympic-marathon.svg', \n",
    "                  diagrams='./datasets',\n",
    "                  transparent=True, \n",
    "                  facecolor=(1, 1, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/datasets/olympic-marathon.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Olympic marathon pace times since 1892.</i>\n",
    "\n",
    "Things to notice about the data include the outlier in 1904, in this\n",
    "year, the olympics was in St Louis, USA. Organizational problems and\n",
    "challenges with dust kicked up by the cars following the race meant that\n",
    "participants got lost, and only very few participants completed.\n",
    "\n",
    "More recent years see more consistently quick marathons.\n",
    "\n",
    "We already considered the olympic marathon data. In 1904 we noted there\n",
    "was an outlier example. Today we’ll see if we can deal with that outlier\n",
    "by considering a non-Gaussian likelihood. Noise sampled from a\n",
    "Student-$t$ density is heavier tailed than that sampled from a Gaussian.\n",
    "However, it cannot be trivially assimilated into the Gaussian process.\n",
    "Below we use the *Laplace approximation* to incorporate this noise\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPy.likelihoods.noise_model_constructors.student_t?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPy.models.GPRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a student t likelihood with standard parameters\n",
    "# t_distribution = GPy.likelihoods.noise_model_constructors.student_t(deg_free=5, sigma2=2)\n",
    "# stu_t_likelihood = GPy.likelihoods.Laplace(Y.copy(), t_distribution)\n",
    "\n",
    "# kern = GPy.kern.RBF(1, lengthscale=10) + GPy.kern.Bias(1)\n",
    "# model = GPy.models.GPRegression(X, Y, kernel=kern, likelihood=stu_t_likelihood)\n",
    "\n",
    "t_distribution = GPy.likelihoods.StudentT(deg_free=5, sigma2=2)\n",
    "laplace = GPy.inference.latent_function_inference.Laplace()\n",
    "\n",
    "kern = GPy.kern.RBF(1, lengthscale=5) + GPy.kern.Bias(1)\n",
    "model = GPy.core.GP(X, Y, kernel=kern, inference_method=laplace, likelihood=t_distribution)\n",
    "model.constrain_positive('t_noise')\n",
    "\n",
    "model.optimize()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse GP Classification\n",
    "========================\n",
    "\n",
    "In this section we’ll combine expectation propagation with the low rank\n",
    "approximation to build a simple image classification application. For\n",
    "this toy example we’ll classify whether or not the subject of the image\n",
    "is wearing glasses.\n",
    "\n",
    "Correspond to whether the subject of the image is wearing glasses. Set\n",
    "up the ipython environment and download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let’s retrieve some data. We will use the ORL faces data set, our\n",
    "objective will be to classify whether or not the subject in an image is\n",
    "wearing glasess.\n",
    "\n",
    "Here’s a simple way to visualise the data. Each pixel in the image will\n",
    "become an input to the GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Xtest'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0b7c677ee666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lbls'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mXtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Xtest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mytest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ytest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'details'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'citation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Xtest'"
     ]
    }
   ],
   "source": [
    "data = pods.datasets.olivetti_faces()\n",
    "X = data['Y']\n",
    "y = data['lbls']\n",
    "Xtest = data['Xtest']\n",
    "ytest = data['Ytest']\n",
    "print(data['info'], data['details'], data['citation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 10304 into shape (64,64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-17cc844e376c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbig_figsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmlai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'olivetti-glasses-image.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 10304 into shape (64,64)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "ax.imshow(X[120].reshape(64, 64, order='F'),interpolation='nearest',cmap=pb.cm.gray)\n",
    "\n",
    "mlai.write_figure('olivetti-glasses-image.png', directory='./datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/'../slides/diagrams/datasets/olivetti-glasses-image.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>Image from the Oivetti glasses data sets.</i>\n",
    "\n",
    "Next we choose some inducing inputs. Here we’ve chosen inducing inputs\n",
    "by applying k-means clustering to the training data. Think about whether\n",
    "this is a good scheme for choosing the inputs? Can you devise a better\n",
    "one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n",
    "y.shape\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 8\n",
    "X = (X - X.mean(0)[None,:])/X.std(0)[None,:]\n",
    "Z = np.random.permutation(X)[:M]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we’re ready to build the classifier object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = GPy.kern.RBF(X.shape[1],lengthscale=20) + GPy.kern.White(X.shape[1],0.001)\n",
    "m = GPy.models.SparseGPClassification(X, y, kernel=kern, Z=Z)\n",
    "m.optimize()\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "ax.imshow(m.Z.gradient[0].reshape(64,64,order='F'),interpolation='nearest',cmap=pb.cm.gray)\n",
    "\n",
    "mlai.write_figure('olivetti-inducing-variable-gradients.png', directory='./gp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/'../slides/diagrams/datasets/olivetti-inducing-variable-gradients.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>The gradients of the inducing variable.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Álvarez, M.A., Luengo, D., Titsias, M.K., Lawrence, N.D., 2010.\n",
    "Efficient multioutput Gaussian processes through variational inducing\n",
    "kernels, in:. pp. 25–32.\n",
    "\n",
    "Bui, T.D., Yan, J., Turner, R.E., 2017. A unifying framework for\n",
    "Gaussian process pseudo-point approximations using power expectation\n",
    "propagation. Journal of Machine Learning Research 18, 1–72.\n",
    "\n",
    "Csató, L., 2002. Gaussian processes — iterative sparse approximations\n",
    "(PhD thesis). Aston University.\n",
    "\n",
    "Csató, L., Opper, M., 2002. Sparse on-line Gaussian processes. Neural\n",
    "Computation 14, 641–668.\n",
    "\n",
    "Dai, Z., Damianou, A., Hensman, J., Lawrence, N.D., 2014. Gaussian\n",
    "process models with parallelization and GPU acceleration.\n",
    "\n",
    "Damianou, A., 2015. Deep Gaussian processes and variational propagation\n",
    "of uncertainty (PhD thesis). University of Sheffield.\n",
    "\n",
    "Damianou, A., Lawrence, N.D., 2013. Deep Gaussian processes, in:. pp.\n",
    "207–215.\n",
    "\n",
    "Damianou, A., Titsias, M.K., Lawrence, N.D., 2016. Variational inference\n",
    "for latent variables and uncertain inputs in Gaussian processes. Journal\n",
    "of Machine Learning Research 17.\n",
    "\n",
    "Gal, Y., Wilk, M. van der, Rasmussen, C.E., n.d. Distributed variational\n",
    "inference in sparse Gaussian process regression and latent variable\n",
    "models, in:.\n",
    "\n",
    "Hensman, J., Fusi, N., Lawrence, N.D., n.d. Gaussian processes for big\n",
    "data, in:.\n",
    "\n",
    "Hensman, J., Lawrence, N.D., 2014. Nested variational compression in\n",
    "deep Gaussian processes. University of Sheffield.\n",
    "\n",
    "Hensman, J., Rattray, M., Lawrence, N.D., 2012. Fast variational\n",
    "inference in the conjugate exponential family, in:.\n",
    "\n",
    "Hoffman, M., Blei, D.M., Wang, C., Paisley, J., 2012. Stochastic\n",
    "variational inference, arXiv preprint arXiv:1206.7051.\n",
    "\n",
    "King, N.J., Lawrence, N.D., n.d. Fast variational inference for Gaussian\n",
    "Process models through KL-correction, in:. pp. 270–281.\n",
    "\n",
    "Lawrence, N.D., n.d. Learning for larger datasets with the Gaussian\n",
    "process latent variable model, in:. pp. 243–250.\n",
    "\n",
    "Lawrence, N.D., Seeger, M., Herbrich, R., n.d. Fast sparse Gaussian\n",
    "process methods: The informative vector machine, in:. pp. 625–632.\n",
    "\n",
    "Lázaro-Gredilla, M., Quiñonero-Candela, J., Rasmussen, C.E., 2010.\n",
    "Sparse spectrum gaussian processes. Journal of Machine Learning Research\n",
    "11, 1865–1881.\n",
    "\n",
    "Quiñonero Candela, J., Rasmussen, C.E., 2005. A unifying view of sparse\n",
    "approximate Gaussian process regression. Journal of Machine Learning\n",
    "Research 6, 1939–1959.\n",
    "\n",
    "Salimbeni, H., Deisenroth, M., 2017. Doubly stochastic variational\n",
    "inference for deep Gaussian processes, in: Guyon, I., Luxburg, U.V.,\n",
    "Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R.\n",
    "(Eds.), Advances in Neural Information Processing Systems 30. Curran\n",
    "Associates, Inc., pp. 4591–4602.\n",
    "\n",
    "Seeger, M.W., Hetzel, A., Dai, Z., Lawrence, N.D., 2017.\n",
    "Auto-differentiating linear algebra. CoRR abs/1710.08717.\n",
    "\n",
    "Seeger, M., Williams, C.K.I., Lawrence, N.D., n.d. Fast forward\n",
    "selection to speed up sparse Gaussian process regression, in:.\n",
    "\n",
    "Smola, A.J., Bartlett, P.L., n.d. Sparse greedy Gaussian process\n",
    "regression, in:. pp. 619–625.\n",
    "\n",
    "Snelson, E., Ghahramani, Z., n.d. Sparse Gaussian processes using\n",
    "pseudo-inputs, in:.\n",
    "\n",
    "Titsias, M.K., n.d. Variational learning of inducing variables in sparse\n",
    "Gaussian processes, in:. pp. 567–574.\n",
    "\n",
    "Williams, C.K.I., Seeger, M., n.d. Using the Nyström method to speed up\n",
    "kernel machines, in:. pp. 682–688."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
