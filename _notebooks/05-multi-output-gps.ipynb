{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-output Gaussian Processes\n",
    "===============================\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), University of\n",
    "\n",
    "Cambridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: In this talk we review multi-output Gaussian processes.\n",
    "Introducing them initially through a Kalman filter representation of a\n",
    "GP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\tk}[1]{}\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\uniformDist}[3]{\\mathcal{U}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\uniformSamp}[2]{\\mathcal{U}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- To compile -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Kalman Filter\n",
    "--------------------\n",
    "\n",
    "-   We have state vector\n",
    "    $\\mathbf{X}= \\left[\\mathbf{ x}_1 \\dots \\mathbf{ x}_q\\right] \\in \\mathbb{R}^{T \\times q}$\n",
    "    and if each state evolves independently we have $$      \n",
    "    \\begin{align*}\n",
    "      p(\\mathbf{X}) &= \\prod_{i=1}^qp(\\mathbf{ x}_{:, i}) \\\\\n",
    "     p(\\mathbf{ x}_{:, i}) &= \\mathcal{N}\\left(\\mathbf{ x}_{:, i}|\\mathbf{0},\\mathbf{K}\\right).\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "-   We want to obtain outputs through: $$\n",
    "    \\mathbf{ y}_{i, :} = \\mathbf{W}\\mathbf{ x}_{i, :}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking and Kronecker Products\n",
    "-------------------------------\n",
    "\n",
    "-   Represent with a ‘stacked’ system: $$\n",
    "    p(\\mathbf{ x}) = \\mathcal{N}\\left(\\mathbf{ x}|\\mathbf{0},\\mathbf{I}\\otimes \\mathbf{K}\\right)\n",
    "    $$ where the stacking is placing each column of $\\mathbf{X}$ one on\n",
    "    top of another as $$\n",
    "    \\mathbf{ x}= \\begin{bmatrix}\n",
    "          \\mathbf{ x}_{:, 1}\\\\\n",
    "          \\mathbf{ x}_{:, 2}\\\\\n",
    "          \\vdots\\\\\n",
    "          \\mathbf{ x}_{:, q}\n",
    "        \\end{bmatrix}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kronecker Product\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.kronecker_illustrate(diagrams='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/kronecker_illustrate.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Illustration of the Kronecker product.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.kronecker_IK(diagrams='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/kronecker_IK.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Kronecker product between two matrices.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking and Kronecker Products\n",
    "-------------------------------\n",
    "\n",
    "-   Represent with a ‘stacked’ system: $$p\n",
    "    (\\mathbf{ x}) = \\mathcal{N}\\left(\\mathbf{ x}|\\mathbf{0},\\mathbf{I}\\otimes \\mathbf{K}\\right)\n",
    "    $$ where the stacking is placing each column of $\\mathbf{X}$ one on\n",
    "    top of another as $$\n",
    "    \\mathbf{ x}= \\begin{bmatrix}\n",
    "          \\mathbf{ x}_{:, 1}\\\\\n",
    "          \\mathbf{ x}_{:, 2}\\\\\n",
    "          \\vdots\\\\\n",
    "          \\mathbf{ x}_{:, q}\n",
    "        \\end{bmatrix}\n",
    "        $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column Stacking\n",
    "---------------\n",
    "\n",
    "For this stacking the marginal distribution over *time* is given by the\n",
    "block diagonals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.kronecker_IK_highlight(diagrams='./kern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('kronecker_IK_highlighted{count:0>3}.svg', \n",
    "                            diagrams='./kern', count=(1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/kronecker_IK_highlighted005.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The marginal distribution for the first three variables\n",
    "(highlighted) is givne by $\\mathbf{K}$ when the form of stacking is\n",
    "$\\mathbf{I}\\otimes \\mathbf{K}$. This is the ‘column stacking’ case.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two Ways of Stacking\n",
    "--------------------\n",
    "\n",
    "Can also stack each row of $\\mathbf{X}$ to form column vector:\n",
    "$$\\mathbf{ x}= \\begin{bmatrix}\n",
    "      \\mathbf{ x}_{1, :}\\\\\n",
    "      \\mathbf{ x}_{2, :}\\\\\n",
    "      \\vdots\\\\\n",
    "      \\mathbf{ x}_{T, :}\n",
    "    \\end{bmatrix}$$\n",
    "$$p(\\mathbf{ x}) = \\mathcal{N}\\left(\\mathbf{ x}|\\mathbf{0},\\mathbf{K}\\otimes \\mathbf{I}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row Stacking\n",
    "------------\n",
    "\n",
    "For this stacking the marginal distribution over the latent *dimensions*\n",
    "is given by the block diagonals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.kronecker_IK_highlight(reverse=True, diagrams='./kern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('kronecker_KI_highlighted{count:0>3}.svg', './kern', count=(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.kronecker_IK(reverse=True, diagrams='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/kronecker_KI_highlighted002.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The marginal distribution for the first three variables\n",
    "(highlighted) is independent when the form of stacking is\n",
    "$\\mathbf{K}\\otimes \\mathbf{I}$. This is the ‘row stacking’ case.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping from Latent Process to Observed\n",
    "---------------------------------------\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/kronecker_KI.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Mapping from latent process to observed</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed Process\n",
    "----------------\n",
    "\n",
    "The observations are related to the latent points by a linear mapping\n",
    "matrix, $$\n",
    "\\mathbf{ y}_{i, :} = \\mathbf{W}\\mathbf{ x}_{i, :} + \\boldsymbol{ \\epsilon}_{i, :}\n",
    "$$ $$\n",
    "\\boldsymbol{ \\epsilon}\\sim \\mathcal{N}\\left(\\mathbf{0},\\sigma^2\\mathbf{I}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.kronecker_WX(diagrams='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/kronecker_WX.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Covariance\n",
    "-----------------\n",
    "\n",
    "This leads to a covariance of the form $$\n",
    "(\\mathbf{I}\\otimes \\mathbf{W}) (\\mathbf{K}\\otimes \\mathbf{I}) (\\mathbf{I}\\otimes \\mathbf{W}^\\top) + \\mathbf{I}\\sigma^2\n",
    "$$ Using\n",
    "$(\\mathbf{A}\\otimes\\mathbf{B}) (\\mathbf{C}\\otimes\\mathbf{D}) = \\mathbf{A}\\mathbf{C} \\otimes \\mathbf{B}\\mathbf{D}$\n",
    "This leads to $$\n",
    "\\mathbf{K}\\otimes {\\mathbf{W}}{\\mathbf{W}}^\\top + \\mathbf{I}\\sigma^2\n",
    "$$ or $$\n",
    "\\mathbf{ y}\\sim \\mathcal{N}\\left(\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top \\otimes \\mathbf{K}+ \\mathbf{I}\\sigma^2\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernels for Vector Valued Outputs: A Review\n",
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kronecker Structure GPs\n",
    "-----------------------\n",
    "\n",
    "-   This Kronecker structure leads to several published models. $$\n",
    "    (\\mathbf{K}(\\mathbf{ x},\\mathbf{ x}^\\prime))_{i,i^\\prime}=k(\\mathbf{ x},\\mathbf{ x}^\\prime)k_T(i,i^\\prime),\n",
    "    $$ where $k$ has $\\mathbf{ x}$ and $k_T$ has $n$ as inputs.\n",
    "\n",
    "-   Can think of multiple output covariance functions as covariances\n",
    "    with augmented input.\n",
    "\n",
    "-   Alongside $\\mathbf{ x}$ we also input the $i$ associated with the\n",
    "    *output* of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separable Covariance Functions\n",
    "------------------------------\n",
    "\n",
    "-   Taking $\\mathbf{B}= {\\mathbf{W}}{\\mathbf{W}}^\\top$ we have a matrix\n",
    "    expression across outputs.\n",
    "    $$\\mathbf{K}(\\mathbf{ x},\\mathbf{ x}^\\prime)=k(\\mathbf{ x},\\mathbf{ x}^\\prime)\\mathbf{B},$$\n",
    "    where $\\mathbf{B}$ is a $p\\times p$ symmetric and positive\n",
    "    semi-definite matrix.\n",
    "\n",
    "-   $\\mathbf{B}$ is called the *coregionalization* matrix.\n",
    "\n",
    "-   We call this class of covariance functions *separable* due to their\n",
    "    product structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum of Separable Covariance Functions\n",
    "-------------------------------------\n",
    "\n",
    "-   In the same spirit a more general class of kernels is given by\n",
    "    $$\\mathbf{K}(\\mathbf{ x},\\mathbf{ x}^\\prime)=\\sum_{j=1}^qk_{j}(\\mathbf{ x},\\mathbf{ x}^\\prime)\\mathbf{B}_{j}.$$\n",
    "\n",
    "-   This can also be written as\n",
    "    $$\\mathbf{K}(\\mathbf{X}, \\mathbf{X}) = \\sum_{j=1}^q\\mathbf{B}_{j}\\otimes k_{j}(\\mathbf{X}, \\mathbf{X}),$$\n",
    "\n",
    "-   This is like several Kalman filter-type models added together, but\n",
    "    each one with a different set of latent functions.\n",
    "\n",
    "-   We call this class of kernels sum of separable kernels (SoS\n",
    "    kernels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geostatistics\n",
    "-------------\n",
    "\n",
    "-   Use of GPs in Geostatistics is called kriging.\n",
    "\n",
    "-   These multi-output GPs pioneered in geostatistics: prediction over\n",
    "    vector-valued output data is known as *cokriging*.\n",
    "\n",
    "-   The model in geostatistics is known as the *linear model of\n",
    "    coregionalization* (LMC, Journel and Huijbregts (1978)\n",
    "    Goovaerts (1997)).\n",
    "\n",
    "-   Most machine learning multitask models can be placed in the context\n",
    "    of the LMC model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted sum of Latent Functions\n",
    "--------------------------------\n",
    "\n",
    "-   In the linear model of coregionalization (LMC) outputs are expressed\n",
    "    as linear combinations of independent random functions.\n",
    "\n",
    "-   In the LMC, each component $f_i$ is expressed as a linear sum\n",
    "    $$f_i(\\mathbf{ x}) = \\sum_{j=1}^q{w}_{i,{j}}{u}_{j}(\\mathbf{ x}).$$\n",
    "    where the latent functions are independent and have covariance\n",
    "    functions $k_{j}(\\mathbf{ x},\\mathbf{ x}^\\prime)$.\n",
    "\n",
    "-   The processes $\\{f_j(\\mathbf{ x})\\}_{j=1}^q$ are independent for\n",
    "    $q\\neq {j}^\\prime$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kalman Filter Special Case\n",
    "--------------------------\n",
    "\n",
    "-   The Kalman filter is an example of the LMC where\n",
    "    ${u}_i(\\mathbf{ x}) \\rightarrow {x}_i(t)$.\n",
    "\n",
    "-   I.e. we’ve moved form time input to a more general input space.\n",
    "\n",
    "-   In matrix notation:\n",
    "\n",
    "    1.  Kalman filter $$\\mathbf{F}= {\\mathbf{W}}\\mathbf{X}$$\n",
    "    2.  LMC $$\\mathbf{F}= {\\mathbf{W}}{\\mathbf{U}}$$ where the rows of\n",
    "        these matrices ${\\mathbf{F}}$, $\\mathbf{X}$, ${\\mathbf{U}}$ each\n",
    "        contain $q$ samples from their corresponding functions at a\n",
    "        different time (Kalman filter) or spatial location (LMC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrinsic Coregionalization Model\n",
    "---------------------------------\n",
    "\n",
    "-   If one covariance used for latent functions (like in Kalman filter).\n",
    "\n",
    "-   This is called the intrinsic coregionalization model (ICM,\n",
    "    Goovaerts (1997)).\n",
    "\n",
    "-   The kernel matrix corresponding to a dataset $\\mathbf{X}$ takes the\n",
    "    form $$\n",
    "    \\mathbf{K}(\\mathbf{X}, \\mathbf{X}) =  \\mathbf{B}\\otimes k(\\mathbf{X}, \\mathbf{X}).\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autokrigeability\n",
    "----------------\n",
    "\n",
    "-   If outputs are noise-free, maximum likelihood is equivalent to\n",
    "    independent fits of $\\mathbf{B}$ and\n",
    "    $k(\\mathbf{ x}, \\mathbf{ x}^\\prime)$ (Helterbrand and\n",
    "    Cressie, 1994).\n",
    "\n",
    "-   In geostatistics this is known as autokrigeability\n",
    "    (Wackernagel, 2003).\n",
    "\n",
    "-   In multitask learning its the cancellation of intertask transfer\n",
    "    (Bonilla et al., n.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrinsic Coregionalization Model\n",
    "---------------------------------\n",
    "\n",
    "$$\n",
    "\\mathbf{K}(\\mathbf{X}, \\mathbf{X}) =  \\mathbf{ w}\\mathbf{ w}^\\top  \\otimes k(\\mathbf{X}, \\mathbf{X}).\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{ w}= \\begin{bmatrix} 1 \\\\ 5\\end{bmatrix}\n",
    "$$ $$\n",
    "\\mathbf{B}= \\begin{bmatrix} 1 & 5\\\\ 5&25\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<!--![image](../../../multigp/tex/diagrams/icmCovarianceImage)![image](../../../multigp/tex/diagrams/icmCovarianceSample1)![image](../../../multigp/tex/diagrams/icmCovarianceSample2)![image](../../../multigp/tex/diagrams/icmCovarianceSample3)![image](../../../multigp/tex/diagrams/icmCovarianceSample4)-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrinsic Coregionalization Model Covariance\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s icm_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, anim=plot.animate_covariance_function(mlai.compute_kernel, \n",
    "                                         kernel=icm_cov, subkernel=eq_cov,\n",
    "                                         B = np.asarray([[1, 0.5],[0.5, 1.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.save_animation(anim, \n",
    "                    diagrams='./kern', \n",
    "                    filename='icm_covariance.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "$$k(i, j, \\mathbf{ x}, \\mathbf{ x}^\\prime) = b_{i,j} k(\\mathbf{ x}, \\mathbf{ x}^\\prime)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/icm_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/icm_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Intrinsic coregionalization model covariance function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrinsic Coregionalization Model\n",
    "---------------------------------\n",
    "\n",
    "$$\n",
    "\\mathbf{K}(\\mathbf{X}, \\mathbf{X}) =  \\mathbf{B}\\otimes k(\\mathbf{X}, \\mathbf{X}).\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{B}= \\begin{bmatrix} 1 & 0.5\\\\ 0.5& 1.5\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<!--![image](../../../multigp/tex/diagrams/icm2CovarianceImage)![image](../../../multigp/tex/diagrams/icm2CovarianceSample1)![image](../../../multigp/tex/diagrams/icm2CovarianceSample2)![image](../../../multigp/tex/diagrams/icm2CovarianceSample3)![image](../../../multigp/tex/diagrams/icm2CovarianceSample4)-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMC Samples\n",
    "-----------\n",
    "\n",
    "$$\\mathbf{K}(\\mathbf{X}, \\mathbf{X}) = \\mathbf{B}_1 \\otimes k_1(\\mathbf{X}, \\mathbf{X}) + \\mathbf{B}_2 \\otimes k_2(\\mathbf{X}, \\mathbf{X})$$\n",
    "\n",
    "$$\\mathbf{B}_1 = \\begin{bmatrix} 1.4 & 0.5\\\\ 0.5& 1.2\\end{bmatrix}$$\n",
    "$${\\ell}_1 = 1$$\n",
    "$$\\mathbf{B}_2 = \\begin{bmatrix} 1 & 0.5\\\\ 0.5& 1.3\\end{bmatrix}$$\n",
    "$${\\ell}_2 = 0.2$$\n",
    "\n",
    "<!--![image](../../../multigp/tex/diagrams/lmc2CovarianceImage)![image](../../../multigp/tex/diagrams/lmc2CovarianceSample1)![image](../../../multigp/tex/diagrams/lmc2CovarianceSample2)![image](../../../multigp/tex/diagrams/lmc2CovarianceSample3)![image](../../../multigp/tex/diagrams/lmc2CovarianceSample4)-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMC in Machine Learning and Statistics\n",
    "--------------------------------------\n",
    "\n",
    "-   Used in machine learning for GPs for multivariate regression and in\n",
    "    statistics for computer emulation of expensive multivariate computer\n",
    "    codes.\n",
    "\n",
    "-   Imposes the correlation of the outputs explicitly through the set of\n",
    "    coregionalization matrices.\n",
    "\n",
    "-   Setting $\\mathbf{B}= \\mathbf{I}_p$ assumes outputs are conditionally\n",
    "    independent given the parameters $\\boldsymbol{ \\theta}$. (Lawrence\n",
    "    and Platt, 2004; Minka and Picard, 1997; Yu et al., 2005).\n",
    "\n",
    "-   More recent approaches for multiple output modeling are different\n",
    "    versions of the linear model of coregionalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semiparametric Latent Factor Model\n",
    "----------------------------------\n",
    "\n",
    "-   Coregionalization matrices are rank 1 Teh et al. (n.d.). rewrite\n",
    "    equation as\n",
    "    $$\\mathbf{K}(\\mathbf{X}, \\mathbf{X}) = \\sum_{j=1}^q\\mathbf{ w}_{:, {j}}\\mathbf{ w}^{\\top}_{:, {j}} \\otimes k_{j}(\\mathbf{X}, \\mathbf{X}).$$\n",
    "\n",
    "-   Like the Kalman filter, but each latent function has a *different*\n",
    "    covariance.\n",
    "\n",
    "-   Authors suggest using an exponentiated quadratic characteristic\n",
    "    length-scale for each input dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi Parametric Latent Factor Covariance\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s icm_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s slfm_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, anim=plot.animate_covariance_function(mlai.compute_kernel, \n",
    "                                         kernel=slfm_cov, subkernel=eq_cov,\n",
    "                                         W = np.asarray([[1],[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.save_animation(anim, \n",
    "                    diagrams='./kern', \n",
    "                    filename='slfm_covariance.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semiparametric Latent Factor Model Samples\n",
    "------------------------------------------\n",
    "\n",
    "$$\n",
    "\\mathbf{K}(\\mathbf{X}, \\mathbf{X}) = \\mathbf{ w}_{:, 1}\\mathbf{ w}_{:, 1}^\\top \\otimes k_1(\\mathbf{X}, \\mathbf{X}) + \\mathbf{ w}_{:, 2} \\mathbf{ w}_{:, 2}^\\top \\otimes k_2(\\mathbf{X}, \\mathbf{X})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{ w}_1 = \\begin{bmatrix} 0.5 \\\\ 1\\end{bmatrix}\n",
    "$$ $$\n",
    "\\mathbf{ w}_2 = \\begin{bmatrix} 1 \\\\ 0.5\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<!--![image](../../../multigp/tex/diagrams/slfmCovarianceImage)![image](../../../multigp/tex/diagrams/slfmCovarianceSample1)![image](../../../multigp/tex/diagrams/slfmCovarianceSample2)![image](../../../multigp/tex/diagrams/slfmCovarianceSample3)![image](../../../multigp/tex/diagrams/slfmCovarianceSample4)-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian processes for Multi-task, Multi-output and Multi-class\n",
    "---------------------------------------------------------------\n",
    "\n",
    "-   Bonilla et al. (n.d.) suggest ICM for multitask learning.\n",
    "\n",
    "-   Use a PPCA form for $\\mathbf{B}$: similar to our Kalman filter\n",
    "    example.\n",
    "\n",
    "-   Refer to the autokrigeability effect as the cancellation of\n",
    "    inter-task transfer.\n",
    "\n",
    "-   Also discuss the similarities between the multi-task GP and the ICM,\n",
    "    and its relationship to the SLFM and the LMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multitask Classification\n",
    "------------------------\n",
    "\n",
    "-   Mostly restricted to the case where the outputs are conditionally\n",
    "    independent given the hyperparameters $\\boldsymbol{\\phi}$ (Lawrence\n",
    "    and Platt, 2004; Minka and Picard, 1997; Rasmussen and Williams,\n",
    "    2006; Seeger and Jordan, 2004; Williams and Barber, 1998; Yu et\n",
    "    al., 2005).\n",
    "\n",
    "-   Intrinsic coregionalization model has been used in the multiclass\n",
    "    scenario. Skolidis and Sanguinetti (2011) use the intrinsic\n",
    "    coregionalization model for classification, by introducing a probit\n",
    "    noise model as the likelihood.\n",
    "\n",
    "-   Posterior distribution is no longer analytically tractable:\n",
    "    approximate inference is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer Emulation\n",
    "------------------\n",
    "\n",
    "-   A statistical model used as a surrogate for a computationally\n",
    "    expensive computer model.\n",
    "\n",
    "-   Higdon et al. (2008) use the linear model of coregionalization to\n",
    "    model images representing the evolution of the implosion of steel\n",
    "    cylinders.\n",
    "\n",
    "-   In Conti and O’Hagan (2009) use the ICM to model a vegetation model:\n",
    "    called the Sheffield Dynamic Global Vegetation Model Woodward et\n",
    "    al. (1998)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling Multiple Outputs\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Example\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olympic Sprint Data\n",
    "-------------------\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "\n",
    "-   Gold medal times for Olympic Sprints for Men and Women\n",
    "-   100m, 200m, 400m\n",
    "-   In early years of olympics not all events run.\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/100m_final_start.jpg\" style=\"width:100%\">\n",
    "<small>Image from Wikimedia Commons\n",
    "<a href=\"http://bit.ly/16kMKHQ\" class=\"uri\">http://bit.ly/16kMKHQ</a> by\n",
    "[Darren Wilkinson](https://www.staff.ncl.ac.uk/d.j.wilkinson/)</small>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade git+https://github.com/sods/ods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first think we will look at is a multiple output model. Our aim is\n",
    "to jointly model all *sprinting* events from olympics since 1896. Data\n",
    "is provided by Rogers & Girolami’s “First Course in Machine Learning”.\n",
    "Firstly, let’s load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.datasets.authorize_download = lambda x: True # prevents requesting authorization for download.\n",
    "data = pods.datasets.olympic_sprints()\n",
    "X = data['X']\n",
    "y = data['Y']\n",
    "print(data['info'], data['details'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using data sets it’s good practice to cite the originators of the\n",
    "data, you can get information about the source of the data from\n",
    "`data['citation']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['citation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of all the male and female sprinting data for 100m,\n",
    "200m and 400m since 1896 (six outputs in total). The ouput information\n",
    "can be found from: `data['output_info']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['output_info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `GPy` we deal with multiple output data in a particular way. We\n",
    "specify the output we are interested in for modelling as an additional\n",
    "*input*. So whilst for this data, normally, the only input would be the\n",
    "year of the event. We additionally have an input giving the index of the\n",
    "output we are modelling. This can be seen from examining `data['X']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First column of X contains the olympic years.')\n",
    "print(data['X'][:, 0])\n",
    "print('Second column of X contains the event index.')\n",
    "print(data['X'][:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=plot.big_wide_figsize)\n",
    "markers = ['bo', 'ro', 'bx', 'rx', 'bs', 'rs']\n",
    "for i in range(6):\n",
    "    # extract the event \n",
    "    x_event = X[np.nonzero(X[:, 1]==i), 0]\n",
    "    y_event = y[np.nonzero(X[:, 1]==i), 0]\n",
    "    ax.plot(x_event, y_event, markers[i])\n",
    "ax.title('Olympic Sprint Times')\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('time/s')\n",
    "\n",
    "mlai.write_figure('olympic-sprint-data.svg', directory='./datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/datasets/olympic-sprint-data.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Olympic sprint gold medal winning times from Rogers and\n",
    "Girolami (2011).</i>\n",
    "\n",
    "In the plot above red is women’s events, blue is men’s. Squares are 400\n",
    "m, crosses 200m and circles 100m. Not all events were run in all years,\n",
    "for example the women’s 400 m only started in 1964."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Fit\n",
    "--------------------\n",
    "\n",
    "We will perform a multi-output Gaussian process fit to the data, we’ll\n",
    "do this using the [GPy software](https://github.com/SheffieldML/GPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at modelling the data using coregionalization approaches\n",
    "described in this morning’s lecture. We introduced these approaches\n",
    "through the Kronecker product. To indicate we want to construct a\n",
    "covariance function of this type in GPy we’ve overloaded the `**`\n",
    "operator. Stricly speaking this operator means to the power of (like `^`\n",
    "in MATLAB). But for covariance functions we’ve used it to indicate a\n",
    "tensor product. The linear models of coregionalization we introduced in\n",
    "the lecture were all based on combining a matrix with a standard\n",
    "covariance function. We can think of the matrix as a particular type of\n",
    "covariance function, whose elements are referenced using the event\n",
    "indices. I.e. $k(0, 0)$ references the first row and column of the\n",
    "coregionalization matrix. $k(1, 0)$ references the second row and first\n",
    "column of the coregionalization matrix. Under this set up, we want to\n",
    "build a covariance where the first column from the features (the years)\n",
    "is passed to a covariance function, and the second column from the\n",
    "features (the event number) is passed to the coregionalisation matrix.\n",
    "Let’s start by trying a intrinsic coregionalisation model (sometimes\n",
    "known as multitask Gaussian processes). Let’s start by checking the help\n",
    "for the `Coregionalize` covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPy.kern.Coregionalize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coregionalize matrix, $\\mathbf{B}$, is itself is constructed from\n",
    "two other matrices,\n",
    "$\\mathbf{B} = \\mathbf{W}\\mathbf{W}^\\top + \\text{diag}(\\boldsymbol{\\kappa})$.\n",
    "This allows us to specify a low rank form for the coregionalization\n",
    "matrix. However, for our first example we want to specify that the\n",
    "matrix $\\mathbf{B}$ is not constrained to have a low rank form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = GPy.kern.RBF(1, lengthscale=80)**GPy.kern.Coregionalize(1,output_dim=6, rank=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that the rank we specify is that of the\n",
    "$\\mathbf{W}\\mathbf{W}^\\top$ part. When this part is combined with the\n",
    "diagonal matrix from $\\mathbf{\\kappa}$ the matrix $\\mathbf{B}$ is\n",
    "totally general. This covariance function can now be used in a standard\n",
    "Gaussian process regression model. Let’s build the model and optimize\n",
    "it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPy.models.GPRegression(X, y, kern)\n",
    "model.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the results using the ability to ‘fix inputs’ in the\n",
    "`model.plot()` function. We can specify that column 1 should be fixed to\n",
    "event number 2 by passing `fixed_inputs = [(1, 2)]` to the model. To\n",
    "plot the results for all events on the same figure we also specify\n",
    "`fignum=1` in the loop as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "for i in range(6):\n",
    "    model.plot(ax = ax, fixed_inputs=[(1, i)])\n",
    "ax.set_xlabel('years')\n",
    "ax.set_ylabel('time/s')\n",
    "\n",
    "mla.write_figure('olympic-sprint-gp.svg',\n",
    "                 directory='./gp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/olympic-sprint-gp.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gaussian process fit to the Olympic Sprint data.</i>\n",
    "\n",
    "There is a lot we can do with this model. First of all, each of the\n",
    "races is a different length, so the series have a different mean. We can\n",
    "include another coregionalization term to deal with the mean. Below we\n",
    "do this and reduce the rank of the coregionalization matrix to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern1 = GPy.kern.RBF(1, lengthscale=80)**GPy.kern.Coregionalize(1, output_dim=6, rank=1)\n",
    "kern2 = GPy.kern.Bias(1)**GPy.kern.Coregionalize(1,output_dim=6, rank=1)\n",
    "kern = kern1 + kern2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPy.models.GPRegression(X, y, kern)\n",
    "model.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "for i in range(6):\n",
    "    model.plot(ax=ax, fignum=1,fixed_inputs=[(1, i)])\n",
    "ax.set_xlabel('years')\n",
    "ax.set_ylabel('time/s')\n",
    "\n",
    "mla.write_figure('olympic-sprint-lmc-gp.svg',\n",
    "                 directory='./gp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/olympic-sprint-lmc-gp.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gaussian process fit to the Olympic Sprint data.</i>\n",
    "\n",
    "This is a simple form of the linear model of coregionalization. Note how\n",
    "confident the model is about what the women’s 400 m performance would\n",
    "have been. You might feel that the model is being over confident in this\n",
    "region. Perhaps we are forcing too much sharing of information between\n",
    "the sprints. We could return to the intrinsic coregionalization model\n",
    "and force the two base covariance functions to share the same\n",
    "coregionalization matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern1 = GPy.kern.RBF(1, lengthscale=80) + GPy.kern.Bias(1)\n",
    "kern2 = GPy.kern.Coregionalize(1, output_dim=6, rank=5)\n",
    "kern = kern1**kern2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPy.models.GPRegression(X, y, kern)\n",
    "model.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "for i in range(6):\n",
    "    model.plot(fignum=1,fixed_inputs=[(1, i)])\n",
    "ax.set_xlabel('years')\n",
    "ax.set_ylabel('time/s')\n",
    "\n",
    "mlai.write_figure('olympic-sprint-icm-gp.svg',\n",
    "                  directory='./gp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/olympic-sprint-icm-gp.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gaussian process fit to the Olympic Sprint data.</i>\n",
    "\n",
    "Predictions in the multioutput case can be very effected by our\n",
    "covariance function *design*. This reflects the themes we saw on the\n",
    "first day where the importance of covariance function choice was\n",
    "emphasized at design time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonilla, E.V., Chai, K.M., Williams, C.K.I., n.d. Multi-task Gaussian\n",
    "process prediction, in:.\n",
    "\n",
    "Conti, S., O’Hagan, A., 2009. Bayesian emulation of complex multi-output\n",
    "and dynamic computer models. Journal of Statistical Planning and\n",
    "Inference 140, 640–651. <https://doi.org/doi:10.1016/j.jspi.2009.08.006>\n",
    "\n",
    "Goovaerts, P., 1997. Geostatistics For Natural Resources Evaluation.\n",
    "Oxford University Press.\n",
    "\n",
    "Helterbrand, J.D., Cressie, N.A.C., 1994. Universal cokriging under\n",
    "intrinsic coregionalization. Mathematical Geology 26, 205–226.\n",
    "\n",
    "Higdon, D.M., Gattiker, J., Williams, B., Rightley, M., 2008. Computer\n",
    "model calibration using high dimensional output. Journal of the American\n",
    "Statistical Association 103, 570–583.\n",
    "\n",
    "Journel, A.G., Huijbregts, C.J., 1978. Mining geostatistics. Academic\n",
    "Press, London.\n",
    "\n",
    "Lawrence, N.D., Platt, J.C., 2004. Learning to learn with the\n",
    "informative vector machine, in:. pp. 512–519.\n",
    "<https://doi.org/10.1145/1015330.1015382>\n",
    "\n",
    "Minka, T.P., Picard, R.W., 1997. Learning how to learn is learning with\n",
    "point sets.\n",
    "\n",
    "Rasmussen, C.E., Williams, C.K.I., 2006. Gaussian processes for machine\n",
    "learning. mit, Cambridge, MA.\n",
    "\n",
    "Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC\n",
    "Press.\n",
    "\n",
    "Seeger, M., Jordan, M.I., 2004. Sparse Gaussian Process classification\n",
    "with multiple classes (No. 661). Department of Statistics, University of\n",
    "California at Berkeley.\n",
    "\n",
    "Skolidis, G., Sanguinetti, G., 2011. Bayesian multitask classification\n",
    "with Gaussian process priors. IEEE Transactions on Neural Networks 22,\n",
    "2011–2021.\n",
    "\n",
    "Teh, Y.W., Seeger, M., Jordan, M.I., n.d. Semiparametric latent factor\n",
    "models, in:. pp. 333–340.\n",
    "\n",
    "Wackernagel, H., 2003. Multivariate geostatistics: An introduction with\n",
    "applications, 3rd ed. springer.\n",
    "\n",
    "Williams, C.K.I., Barber, D., 1998. Bayesian Classification with\n",
    "Gaussian processes. IEEE Transactions on Pattern Analysis and Machine\n",
    "Intelligence 20, 1342–1351.\n",
    "\n",
    "Woodward, I., Lomas, M.R., Betts, R.A., 1998. Vegetation-climate\n",
    "feedbacks in a greenhouse world. Philosophical Transactions: Biological\n",
    "Sciences 353, 29–39.\n",
    "\n",
    "Yu, K., Tresp, V., Schwaighofer, A., 2005. Learning Gaussian processes\n",
    "from multiple tasks, in: Proceedings of the 22nd International\n",
    "Conference on Machine Learning (Icml 2005). pp. 1012–1019."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
