{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised Learning with Gaussian Processes\n",
    "=============================================\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), University of\n",
    "\n",
    "Cambridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\tk}[1]{}\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\uniformDist}[3]{\\mathcal{U}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\uniformSamp}[2]{\\mathcal{U}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic PCA\n",
    "=================\n",
    "\n",
    "In 1997 [Tipping and\n",
    "Bishop](http://research.microsoft.com/pubs/67218/bishop-ppca-jrss.pdf)\n",
    "(Tipping and Bishop, 1999) and\n",
    "[Roweis](https://www.cs.nyu.edu/~roweis/papers/empca.pdf) (Roweis, n.d.)\n",
    "independently revisited Hotelling’s model and considered the case where\n",
    "the noise variance was finite, but *shared* across all output dimensons.\n",
    "Their model can be thought of as a factor analysis where $$\n",
    "\\boldsymbol{\\Sigma} = \\noiseStd^2 \\mathbf{I}.\n",
    "$$ This leads to a marginal likelihood of the form $$\n",
    "p(\\mathbf{Y}|\\mathbf{W}, \\noiseStd^2)\n",
    "= \\prod_{i=1}^n\\mathcal{N}\\left(\\mathbf{ y}_{i, :}|\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top + \\noiseStd^2 \\mathbf{I}\\right)\n",
    "$$ where the limit of $\\noiseStd^2\\rightarrow 0$ is *not* taken. This\n",
    "defines a proper probabilistic model. Tippping and Bishop then went on\n",
    "to prove that the *maximum likelihood* solution of this model with\n",
    "respect to $\\mathbf{W}$ is given by an eigenvalue problem. In the\n",
    "probabilistic PCA case the eigenvalues and eigenvectors are given as\n",
    "follows. $$\n",
    "\\mathbf{W}= \\mathbf{U}\\mathbf{L} \\mathbf{R}^\\top\n",
    "$$ where $\\mathbf{U}$ is the eigenvectors of the empirical covariance\n",
    "matrix $$\n",
    "\\mathbf{S} = \\sum_{i=1}^n(\\mathbf{ y}_{i, :} - \\boldsymbol{ \\mu})(\\mathbf{ y}_{i,\n",
    ":} - \\boldsymbol{ \\mu})^\\top,\n",
    "$$ which can be written\n",
    "$\\mathbf{S} = \\frac{1}{n} \\mathbf{Y}^\\top\\mathbf{Y}$ if the data is zero\n",
    "mean. The matrix $\\mathbf{L}$ is diagonal and is dependent on the\n",
    "*eigenvalues* of $\\mathbf{S}$, $\\boldsymbol{\\Lambda}$. If the $i$th\n",
    "diagonal element of this matrix is given by $\\lambda_i$ then the\n",
    "corresponding element of $\\mathbf{L}$ is $$\n",
    "\\ell_i = \\sqrt{\\lambda_i - \\noiseStd^2}\n",
    "$$ where $\\noiseStd^2$ is the noise variance. Note that if $\\noiseStd^2$\n",
    "is larger than any particular eigenvalue, then that eigenvalue (along\n",
    "with its corresponding eigenvector) is *discarded* from the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Implementation of Probabilistic PCA\n",
    "------------------------------------------\n",
    "\n",
    "We will now implement this algorithm in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilistic PCA algorithm\n",
    "def ppca(Y, q):\n",
    "    # remove mean\n",
    "    Y_cent = Y - Y.mean(0)\n",
    "\n",
    "    # Comute covariance\n",
    "    S = np.dot(Y_cent.T, Y_cent)/Y.shape[0]\n",
    "    lambd, U = np.linalg.eig(S)\n",
    "\n",
    "    # Choose number of eigenvectors\n",
    "    sigma2 = np.sum(lambd[q:])/(Y.shape[1]-q)\n",
    "    l = np.sqrt(lambd[:q]-sigma2)\n",
    "    W = U[:, :q]*l[None, :]\n",
    "    return W, sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we may not wish to compute the eigenvectors of the\n",
    "covariance matrix directly. This is because it requires us to estimate\n",
    "the covariance, which involves a sum of squares term, before estimating\n",
    "the eigenvectors. We can estimate the eigenvectors directly either\n",
    "through [QR\n",
    "decomposition](http://en.wikipedia.org/wiki/QR_decomposition) or\n",
    "[singular value\n",
    "decomposition](http://en.wikipedia.org/wiki/Singular_value_decomposition).\n",
    "We saw a similar issue arise when , where we also wished to avoid\n",
    "computation of $\\mathbf{Z}^\\top\\mathbf{Z}$ (or in the case of\n",
    "$\\boldsymbol{\\Phi}^\\top\\boldsymbol{\\Phi}$).\n",
    "\n",
    "::: {.cell .markdown}\n",
    "\n",
    "Posterior for Principal Component Analysis\n",
    "==========================================\n",
    "\n",
    "Under the latent variable model justification for principal component\n",
    "analysis, we are normally interested in inferring something about the\n",
    "latent variables given the data. This is the distribution, $$\n",
    "p(\\mathbf{ z}_{i, :} | \\mathbf{ y}_{i, :})\n",
    "$$ for any given data point. Determining this density turns out to be\n",
    "very similar to the approach for determining the Bayesian posterior of\n",
    "$\\mathbf{ w}$ in Bayesian linear regression, only this time we place the\n",
    "prior density over $\\mathbf{ z}_{i, :}$ instead of $\\mathbf{ w}$. The\n",
    "posterior is proportional to the joint density as follows, $$\n",
    "p(\\mathbf{ z}_{i, :} | \\mathbf{ y}_{i, :}) \\propto p(\\mathbf{ y}_{i,\n",
    ":}|\\mathbf{W}, \\mathbf{ z}_{i, :}, \\noiseStd^2) p(\\mathbf{ z}_{i, :})\n",
    "$$ And as in the Bayesian linear regression case we first consider the\n",
    "log posterior, $$\n",
    "\\log p(\\mathbf{ z}_{i, :} | \\mathbf{ y}_{i, :}) = \\log p(\\mathbf{ y}_{i, :}|\\mathbf{W},\n",
    "\\mathbf{ z}_{i, :}, \\noiseStd^2) + \\log p(\\mathbf{ z}_{i, :}) + \\text{const}\n",
    "$$ where the constant is not dependent on $\\mathbf{ z}$. As before we\n",
    "collect the quadratic terms in $\\mathbf{ z}_{i, :}$ and we assemble them\n",
    "into a Gaussian density over $\\mathbf{ z}$. $$\n",
    "\\log p(\\mathbf{ z}_{i, :} | \\mathbf{ y}_{i, :}) =\n",
    "-\\frac{1}{2\\noiseStd^2} (\\mathbf{ y}_{i, :} - \\mathbf{W}\\mathbf{ z}_{i,\n",
    ":})^\\top(\\mathbf{ y}_{i, :} - \\mathbf{W}\\mathbf{ z}_{i, :}) - \\frac{1}{2}\n",
    "\\mathbf{ z}_{i, :}^\\top \\mathbf{ z}_{i, :} + \\text{const}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0\n",
    "\n",
    "Multiply out the terms in the brackets. Then collect the quadratic term\n",
    "and the linear terms together. Show that the posterior has the form $$\n",
    "\\mathbf{ z}_{i, :} | \\mathbf{W}\\sim \\mathcal{N}\\left(\\boldsymbol{ \\mu}_x,\\mathbf{C}_x\\right)\n",
    "$$ where $$\n",
    "\\mathbf{C}_x = \\left(\\noiseStd^{-2}\n",
    "\\mathbf{W}^\\top\\mathbf{W}+ \\mathbf{I}\\right)^{-1}\n",
    "$$ and $$\n",
    "\\boldsymbol{ \\mu}_x\n",
    "= \\mathbf{C}_x \\noiseStd^{-2}\\mathbf{W}^\\top \\mathbf{ y}_{i, :} \n",
    "$$ Compare this to the posterior for the Bayesian linear regression from\n",
    "last week, do they have similar forms? What matches and what differs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0 Answer\n",
    "\n",
    "Write your answer to Exercise 0 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Implementation of the Posterior\n",
    "--------------------------------------\n",
    "\n",
    "Now let’s implement the system in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Use the values for $\\mathbf{W}$ and $\\noiseStd^2$ you have computed,\n",
    "along with the data set $\\mathbf{Y}$ to compute the posterior density\n",
    "over $\\mathbf{Z}$. Write a function of the form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer to Exercise 1 here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python mu\\_x, C\\_x = posterior(Y, W, sigma2)} where `mu_x` and `C_x` are\n",
    "the posterior mean and posterior covariance for the given $\\mathbf{Y}$.\n",
    "\n",
    "Don’t forget to subtract the mean of the data `Y` inside your function\n",
    "before computing the posterior: remember we assumed at the beginning of\n",
    "our analysis that the data had been centred (i.e. the mean was\n",
    "removed).}{20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4 Answer Code\n",
    "# Write code for you answer to this question in this box\n",
    "# Do not delete these comments, otherwise you will get zero for this answer.\n",
    "# Make sure your code has run and the answer is correct *before* submitting your notebook for marking.\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "def posterior(Y, W, sigma2):\n",
    "    Y_cent = Y - Y.mean(0)\n",
    "    # Compute posterior over X\n",
    "    C_x = \n",
    "    mu_x = \n",
    "    return mu_x, C_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerically Stable and Efficient Version\n",
    "========================================\n",
    "\n",
    "Just as we saw for and computation of a matrix such as\n",
    "$\\mathbf{Y}^\\top\\mathbf{Y}$ (or its centred version) can be a bad idea\n",
    "in terms of loss of numerical accuracy. Fortunately, we can find the\n",
    "eigenvalues and eigenvectors of the matrix $\\mathbf{Y}^\\top\\mathbf{Y}$\n",
    "without direct computation of the matrix. This can be done with the\n",
    "[*singular value\n",
    "decomposition*](http://en.wikipedia.org/wiki/Singular_value_decomposition).\n",
    "The singular value decompsition takes a matrix, $\\mathbf{Z}$ and\n",
    "represents it in the form, $$\n",
    "\\mathbf{Z} = \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{V}^\\top\n",
    "$$ where $\\mathbf{U}$ is a matrix of orthogonal vectors in the columns,\n",
    "meaning $\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{I}$. It has the same number\n",
    "of rows and columns as $\\mathbf{Z}$. The matrices $\\mathbf{\\Lambda}$ and\n",
    "$\\mathbf{V}$ are both square with dimensionality given by the number of\n",
    "columns of $\\mathbf{Z}$. The matrix $\\mathbf{\\Lambda}$ is *diagonal* and\n",
    "$\\mathbf{V}$ is an orthogonal matrix so\n",
    "$\\mathbf{V}^\\top\\mathbf{V} = \\mathbf{V}\\mathbf{V}^\\top = \\mathbf{I}$.\n",
    "The eigenvalues of the matrix $\\mathbf{Y}^\\top\\mathbf{Y}$ are then given\n",
    "by the singular values of the matrix $\\mathbf{Y}^\\top$ squared and the\n",
    "eigenvectors are given by $\\mathbf{U}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution for $\\mathbf{W}$\n",
    "-------------------------\n",
    "\n",
    "Given the singular value decomposition of $\\mathbf{Y}$ then we have $$\n",
    "\\mathbf{W}=\n",
    "\\mathbf{U}\\mathbf{L}\\mathbf{R}^\\top\n",
    "$$ where $\\mathbf{R}$ is an arbitrary rotation matrix. This implies that\n",
    "the posterior is given by $$\n",
    "\\mathbf{C}_x =\n",
    "\\left[\\noiseStd^{-2}\\mathbf{R}\\mathbf{L}^2\\mathbf{R}^\\top + \\mathbf{I}\\right]^{-1}\n",
    "$$ because $\\mathbf{U}^\\top \\mathbf{U} = \\mathbf{I}$. Since, by\n",
    "convention, we normally take $\\mathbf{R} = \\mathbf{I}$ to ensure that\n",
    "the principal components are orthonormal we can write $$\n",
    "\\mathbf{C}_x = \\left[\\noiseStd^{-2}\\mathbf{L}^2 +\n",
    "\\mathbf{I}\\right]^{-1}\n",
    "$$ which implies that $\\mathbf{C}_x$ is actually diagonal with elements\n",
    "given by $$\n",
    "c_i = \\frac{\\noiseStd^2}{\\noiseStd^2 + \\ell^2_i}\n",
    "$$ and allows us to write $$\n",
    "\\boldsymbol{ \\mu}_x = [\\mathbf{L}^2 + \\noiseStd^2\n",
    "\\mathbf{I}]^{-1} \\mathbf{L} \\mathbf{U}^\\top \\mathbf{ y}_{i, :}\n",
    "$$ $$\n",
    "\\boldsymbol{ \\mu}_x = \\mathbf{D}\\mathbf{U}^\\top \\mathbf{ y}_{i, :}\n",
    "$$ where $\\mathbf{D}$ is a diagonal matrix with diagonal elements given\n",
    "by $d_{i} = \\frac{\\ell_i}{\\noiseStd^2 + \\ell_i^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilistic PCA algorithm using SVD\n",
    "def ppca(Y, q, center=True):\n",
    "    \"\"\"Probabilistic PCA through singular value decomposition\"\"\"\n",
    "    # remove mean\n",
    "    if center:\n",
    "        Y_cent = Y - Y.mean(0)\n",
    "    else:\n",
    "        Y_cent = Y\n",
    "        \n",
    "    # Comute singluar values, discard 'R' as we will assume orthogonal\n",
    "    U, sqlambd, _ = sp.linalg.svd(Y_cent.T,full_matrices=False)\n",
    "    lambd = (sqlambd**2)/Y.shape[0]\n",
    "    # Compute residual and extract eigenvectors\n",
    "    sigma2 = np.sum(lambd[q:])/(Y.shape[1]-q)\n",
    "    ell = np.sqrt(lambd[:q]-sigma2)\n",
    "    return U[:, :q], ell, sigma2\n",
    "\n",
    "def posterior(Y, U, ell, sigma2, center=True):\n",
    "    \"\"\"Posterior computation for the latent variables given the eigendecomposition.\"\"\"\n",
    "    if center:\n",
    "        Y_cent = Y - Y.mean(0)\n",
    "    else:\n",
    "        Y_cent = Y\n",
    "    C_x = np.diag(sigma2/(sigma2+ell**2))\n",
    "    d = ell/(sigma2+ell**2)\n",
    "    mu_x = np.dot(Y_cent, U)*d[None, :]\n",
    "    return mu_x, C_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difficulty for Probabilistic Approaches\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.non_linear_difficulty_plot_3(diagrams='./dimred/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge for composition of probabilistic models is that you need\n",
    "to propagate a probability densities through non linear mappings. This\n",
    "allows you to create broader classes of probability density.\n",
    "Unfortunately it renders the resulting densities *intractable*.\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/nonlinear-mapping-3d-plot.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A two dimensional grid mapped into three dimensions to form a\n",
    "two dimensional manifold.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.non_linear_difficulty_plot_2(diagrams='./dimred/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/nonlinear-mapping-2d-plot.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A one dimensional line mapped into two dimensions by two\n",
    "separate independent functions. Each point can be mapped exactly through\n",
    "the mappings.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.non_linear_difficulty_plot_1(diagrams='./dimred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/gaussian-through-nonlinear.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A Gaussian density over the input of a non linear function\n",
    "leads to a very non Gaussian output. Here the output is multimodal.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Started and Downloading Data\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import GPy\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is for plotting and to prepare the bigger models for\n",
    "later useage. If you are interested, you can have a look, but this is\n",
    "not essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#3FCC94\", \"#DD4F23\", \"#C6D63B\", \"#D44271\", \n",
    "          \"#E4A42C\", \"#4F9139\", \"#6DDA4C\", \"#85831F\", \n",
    "          \"#B36A29\", \"#CF4E4A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(X, which_dims, labels):\n",
    "    fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "    X = X[:,which_dims]\n",
    "    ulabs = []\n",
    "    for lab in labels:\n",
    "        if not lab in ulabs:\n",
    "            ulabs.append(lab)\n",
    "            pass\n",
    "        pass\n",
    "    for i, lab in enumerate(ulabs):\n",
    "        ax.scatter(*X[labels==lab].T,marker='o',color=colors[i],label=lab)\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we’ll use a data set containing all handwritten digits\n",
    "from $0 \\cdots 9$ handwritten, provided by de Campos et al. (2009). We\n",
    "will only use some of the digits for the demonstrations in this lab\n",
    "class, but you can edit the code below to select different subsets of\n",
    "the digit data as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which = [0,1,2,6,7,9] # which digits to work on\n",
    "data = pods.datasets.decampos_digits(which_digits=which)\n",
    "Y = data['Y']\n",
    "labels = data['str_lbls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to plot some of the digits using `plt.matshow` (the digit\n",
    "images have size `16x16`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis\n",
    "----------------------------\n",
    "\n",
    "Principal component analysis (PCA) finds a rotation of the observed\n",
    "outputs, such that the rotated principal component (PC) space maximizes\n",
    "the variance of the data observed, sorted from most to least important\n",
    "(most to least variable in the corresponding PC).\n",
    "\n",
    "In order to apply PCA in an easy way, we have included a PCA module in\n",
    "pca.py. You can import the module by import \\<path.to.pca\\> (without the\n",
    "ending .py!). To run PCA on the digits we have to reshape (Hint:\n",
    "np.reshape ) digits .\n",
    "\n",
    "-   What is the right shape $n\\times p$ to use?\n",
    "\n",
    "We will call the reshaped observed outputs $\\mathbf{Y}$ in the\n",
    "following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yn = Y#Y-Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s run PCA on the reshaped dataset $\\mathbf{Y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPy.util import pca\n",
    "p = pca.pca(Y) # create PCA class with digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot will show the lower dimensional representation of the\n",
    "digits in 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plot_fracs(20) # plot first 20 eigenvalue fractions\n",
    "p.plot_2d(Y,labels=labels.flatten(), colors=colors)\n",
    "pb.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Latent Variable Model\n",
    "--------------------------------------\n",
    "\n",
    "The Gaussian Process Latent Variable Model (GP-LVM) (Lawrence, 2005)\n",
    "embeds PCA into a Gaussian process framework, where the latent inputs\n",
    "$\\mathbf{Z}$ are learnt as hyperparameters and the mapping variables\n",
    "$\\mathbf{W}$ are integrated out. The advantage of this interpretation is\n",
    "it allows PCA to be generalized in a non linear way by replacing the\n",
    "resulting *linear* covariance witha non linear covariance. But first,\n",
    "let’s see how GPLVM is equivalent to PCA using an automatic relevance\n",
    "determination (ARD, see e.g. Bishop (2006)) linear kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4 # How many latent dimensions to use\n",
    "kernel = GPy.kern.Linear(input_dim, ARD=True) # ARD kernel\n",
    "m = GPy.models.GPLVM(Yn, input_dim=input_dim, kernel=kernel)\n",
    "\n",
    "m.optimize(messages=1, max_iters=1000) # optimize for 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.kern.plot_ARD()\n",
    "plot_model(m.X, m.linear.variances.argsort()[-2:], labels.flatten())\n",
    "pb.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the solution with a linear kernel is the same as the PCA\n",
    "solution with the exception of rotational changes and axis flips.\n",
    "\n",
    "For the sake of time, the solution you see was only running for 1000\n",
    "iterations, thus it might not be converged fully yet. The GP-LVM\n",
    "proceeds by iterative optimization of the *inputs* to the covariance. As\n",
    "we saw in the lecture earlier, for the linear covariance, these latent\n",
    "points can be optimized with an eigenvalue problem, but generally, for\n",
    "non-linear covariance functions, we are obliged to use gradient based\n",
    "optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Latent Doodle Space\n",
    "----------------------------\n",
    "\n",
    "[![](attachment:https://i.vimeocdn.com/video/\\id_640x480.jpg)](https://vimeo.com/3235882#t=)\n",
    "\n",
    "Figure: <i>The latent doodle space idea of Baxter and Anjyo (2006)\n",
    "manages to build a smooth mapping across very sparse data.</i>\n",
    "\n",
    "**Generalization with much less Data than Dimensions**\n",
    "\n",
    "-   Powerful uncertainly handling of GPs leads to surprising properties.\n",
    "\n",
    "-   Non-linear models can be used where there are fewer data points than\n",
    "    dimensions *without overfitting*.\n",
    "\n",
    "<span style=\"text-align:right\">(Baxter and Anjyo, 2006)</span>"
   ],
   "attachments": {
    "https://i.vimeocdn.com/video/\\id_640x480.jpg": {
     "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAUFBQUFBQUGBgUICAcICAsKCQkKCxEMDQwNDBEaEBMQ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\nBsY0806EUzXY91yXcaf0ToSNzA2MaeadCKZrsef/2Q==\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous Character Control\n",
    "----------------------------\n",
    "\n",
    "(Levine et al., 2012){style=“text-align:right”} - Graph diffusion prior\n",
    "for enforcing connectivity between motions.\n",
    "$$\\log p(\\mathbf{X}) = w_c \\sum_{i,j} \\log K_{ij}^d$$ with the graph\n",
    "diffusion kernel $\\mathbf{K}^d$ obtain from\n",
    "$$K_{ij}^d = \\exp(\\beta \\mathbf{H})\n",
    "    \\qquad \\text{with} \\qquad \\mathbf{H} = -\\mathbf{T}^{-1/2} \\mathbf{L} \\mathbf{T}^{-1/2}$$\n",
    "the graph Laplacian, and $\\mathbf{T}$ is a diagonal matrix with\n",
    "$T_{ii} = \\sum_j w(\\mathbf{ x}_i, \\mathbf{ x}_j)$,\n",
    "$$L_{ij} = \\begin{cases} \\sum_k w(\\mathbf{ x}_i,\\mathbf{ x}_k) & \\text{if $i=j$}\n",
    "    \\\\\n",
    "    -w(\\mathbf{ x}_i,\\mathbf{ x}_j) &\\text{otherwise.}\n",
    "    \\end{cases}$$ and\n",
    "$w(\\mathbf{ x}_i,\\mathbf{ x}_j) = || \\mathbf{ x}_i - \\mathbf{ x}_j||^{-p}$\n",
    "measures similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character Control: Results\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('hr3pdDl5IAg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>Character control in the latent space described the the\n",
    "GP-LVM Levine et al. (2012)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baxter, W.V., Anjyo, K.-I., 2006. Latent doodle space, in: EUROGRAPHICS.\n",
    "Vienna, Austria, pp. 477–485.\n",
    "<https://doi.org/10.1111/j.1467-8659.2006.00967.x>\n",
    "\n",
    "Bishop, C.M., 2006. Pattern recognition and machine learning. springer.\n",
    "\n",
    "de Campos, T.E., Babu, B.R., Varma, M., 2009. Character recognition in\n",
    "natural images, in: Proceedings of the Fourth International Conference\n",
    "on Computer Vision Theory and Applications - Volume 2: VISAPP,\n",
    "(Visigrapp 2009). INSTICC; SciTePress, pp. 273–280.\n",
    "<https://doi.org/10.5220/0001770102730280>\n",
    "\n",
    "Lawrence, N.D., 2005. Probabilistic non-linear principal component\n",
    "analysis with Gaussian process latent variable models. Journal of\n",
    "Machine Learning Research 6, 1783–1816.\n",
    "\n",
    "Levine, S., Wang, J.M., Haraux, A., Popović, Z., Koltun, V., 2012.\n",
    "Continuous character control with low-dimensional embeddings. ACM\n",
    "Transactions on Graphics (SIGGRAPH 2012) 31.\n",
    "\n",
    "Roweis, S.T., n.d. EM algorithms for PCA and SPCA, in:. pp. 626–632.\n",
    "\n",
    "Tipping, M.E., Bishop, C.M., 1999. Mixtures of probabilistic principal\n",
    "component analysers. Neural Computation 11, 443–482."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
